{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoL-karman2d.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM89D+95P00z0HKqgBgDx9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thunil/jupyter-notebooks/blob/main/SoL_karman2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT_RWmTEugu9"
      },
      "source": [
        "# Reducing Numerical Errors with Deep Learning\n",
        "\n",
        "Next, we'll target numerical errors that arise in the discretization of a continuous PDE $\\mathcal P^*$, i.e. when we formulate $\\mathcal P$. This approach will demonstrate that, despite the lack of closed-form descriptions, discretization errors often are functions with regular and repeating structures and, thus, can be learned by a neural network. Once the network is trained, it can be evaluated locally to improve the solution of a PDE-solver, i.e., to reduce its numerical error. The resulting method is a hybrid one: it will always run (a coarse) PDE solver, and then improve if at runtime with corrections inferred by an NN.\n",
        "\n",
        "Pretty much all numerical methods contain some form of iterative process. That can be repeated updates over time for explicit solvers,or within a single update step for implicit solvers. Below we'll target iterations over time, an example for the second case could be found [here](https://github.com/tum-pbs/CG-Solver-in-the-Loop).\n",
        "\n",
        "In the context of reducing errors, it's crucial to have a _differentiable physics solver_, so that the learning process can take the reaction of the solver into account. This interaction is not possible with supervised learning or PINN training. Even small inference errors of a supervised model can accumulate over time, and lead to a data distribution that differs from the distribution of the pre-computed data. This distribution shift can lead to sub-optimal results, or even cause blow-ups of the solver.\n",
        "\n",
        "In order to learn the error function, we'll consider two different discretizations of the same PDE $\\mathcal P^*$: \n",
        "a _reference_ version, which we assume to be accurate, with a discretized version \n",
        "$\\mathcal P_r$, and solutions $\\mathbf r \\in \\mathscr R$, where $\\mathscr R$ denotes the manifold of solution of $\\mathcal P_r$.\n",
        "In parallel to this, we have a less accurate approximation of the same PDE, which we'll refer to as the _source_ version, as this will be the solver that our NN should later on interact with. Analogously,\n",
        "we have $\\mathcal P_s$ with solutions $\\mathbf s \\in \\mathscr S$.\n",
        "And after training, we'll have a _hybrid_ solver that uses $\\mathcal P_s$ in conjunction with a trained network to obtain improved solutions, i.e., solutions that are closer to the ones produced by $\\mathcal P_r$.\n",
        "\n",
        "```{figure} resources/diffphys-sol-manifolds.jpeg\n",
        "---\n",
        "height: 280px\n",
        "name: diffphys-sol-manifolds\n",
        "---\n",
        "Visual overview of coarse and reference manifolds\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tayrJa7_ZzS_"
      },
      "source": [
        "\n",
        "explain coarse/ref setup...\n",
        "Let's assume $\\mathcal{P}$ advances a solution by a time step $\\Delta t$, and let's denote $n$ consecutive steps by a superscript:\n",
        "$\n",
        "\\newcommand{\\pde}{\\mathcal{P}}\n",
        "\\newcommand{\\pdec}{\\pde_{s}}\n",
        "\\newcommand{\\vc}[1]{\\mathbf{s}_{#1}} \n",
        "\\newcommand{\\vr}[1]{\\mathbf{r}_{#1}} \n",
        "\\newcommand{\\vcN}{\\vs}          \n",
        "\\newcommand{\\project}{\\mathcal{T}}   \n",
        "\\vc{t+n} = \\pdec(\\pdec(\\cdots \\pdec( \\project \\vr{t}  )\\cdots)) = \\pdec^n ( \\project \\vr{t} ) .\n",
        "$\n",
        "Here we assume a mapping operator $\\mathcal{T}$ exists that transfers a reference solution to the source manifold. This could, e.g., be a simple downsampling operation.\n",
        "Especially for longer sequences, i.e. larger $n$, the source state \n",
        "$\\newcommand{\\vc}[1]{\\mathbf{s}_{#1}} \\vc{t+n}$\n",
        "will deviate from a corresponding reference state\n",
        "$\\newcommand{\\vr}[1]{\\mathbf{r}_{#1}} \\vr{t+n}$. \n",
        "This is what we will address with an NN in the following.\n",
        "\n",
        "We'll use an $L^2$-norm in the following to quantify the deviations, i.e., \n",
        "$\n",
        "\\newcommand{\\loss}{\\mathcal{L}} \n",
        "\\newcommand{\\corr}{\\mathcal{C}} \n",
        "\\newcommand{\\vc}[1]{\\mathbf{s}_{#1}} \n",
        "\\newcommand{\\vr}[1]{\\mathbf{r}_{#1}} \n",
        "\\loss (\\vc{t},\\project \\vr{t})=\\Vert\\vc{t}-\\project \\vr{t}\\Vert_2$. \n",
        "Our learning goal is to train at a correction operator $\\corr ( \\vc{} )$ such that \n",
        "a solution to which the correction is applied has a lower error than the original unmodified (source) solution: \n",
        "$\\loss ( \\pdec( \\corr (\\project \\vr{t_0}) ) , \\project \\vr{t_1}) < \\loss ( \\pdec( \\project \\vr{t_0} ), \\project \\vr{t_1})$. \n",
        "\n",
        "The correction function \n",
        "$\\newcommand{\\vcN}{\\mathbf{s}} \\newcommand{\\corr}{\\mathcal{C}} \\corr (\\vcN | \\theta)$ \n",
        "is represented as a deep neural network with weights $\\theta$\n",
        "and receives the state $\\vcN$ to infer an additive correction field with the same dimension.\n",
        "To distinguish the original states $\\vcN$ from the corrected ones, we'll denote the latter with an added tilde\n",
        "$\\newcommand{\\vctN}{\\tilde{\\mathbf{s}}} \\vctN$.\n",
        "The overall learning goal now becomes\n",
        "\n",
        "$\n",
        "\\text{argmin}_\\theta | \n",
        "( \\pdec \\corr )^n ( \\project \\vr{t} )\n",
        "- \\project \\vr{t}|^2\n",
        "$\n",
        "\n",
        "A crucial bit here that's easy to overlook is that the correction depends on the modified states, i.e.\n",
        "it is a function of\n",
        "$\\newcommand{\\vctN}{\\tilde{\\mathbf{s}}} \\vctN$, so we have \n",
        "$\\newcommand{\\vctN}{\\tilde{\\mathbf{s}}} \\newcommand{\\corr}{\\mathcal{C}} \\corr (\\vctN | \\theta)$.\n",
        "These states actually evolve over time when training. They don't exist beforehand.\n",
        "\n",
        "**TL;DR**:\n",
        "We'll train a network $\\mathcal{C}$ to reduce the numerical errors of a simulator with a more accurate reference. Here it's crucial to have the _source_ solver realized as a differential physics operator, such that it can give gradients for an improved training of $\\mathcal{C}$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "---\n",
        "\n",
        "First, let's download the prepared data set (for details on generation & loading cf. https://github.com/tum-pbs/Solver-in-the-Loop), and let's get the data handling out of the way, so that we can focus on the _interesting_ parts..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwZudtWauiGa",
        "outputId": "cd82011f-0a18-47d6-f1e2-d7c87b6f4a9e"
      },
      "source": [
        "import os, sys, logging, argparse, pickle, glob, random, distutils.dir_util\n",
        "\n",
        "if not os.path.isfile('data-karman2d-train.pickle'):\n",
        "  import urllib.request\n",
        "  url=\"https://ge.in.tum.de/download/2020-solver-in-the-loop/sol-karman-2d-data.pickle\"\n",
        "  print(\"Downloading training data (73MB), this can take a moment the first time...\")\n",
        "  urllib.request.urlretrieve(url, 'data-karman2d-train.pickle')\n",
        "\n",
        "with open('data-karman2d-train.pickle', 'rb') as f: dataPreloaded = pickle.load(f)\n",
        "print(\"Loaded data, {} training sims\".format(len(dataPreloaded)) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading training data (73MB), this can take a moment the first time...\n",
            "Loaded data, 6 training sims\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY1F4kdWPLNG"
      },
      "source": [
        "Also let's get installing / importing all the necessary libraries out of the way. And while we're at it, we can set the random seed - obviously, 42 is the ultimate choice here ðŸ™‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGN4GqxkIueM",
        "outputId": "5d7353b7-0169-42e7-c1bb-7b203d123f82"
      },
      "source": [
        "!pip install --upgrade --quiet phiflow\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "from phi.tf.flow import *\n",
        "import phi.tf.util\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.compat.v1.set_random_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.7MB 8.8MB/s \n",
            "\u001b[?25h  Building wheel for phiflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "TensorFlow 1.x selected.\n",
            "Could not load resample cuda libraries: CUDA binaries not found at /usr/local/lib/python3.6/dist-packages/phi/tf/cuda/build/resample.so. Run \"python setup.py cuda\" to compile them\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/util.py:119: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/profiling.py:12: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/profiling.py:13: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/phi/viz/display.py:80: UserWarning: GUI is disabled because of missing dependencies: No module named 'dash_core_components'. To install all dependencies, run $ pip install phiflow[gui]\n",
            "  warnings.warn('GUI is disabled because of missing dependencies: %s. To install all dependencies, run $ pip install phiflow[gui]' % import_error)\n",
            "/usr/local/lib/python3.6/dist-packages/phi/tf/flow.py:15: UserWarning: TensorFlow-CUDA solver is not available. To compile it, download phiflow sources and run\n",
            "$ python setup.py tf_cuda\n",
            "before reinstalling phiflow.\n",
            "  warnings.warn(\"TensorFlow-CUDA solver is not available. To compile it, download phiflow sources and run\\n$ python setup.py tf_cuda\\nbefore reinstalling phiflow.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnzPdoww11P"
      },
      "source": [
        "Now we can set up the _source_ simulation $\\newcommand{\\pdec}{\\pde_{s}} \\pdec$. \n",
        "Note that we won't deal with \n",
        "$\\newcommand{\\pder}{\\pde_{r}} \\pder$\n",
        "below: the downsampled reference data is contained in the training data set. It was generated with a four times finer discretization. Below we're focusing on the interaction of the source solver and the NN. \n",
        "\n",
        "This code block and the next ones will define lots of functions, that will be used later on for training.\n",
        "\n",
        "The `KarmanFlow` solver below simulates a relatively standard wake flow case with a spherical obstacle in a rectangular domain, and an explicit viscosity solve to obtain different Reynolds numbers. This is the geometry of the setup:\n",
        "\n",
        "```{figure} resources/diffphys-sol-domain.png\n",
        "---\n",
        "height: 200px\n",
        "name: diffphys-sol-domain\n",
        "---\n",
        "Domain setup for the wake flow case.\n",
        "```\n",
        "\n",
        "The solver applies inflow boundary conditions for the y-velocity with a pre-multiplied mask (`velBCy,velBCyMask`), and then calls `super().step()` to run the _regular_ phiflow fluid solving step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WNMcdWUw4EP"
      },
      "source": [
        "class KarmanFlow(IncompressibleFlow):\n",
        "    def __init__(self, pressure_solver=None, make_input_divfree=False, make_output_divfree=True):\n",
        "        IncompressibleFlow.__init__(self, pressure_solver, make_input_divfree, make_output_divfree)\n",
        "\n",
        "        self.infl = Inflow(box[5:10, 25:75])\n",
        "        self.obst = Obstacle(Sphere([50, 50], 10))\n",
        "\n",
        "    def step(self, fluid, re, res, velBCy, velBCyMask, dt=1.0, gravity=Gravity()):\n",
        "        # apply viscosity\n",
        "        alpha = 1.0/math.reshape(re, [fluid._batch_size, 1, 1, 1]) * dt * res * res\n",
        "\n",
        "        cy = diffuse(CenteredGrid(fluid.velocity.data[0].data), alpha)\n",
        "        cx = diffuse(CenteredGrid(fluid.velocity.data[1].data), alpha)\n",
        "\n",
        "        # apply velocity BCs, only for y velocity for now. note: content of velBCy should be pre-multiplied\n",
        "        cy = cy*(1.0 - velBCyMask) + velBCy\n",
        "\n",
        "        fluid = fluid.copied_with(velocity=StaggeredGrid([cy.data, cx.data], fluid.velocity.box))\n",
        "\n",
        "        return super().step(fluid=fluid, dt=dt, obstacles=[self.obst], gravity=gravity, density_effects=[self.infl], velocity_effects=())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFUGICgxk0K"
      },
      "source": [
        "Network arch: fully convolutional\n",
        "input: 2 fields with x,y velociy, plus Reynolds number as constant channel\n",
        "output: 2 component velocity field\n",
        "\n",
        "simple model, using keras for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIrWYTy6xscA"
      },
      "source": [
        "def model_small(tensor_in):\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Input(tensor=tensor_in),\n",
        "        keras.layers.Conv2D(filters=32, kernel_size=5, padding='same', activation=tf.nn.relu),\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=5, padding='same', activation=tf.nn.relu),\n",
        "        keras.layers.Conv2D(filters=2,  kernel_size=5, padding='same', activation=None),  # u, v\n",
        "    ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfHvdI7yxtdj"
      },
      "source": [
        "and for flexibility (and larger-scale tests), let's define a _proper_ ResNet with a few more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyfpA7Fbx0ro"
      },
      "source": [
        "def model_medium(tensor_in):\n",
        "    l_input = keras.layers.Input(tensor=tensor_in)\n",
        "    block_0 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(l_input)\n",
        "    block_0 = keras.layers.LeakyReLU()(block_0)\n",
        "\n",
        "    l_conv1 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(block_0)\n",
        "    l_conv1 = keras.layers.LeakyReLU()(l_conv1)\n",
        "    l_conv2 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(l_conv1)\n",
        "    l_skip1 = keras.layers.add([block_0, l_conv2])\n",
        "    block_1 = keras.layers.LeakyReLU()(l_skip1)\n",
        "\n",
        "    l_conv3 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(block_1)\n",
        "    l_conv3 = keras.layers.LeakyReLU()(l_conv3)\n",
        "    l_conv4 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(l_conv3)\n",
        "    l_skip2 = keras.layers.add([block_1, l_conv4])\n",
        "    block_2 = keras.layers.LeakyReLU()(l_skip2)\n",
        "\n",
        "    l_conv5 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(block_2)\n",
        "    l_conv5 = keras.layers.LeakyReLU()(l_conv5)\n",
        "    l_conv6 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(l_conv5)\n",
        "    l_skip3 = keras.layers.add([block_2, l_conv6])\n",
        "    block_3 = keras.layers.LeakyReLU()(l_skip3)\n",
        "\n",
        "    l_conv7 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(block_3)\n",
        "    l_conv7 = keras.layers.LeakyReLU()(l_conv7)\n",
        "    l_conv8 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(l_conv7)\n",
        "    l_skip4 = keras.layers.add([block_3, l_conv8])\n",
        "    block_4 = keras.layers.LeakyReLU()(l_skip4)\n",
        "\n",
        "    l_conv9 = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(block_4)\n",
        "    l_conv9 = keras.layers.LeakyReLU()(l_conv9)\n",
        "    l_convA = keras.layers.Conv2D(filters=32, kernel_size=5, padding='same')(l_conv9)\n",
        "    l_skip5 = keras.layers.add([block_4, l_convA])\n",
        "    block_5 = keras.layers.LeakyReLU()(l_skip5)\n",
        "\n",
        "    l_output = keras.layers.Conv2D(filters=2,  kernel_size=5, padding='same')(block_5)\n",
        "    return keras.models.Model(inputs=l_input, outputs=l_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew-MgPSlyLW-"
      },
      "source": [
        "pretty important functions: transform simulation state into input tensor for network\n",
        "\n",
        "and then, after network evaluation, transform output tensor into phiflow staggered velocity grid (warning: two _centered_ grids with different sizes, leave work to `unstack_staggered_tensor` function in `StaggeredGrid` constructor)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhGFpTjGyRyg"
      },
      "source": [
        "def to_feature(fluidstate, ext_const_channel):\n",
        "    # drop the unused edges of the staggered velocity grid making its dim same to the centered grid's\n",
        "    with tf.name_scope('to_feature') as scope:\n",
        "        return math.concat(\n",
        "            [\n",
        "                fluidstate.velocity.staggered_tensor()[:, :-1:, :-1:, 0:2],\n",
        "                tf.constant(shape=fluidstate.density.data.shape, value=1.0)*math.reshape(value=ext_const_channel, shape=[fluidstate._batch_size, 1, 1, 1]),\n",
        "            ],\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "def to_staggered(tensor_cen, box):\n",
        "    with tf.name_scope('to_staggered') as scope:\n",
        "        return StaggeredGrid(math.pad(tensor_cen, ((0,0), (0,1), (0,1), (0,0))), box=box)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VngMwN_9y00S"
      },
      "source": [
        "we also need some data handling\n",
        "\n",
        "load all \"ground truth\" reference data, already downsampled!\n",
        "\n",
        "we actually have a lot of data: multiple simulations, with many time steps, each with different fields\n",
        "\n",
        "data format: ['sim', frame, field (dens/vel)] where each field is [batch-size, y-size, x-size, channels]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjywcdD2y20t"
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self, data_preloaded, num_frames, num_sims=None, batch_size=1):\n",
        "        self.epoch         = None\n",
        "        self.epochIdx      = 0\n",
        "        self.batch         = None\n",
        "        self.batchIdx      = 0\n",
        "        self.step          = None\n",
        "        self.stepIdx       = 0\n",
        "\n",
        "        self.dataPreloaded = data_preloaded\n",
        "        self.batchSize     = batch_size\n",
        "\n",
        "        self.numSims       = num_sims\n",
        "        self.numBatches    = num_sims//batch_size\n",
        "        self.numFrames     = num_frames\n",
        "        self.numSteps      = num_frames\n",
        "\n",
        "        #with open('/Users/thuerey/temp/sol-karman-2d-data.pickle', 'rb') as f: self.dataPreloaded = pickle.load(f)\n",
        "        \n",
        "        self.dataSims = ['karman-fdt-hires-set/sim_%06d'%i for i in range(num_sims) ]\n",
        "        self.dataFrms = [ np.arange(num_frames) for _ in self.dataSims ]  \n",
        "\n",
        "        # constant additional per-sim channel: Reynolds numbers from data generation\n",
        "        ReNrs = [160000.0, 320000.0, 640000.0,  1280000.0,  2560000.0,  5120000.0]\n",
        "        self.extConstChannelPerSim = { self.dataSims[i]:[ReNrs[i]] for i in range(num_sims) }\n",
        "\n",
        "        #print(format(self.dataPreloaded[self.dataSims[0]][0][0].shape )) # debugging example: check shape of a single density field\n",
        "\n",
        "        # the data has the following shape ['sim', frame, field (dens/vel)] where each field is [batch-size, y-size, x-size, channels]\n",
        "        self.resolution = self.dataPreloaded[self.dataSims[0]][0][0].shape[1:3]  \n",
        "\n",
        "        # compute data statistics for normalization\n",
        "        self.dataStats = {\n",
        "            'std': (\n",
        "                np.std(np.concatenate([np.absolute(self.dataPreloaded[asim][i][0].reshape(-1)) for asim in self.dataSims for i in range(num_frames)], axis=-1)),  # density\n",
        "                (\n",
        "                    np.std(np.concatenate([np.absolute(self.dataPreloaded[asim][i][1][...,0].reshape(-1)) for asim in self.dataSims for i in range(num_frames)])),  # vel[0]\n",
        "                    np.std(np.concatenate([np.absolute(self.dataPreloaded[asim][i][1][...,1].reshape(-1)) for asim in self.dataSims for i in range(num_frames)])),  # vel[1]\n",
        "                )\n",
        "            ),\n",
        "            'ext.std': [ np.std([np.absolute(self.extConstChannelPerSim[asim][0]) for asim in self.dataSims]) ] # Re\n",
        "        }\n",
        "\n",
        "        print(\"Data stats: \"+format(self.dataStats))\n",
        "\n",
        "    # re-shuffle data for next epoch\n",
        "    def newEpoch(self, exclude_tail=0, shuffle_data=True):\n",
        "        self.numSteps = self.numFrames - exclude_tail\n",
        "        simSteps = [ (asim, self.dataFrms[i][0:(len(self.dataFrms[i])-exclude_tail)]) for i,asim in enumerate(self.dataSims) ]\n",
        "        sim_step_pair = []\n",
        "        for i,_ in enumerate(simSteps):\n",
        "            sim_step_pair += [ (i, astep) for astep in simSteps[i][1] ]  # (sim_idx, step) ...\n",
        "\n",
        "        if shuffle_data: random.shuffle(sim_step_pair)\n",
        "        self.epoch = [ list(sim_step_pair[i*self.numSteps:(i+1)*self.numSteps]) for i in range(self.batchSize*self.numBatches) ]\n",
        "        self.epochIdx += 1\n",
        "        self.batchIdx = 0\n",
        "        self.stepIdx = 0\n",
        "\n",
        "    def nextBatch(self):  \n",
        "        self.batchIdx += self.batchSize\n",
        "        self.stepIdx = 0\n",
        "\n",
        "    def nextStep(self):\n",
        "        self.stepIdx += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twIMJ3V0N1FX"
      },
      "source": [
        "add function ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfwd4TnqN1Tn"
      },
      "source": [
        "    # for class Dataset():\n",
        "\n",
        "    # get one mini batch of data: [marker density, velocity, Reynolds number] all from ground truth\n",
        "    def getData(self, consecutive_frames, with_skip=1):\n",
        "        marker_dens = [\n",
        "            math.concat([\n",
        "                self.dataPreloaded[\n",
        "                    self.dataSims[self.epoch[self.batchIdx+i][self.stepIdx][0]]  # sim_key\n",
        "                ][\n",
        "                    self.epoch[self.batchIdx+i][self.stepIdx][1]+j*with_skip  # steps\n",
        "                ][0]\n",
        "                for i in range(self.batchSize)\n",
        "            ], axis=0) for j in range(consecutive_frames+1)\n",
        "        ]\n",
        "        velocity = [\n",
        "            math.concat([\n",
        "                self.dataPreloaded[\n",
        "                    self.dataSims[self.epoch[self.batchIdx+i][self.stepIdx][0]]  # sim_key\n",
        "                ][\n",
        "                    self.epoch[self.batchIdx+i][self.stepIdx][1]+j*with_skip  # steps\n",
        "                ][1]\n",
        "                for i in range(self.batchSize)\n",
        "            ], axis=0) for j in range(consecutive_frames+1)\n",
        "        ]\n",
        "        ext = [\n",
        "            self.extConstChannelPerSim[\n",
        "                self.dataSims[self.epoch[self.batchIdx+i][self.stepIdx][0]]\n",
        "            ][0] for i in range(self.batchSize)\n",
        "        ]\n",
        "        return [marker_dens, velocity, ext]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIWnyPYlz8q7"
      },
      "source": [
        "next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59EBdEdj0QR2",
        "outputId": "c8188835-1314-4f6f-c7e2-c75a87f5632a"
      },
      "source": [
        "output_dir = \"./\"  # TODO create? , replaced params['train'] and params['tf']\n",
        "nsims = 6\n",
        "batch_size = 3\n",
        "simsteps = 500\n",
        "\n",
        "dataset = Dataset( data_preloaded=dataPreloaded, num_frames=simsteps, num_sims=nsims, batch_size=batch_size )\n",
        "#dataset.newEpoch()\n",
        "#print(format(getData(dataset,1)))\n",
        "#print(format(dataset.getData(1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data stats: {'std': (2.2194703, (0.32598782, 0.1820292)), 'ext.std': [1732512.6262166172]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N92RooWPzeA"
      },
      "source": [
        "init\n",
        "\n",
        "setup additional global variables and simulation\n",
        "\n",
        "very important: `msteps`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjgkdCzKP2Ip",
        "outputId": "d98585c9-7eec-492e-da62-6f7a10bfe230"
      },
      "source": [
        "# one of the most crucial! how many simulation steps to look into the future while training\n",
        "msteps = 4\n",
        "\n",
        "# this is the actual resolution in terms of cells\n",
        "source_res = list(dataset.resolution)\n",
        "# this is only a virtual size, in terms of abstract units for the bounding box of the domain (in practice it's important for conversions or when rescaling to physical units)\n",
        "sim_len = 100.\n",
        "\n",
        "source     =   Fluid(Domain(resolution=source_res, box=box[0:sim_len*2, 0:sim_len], boundaries=OPEN), buoyancy_factor=0, batch_size=batch_size)\n",
        "reference = [ Fluid(Domain(resolution=source_res, box=box[0:sim_len*2, 0:sim_len], boundaries=OPEN), buoyancy_factor=0, batch_size=batch_size) for _ in range(msteps) ]\n",
        "\n",
        "# velocity BC\n",
        "vn = np.zeros(source.velocity.data[0].data.shape)  # st.velocity.data[0] is considered as the velocity field in y axis!\n",
        "vn[..., 0:2, 0:vn.shape[2]-1, 0] = 1.0\n",
        "vn[..., 0:vn.shape[1], 0:1,   0] = 1.0\n",
        "vn[..., 0:vn.shape[1], -1:,   0] = 1.0\n",
        "velBCy = vn\n",
        "velBCyMask = np.copy(vn)  # warning, mask needs to be binary, 0..1, this only works if vel is also 1\n",
        "\n",
        "lr_in =   tf.placeholder(tf.float32, shape=[])  # learning rate\n",
        "Re_in =   tf.placeholder(tf.float32, shape=[batch_size])  # Reynolds numbers\n",
        "\n",
        "source_in =   phi.tf.util.placeholder_like(source)\n",
        "reference_in = [ phi.tf.util.placeholder_like(source) for _ in range(msteps) ]\n",
        "\n",
        "model = model_small(to_feature(source_in, Re_in)) # use small model for testing\n",
        "#model = model_medium(to_feature(source_in, Re_in)) # optionally switch to larger model\n",
        "model.summary() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (3, 64, 32, 32)           2432      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (3, 64, 32, 64)           51264     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (3, 64, 32, 2)            3202      \n",
            "=================================================================\n",
            "Total params: 56,898\n",
            "Trainable params: 56,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: placeholder_like may not respect the batch dimension. For State objects, use placeholder(state.shape) instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: placeholder_like may not respect the batch dimension. For State objects, use placeholder(state.shape) instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbpNPzplQZMF"
      },
      "source": [
        "finally, some sim steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5NeMcLGQaxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d0c73e-181a-477c-abb0-68024f4757b3"
      },
      "source": [
        "prediction, correction = [], []\n",
        "for i in range(msteps):\n",
        "    prediction += [\n",
        "        KarmanFlow().step(\n",
        "            source_in if i==0 else prediction[-1],\n",
        "            re=Re_in,\n",
        "            res=source_res[-1],  # reference resolution is size in x direction\n",
        "            velBCy=velBCy,\n",
        "            velBCyMask=velBCyMask\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    correction += [\n",
        "        to_staggered(\n",
        "            model(\n",
        "                to_feature(prediction[-1], Re_in)/[\n",
        "                    *(dataset.dataStats['std'][1]),  # velocity\n",
        "                    dataset.dataStats['ext.std'][0]  # Re\n",
        "                ]\n",
        "            ) * (dataset.dataStats['std'][1]),\n",
        "            box=source.velocity.box\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    prediction[-1] = prediction[-1].copied_with(velocity=prediction[-1].velocity + correction[-1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/tf_backend.py:213: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/tf_backend.py:167: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/tf_backend.py:61: The name tf.div_no_nan is deprecated. Please use tf.math.divide_no_nan instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4yLlDM3QfUR"
      },
      "source": [
        "loss calc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2JcO3-QQgC9"
      },
      "source": [
        "loss_steps = [\n",
        "    tf.nn.l2_loss(\n",
        "        (reference_in[i].velocity.staggered_tensor() - prediction[i].velocity.staggered_tensor())\n",
        "        /dataset.dataStats['std'][1]\n",
        "    )\n",
        "    for i in range(msteps)\n",
        "]\n",
        "loss = tf.reduce_sum(loss_steps)/msteps\n",
        "\n",
        "# skip ???\n",
        "# for i,a_step_loss in enumerate(loss_steps): tf.compat.v1.summary.scalar(name='loss_step{:02d}'.format(i), tensor=a_step_loss)\n",
        "# tf.compat.v1.summary.scalar(name='total_loss', tensor=loss)\n",
        "# tf.compat.v1.summary.scalar(name='lr', tensor=lr_in)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Vly1_0QhZ1"
      },
      "source": [
        "setup training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuljFamYQksW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43048d64-44e3-4454-8e5d-e9e534db76c3"
      },
      "source": [
        "lr = 1e-4\n",
        "adapt_lr = True\n",
        "resume = 0 # load model existing model?\n",
        "epochs = 10\n",
        "\n",
        "opt = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_in)\n",
        "train_step = opt.minimize(loss)\n",
        "\n",
        "tf_session = tf.Session() \n",
        "scene = Scene.create(output_dir, count=batch_size, mkdir=False, copy_calling_script=False)\n",
        "sess = Session(scene, session=tf_session)\n",
        "tf.compat.v1.keras.backend.set_session(tf_session)\n",
        "\n",
        "sess.initialize_variables()\n",
        "\n",
        "# optional, load existing model...\n",
        "if resume>0: \n",
        "    ld_model = keras.models.load_model(output_dir+'/model_epoch{:04d}.h5'.format(resume))\n",
        "    model.set_weights(ld_model.get_weights())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/session.py:28: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/phi/tf/session.py:29: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8hUXJDkRQST"
      },
      "source": [
        "another small helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am3hSdNgRPEh"
      },
      "source": [
        "\n",
        "def lr_schedule(epoch, current_lr):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 10, 15, 20, 22 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = current_lr\n",
        "    if   epoch == 23: lr *= 0.5\n",
        "    elif epoch == 21: lr *= 1e-1\n",
        "    elif epoch == 16: lr *= 1e-1\n",
        "    elif epoch == 11: lr *= 1e-1\n",
        "    return lr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrALctV1RWBO"
      },
      "source": [
        "finally, start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Nd8YyHRVFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e25b4f5-c865-488f-ad2b-b969b2dcc0a8"
      },
      "source": [
        "current_lr = lr\n",
        "steps = 0\n",
        "for j in range(epochs):  # training\n",
        "    dataset.newEpoch(exclude_tail=msteps)\n",
        "    if j<resume:\n",
        "        print('resume: skipping {} epoch'.format(j+1))\n",
        "        steps += dataset.numSteps*dataset.numBatches\n",
        "        continue\n",
        "\n",
        "    current_lr = lr_schedule(j, current_lr) if adapt_lr else lr\n",
        "    for ib in range(dataset.numBatches):   \n",
        "        for i in range(dataset.numSteps): \n",
        "            batch = getData(dataset, consecutive_frames=msteps) # should be dataset.getData\n",
        "            re_nr = batch[2] # Reynolds numbers\n",
        "            source = source.copied_with(density=batch[0][0], velocity=batch[1][0])\n",
        "            reference = [ reference[k].copied_with(density=batch[0][k+1], velocity=batch[1][k+1]) for k in range(msteps) ]\n",
        "\n",
        "            my_feed_dict = { source_in: source, Re_in: re_nr, lr_in: current_lr }\n",
        "            my_feed_dict.update(zip(reference_in, reference))\n",
        "            _, l2 = sess.run([train_step, loss], my_feed_dict)\n",
        "            steps += 1\n",
        "\n",
        "            print('epoch {:03d}/{:03d}, batch {:03d}/{:03d}, step {:04d}/{:04d}: loss={}'.format( j+1, epochs, ib+1, dataset.numBatches, i+1, dataset.numSteps, l2 ))\n",
        "            dataset.nextStep()\n",
        "\n",
        "        dataset.nextBatch()\n",
        "\n",
        "    if j%10==9: model.save(output_dir+'/model_epoch{:04d}.h5'.format(j+1))\n",
        "\n",
        "#tf_writer_tr.close()\n",
        "model.save(output_dir+'/model.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch 005/010, batch 002/002, step 0457/0496: loss=3.40366792678833\n",
            "epoch 005/010, batch 002/002, step 0458/0496: loss=3.3733911514282227\n",
            "epoch 005/010, batch 002/002, step 0459/0496: loss=2.1802964210510254\n",
            "epoch 005/010, batch 002/002, step 0460/0496: loss=2.727764844894409\n",
            "epoch 005/010, batch 002/002, step 0461/0496: loss=3.267439126968384\n",
            "epoch 005/010, batch 002/002, step 0462/0496: loss=2.7953271865844727\n",
            "epoch 005/010, batch 002/002, step 0463/0496: loss=3.1806530952453613\n",
            "epoch 005/010, batch 002/002, step 0464/0496: loss=4.665388107299805\n",
            "epoch 005/010, batch 002/002, step 0465/0496: loss=3.5460848808288574\n",
            "epoch 005/010, batch 002/002, step 0466/0496: loss=2.616530418395996\n",
            "epoch 005/010, batch 002/002, step 0467/0496: loss=2.7210044860839844\n",
            "epoch 005/010, batch 002/002, step 0468/0496: loss=3.0125303268432617\n",
            "epoch 005/010, batch 002/002, step 0469/0496: loss=2.7860662937164307\n",
            "epoch 005/010, batch 002/002, step 0470/0496: loss=2.895860195159912\n",
            "epoch 005/010, batch 002/002, step 0471/0496: loss=2.133047103881836\n",
            "epoch 005/010, batch 002/002, step 0472/0496: loss=2.8181519508361816\n",
            "epoch 005/010, batch 002/002, step 0473/0496: loss=2.7309253215789795\n",
            "epoch 005/010, batch 002/002, step 0474/0496: loss=3.3527283668518066\n",
            "epoch 005/010, batch 002/002, step 0475/0496: loss=2.447479724884033\n",
            "epoch 005/010, batch 002/002, step 0476/0496: loss=2.732964038848877\n",
            "epoch 005/010, batch 002/002, step 0477/0496: loss=3.3695316314697266\n",
            "epoch 005/010, batch 002/002, step 0478/0496: loss=3.5359339714050293\n",
            "epoch 005/010, batch 002/002, step 0479/0496: loss=2.6474087238311768\n",
            "epoch 005/010, batch 002/002, step 0480/0496: loss=2.8565104007720947\n",
            "epoch 005/010, batch 002/002, step 0481/0496: loss=2.527487277984619\n",
            "epoch 005/010, batch 002/002, step 0482/0496: loss=3.001659631729126\n",
            "epoch 005/010, batch 002/002, step 0483/0496: loss=2.606401205062866\n",
            "epoch 005/010, batch 002/002, step 0484/0496: loss=3.7144875526428223\n",
            "epoch 005/010, batch 002/002, step 0485/0496: loss=2.424988031387329\n",
            "epoch 005/010, batch 002/002, step 0486/0496: loss=2.378692150115967\n",
            "epoch 005/010, batch 002/002, step 0487/0496: loss=2.9622061252593994\n",
            "epoch 005/010, batch 002/002, step 0488/0496: loss=4.112605094909668\n",
            "epoch 005/010, batch 002/002, step 0489/0496: loss=3.771249294281006\n",
            "epoch 005/010, batch 002/002, step 0490/0496: loss=5.516016960144043\n",
            "epoch 005/010, batch 002/002, step 0491/0496: loss=3.351088523864746\n",
            "epoch 005/010, batch 002/002, step 0492/0496: loss=3.574984550476074\n",
            "epoch 005/010, batch 002/002, step 0493/0496: loss=2.8863203525543213\n",
            "epoch 005/010, batch 002/002, step 0494/0496: loss=2.743931770324707\n",
            "epoch 005/010, batch 002/002, step 0495/0496: loss=3.364583730697632\n",
            "epoch 005/010, batch 002/002, step 0496/0496: loss=2.705111026763916\n",
            "epoch 006/010, batch 001/002, step 0001/0496: loss=4.141987323760986\n",
            "epoch 006/010, batch 001/002, step 0002/0496: loss=2.942171812057495\n",
            "epoch 006/010, batch 001/002, step 0003/0496: loss=4.1793622970581055\n",
            "epoch 006/010, batch 001/002, step 0004/0496: loss=6.962525367736816\n",
            "epoch 006/010, batch 001/002, step 0005/0496: loss=3.460615396499634\n",
            "epoch 006/010, batch 001/002, step 0006/0496: loss=5.6815385818481445\n",
            "epoch 006/010, batch 001/002, step 0007/0496: loss=3.3532779216766357\n",
            "epoch 006/010, batch 001/002, step 0008/0496: loss=3.839423179626465\n",
            "epoch 006/010, batch 001/002, step 0009/0496: loss=3.948444128036499\n",
            "epoch 006/010, batch 001/002, step 0010/0496: loss=4.061919689178467\n",
            "epoch 006/010, batch 001/002, step 0011/0496: loss=3.914436101913452\n",
            "epoch 006/010, batch 001/002, step 0012/0496: loss=3.749380588531494\n",
            "epoch 006/010, batch 001/002, step 0013/0496: loss=4.066138744354248\n",
            "epoch 006/010, batch 001/002, step 0014/0496: loss=3.0905418395996094\n",
            "epoch 006/010, batch 001/002, step 0015/0496: loss=3.2849488258361816\n",
            "epoch 006/010, batch 001/002, step 0016/0496: loss=2.9378480911254883\n",
            "epoch 006/010, batch 001/002, step 0017/0496: loss=3.7614877223968506\n",
            "epoch 006/010, batch 001/002, step 0018/0496: loss=2.71474027633667\n",
            "epoch 006/010, batch 001/002, step 0019/0496: loss=2.7261548042297363\n",
            "epoch 006/010, batch 001/002, step 0020/0496: loss=2.3351120948791504\n",
            "epoch 006/010, batch 001/002, step 0021/0496: loss=3.5571351051330566\n",
            "epoch 006/010, batch 001/002, step 0022/0496: loss=3.136251211166382\n",
            "epoch 006/010, batch 001/002, step 0023/0496: loss=2.326748847961426\n",
            "epoch 006/010, batch 001/002, step 0024/0496: loss=3.132941722869873\n",
            "epoch 006/010, batch 001/002, step 0025/0496: loss=2.905923366546631\n",
            "epoch 006/010, batch 001/002, step 0026/0496: loss=2.5012669563293457\n",
            "epoch 006/010, batch 001/002, step 0027/0496: loss=3.010108232498169\n",
            "epoch 006/010, batch 001/002, step 0028/0496: loss=2.815303325653076\n",
            "epoch 006/010, batch 001/002, step 0029/0496: loss=3.091580390930176\n",
            "epoch 006/010, batch 001/002, step 0030/0496: loss=2.546401023864746\n",
            "epoch 006/010, batch 001/002, step 0031/0496: loss=5.951359748840332\n",
            "epoch 006/010, batch 001/002, step 0032/0496: loss=2.7487409114837646\n",
            "epoch 006/010, batch 001/002, step 0033/0496: loss=5.455943584442139\n",
            "epoch 006/010, batch 001/002, step 0034/0496: loss=2.4361777305603027\n",
            "epoch 006/010, batch 001/002, step 0035/0496: loss=5.581273078918457\n",
            "epoch 006/010, batch 001/002, step 0036/0496: loss=3.203500270843506\n",
            "epoch 006/010, batch 001/002, step 0037/0496: loss=7.269601821899414\n",
            "epoch 006/010, batch 001/002, step 0038/0496: loss=2.5769524574279785\n",
            "epoch 006/010, batch 001/002, step 0039/0496: loss=3.819176197052002\n",
            "epoch 006/010, batch 001/002, step 0040/0496: loss=3.518941879272461\n",
            "epoch 006/010, batch 001/002, step 0041/0496: loss=3.744182825088501\n",
            "epoch 006/010, batch 001/002, step 0042/0496: loss=2.9447479248046875\n",
            "epoch 006/010, batch 001/002, step 0043/0496: loss=3.7902300357818604\n",
            "epoch 006/010, batch 001/002, step 0044/0496: loss=2.5692362785339355\n",
            "epoch 006/010, batch 001/002, step 0045/0496: loss=3.3401832580566406\n",
            "epoch 006/010, batch 001/002, step 0046/0496: loss=4.0417633056640625\n",
            "epoch 006/010, batch 001/002, step 0047/0496: loss=5.140334606170654\n",
            "epoch 006/010, batch 001/002, step 0048/0496: loss=3.9514589309692383\n",
            "epoch 006/010, batch 001/002, step 0049/0496: loss=2.8547286987304688\n",
            "epoch 006/010, batch 001/002, step 0050/0496: loss=2.851580858230591\n",
            "epoch 006/010, batch 001/002, step 0051/0496: loss=3.6282010078430176\n",
            "epoch 006/010, batch 001/002, step 0052/0496: loss=2.1970198154449463\n",
            "epoch 006/010, batch 001/002, step 0053/0496: loss=4.653295516967773\n",
            "epoch 006/010, batch 001/002, step 0054/0496: loss=2.9918007850646973\n",
            "epoch 006/010, batch 001/002, step 0055/0496: loss=4.962690353393555\n",
            "epoch 006/010, batch 001/002, step 0056/0496: loss=3.5648038387298584\n",
            "epoch 006/010, batch 001/002, step 0057/0496: loss=4.287603855133057\n",
            "epoch 006/010, batch 001/002, step 0058/0496: loss=3.249795436859131\n",
            "epoch 006/010, batch 001/002, step 0059/0496: loss=3.4288742542266846\n",
            "epoch 006/010, batch 001/002, step 0060/0496: loss=2.784031391143799\n",
            "epoch 006/010, batch 001/002, step 0061/0496: loss=3.6517651081085205\n",
            "epoch 006/010, batch 001/002, step 0062/0496: loss=3.3750457763671875\n",
            "epoch 006/010, batch 001/002, step 0063/0496: loss=2.942561388015747\n",
            "epoch 006/010, batch 001/002, step 0064/0496: loss=3.2440247535705566\n",
            "epoch 006/010, batch 001/002, step 0065/0496: loss=2.165820360183716\n",
            "epoch 006/010, batch 001/002, step 0066/0496: loss=2.1700217723846436\n",
            "epoch 006/010, batch 001/002, step 0067/0496: loss=3.3015079498291016\n",
            "epoch 006/010, batch 001/002, step 0068/0496: loss=2.802720308303833\n",
            "epoch 006/010, batch 001/002, step 0069/0496: loss=2.682478904724121\n",
            "epoch 006/010, batch 001/002, step 0070/0496: loss=3.4656686782836914\n",
            "epoch 006/010, batch 001/002, step 0071/0496: loss=2.2827625274658203\n",
            "epoch 006/010, batch 001/002, step 0072/0496: loss=3.161332607269287\n",
            "epoch 006/010, batch 001/002, step 0073/0496: loss=3.8452260494232178\n",
            "epoch 006/010, batch 001/002, step 0074/0496: loss=2.915557861328125\n",
            "epoch 006/010, batch 001/002, step 0075/0496: loss=2.800044059753418\n",
            "epoch 006/010, batch 001/002, step 0076/0496: loss=2.664307117462158\n",
            "epoch 006/010, batch 001/002, step 0077/0496: loss=2.5379419326782227\n",
            "epoch 006/010, batch 001/002, step 0078/0496: loss=2.3943262100219727\n",
            "epoch 006/010, batch 001/002, step 0079/0496: loss=3.150108814239502\n",
            "epoch 006/010, batch 001/002, step 0080/0496: loss=2.6150925159454346\n",
            "epoch 006/010, batch 001/002, step 0081/0496: loss=3.3007359504699707\n",
            "epoch 006/010, batch 001/002, step 0082/0496: loss=2.710857391357422\n",
            "epoch 006/010, batch 001/002, step 0083/0496: loss=2.228480339050293\n",
            "epoch 006/010, batch 001/002, step 0084/0496: loss=2.2008109092712402\n",
            "epoch 006/010, batch 001/002, step 0085/0496: loss=3.2038216590881348\n",
            "epoch 006/010, batch 001/002, step 0086/0496: loss=3.1041417121887207\n",
            "epoch 006/010, batch 001/002, step 0087/0496: loss=2.8284575939178467\n",
            "epoch 006/010, batch 001/002, step 0088/0496: loss=3.17734956741333\n",
            "epoch 006/010, batch 001/002, step 0089/0496: loss=3.024416208267212\n",
            "epoch 006/010, batch 001/002, step 0090/0496: loss=2.307509422302246\n",
            "epoch 006/010, batch 001/002, step 0091/0496: loss=3.261718511581421\n",
            "epoch 006/010, batch 001/002, step 0092/0496: loss=2.6782045364379883\n",
            "epoch 006/010, batch 001/002, step 0093/0496: loss=2.1918716430664062\n",
            "epoch 006/010, batch 001/002, step 0094/0496: loss=2.639286756515503\n",
            "epoch 006/010, batch 001/002, step 0095/0496: loss=3.086291790008545\n",
            "epoch 006/010, batch 001/002, step 0096/0496: loss=3.2065820693969727\n",
            "epoch 006/010, batch 001/002, step 0097/0496: loss=2.2984619140625\n",
            "epoch 006/010, batch 001/002, step 0098/0496: loss=2.806384563446045\n",
            "epoch 006/010, batch 001/002, step 0099/0496: loss=3.256448268890381\n",
            "epoch 006/010, batch 001/002, step 0100/0496: loss=2.2684967517852783\n",
            "epoch 006/010, batch 001/002, step 0101/0496: loss=3.5403740406036377\n",
            "epoch 006/010, batch 001/002, step 0102/0496: loss=3.076650619506836\n",
            "epoch 006/010, batch 001/002, step 0103/0496: loss=3.6326560974121094\n",
            "epoch 006/010, batch 001/002, step 0104/0496: loss=2.7596194744110107\n",
            "epoch 006/010, batch 001/002, step 0105/0496: loss=2.261136770248413\n",
            "epoch 006/010, batch 001/002, step 0106/0496: loss=3.072129487991333\n",
            "epoch 006/010, batch 001/002, step 0107/0496: loss=2.8370308876037598\n",
            "epoch 006/010, batch 001/002, step 0108/0496: loss=2.47582745552063\n",
            "epoch 006/010, batch 001/002, step 0109/0496: loss=4.111205101013184\n",
            "epoch 006/010, batch 001/002, step 0110/0496: loss=3.318645477294922\n",
            "epoch 006/010, batch 001/002, step 0111/0496: loss=4.5399017333984375\n",
            "epoch 006/010, batch 001/002, step 0112/0496: loss=3.143967390060425\n",
            "epoch 006/010, batch 001/002, step 0113/0496: loss=3.06925106048584\n",
            "epoch 006/010, batch 001/002, step 0114/0496: loss=2.957818031311035\n",
            "epoch 006/010, batch 001/002, step 0115/0496: loss=3.495938777923584\n",
            "epoch 006/010, batch 001/002, step 0116/0496: loss=3.279869556427002\n",
            "epoch 006/010, batch 001/002, step 0117/0496: loss=3.468461751937866\n",
            "epoch 006/010, batch 001/002, step 0118/0496: loss=3.23148512840271\n",
            "epoch 006/010, batch 001/002, step 0119/0496: loss=2.340902328491211\n",
            "epoch 006/010, batch 001/002, step 0120/0496: loss=3.386626958847046\n",
            "epoch 006/010, batch 001/002, step 0121/0496: loss=2.4375834465026855\n",
            "epoch 006/010, batch 001/002, step 0122/0496: loss=3.6692397594451904\n",
            "epoch 006/010, batch 001/002, step 0123/0496: loss=2.911038875579834\n",
            "epoch 006/010, batch 001/002, step 0124/0496: loss=3.316603422164917\n",
            "epoch 006/010, batch 001/002, step 0125/0496: loss=2.572526454925537\n",
            "epoch 006/010, batch 001/002, step 0126/0496: loss=3.183711051940918\n",
            "epoch 006/010, batch 001/002, step 0127/0496: loss=3.1173043251037598\n",
            "epoch 006/010, batch 001/002, step 0128/0496: loss=4.11005973815918\n",
            "epoch 006/010, batch 001/002, step 0129/0496: loss=2.8621721267700195\n",
            "epoch 006/010, batch 001/002, step 0130/0496: loss=2.2768898010253906\n",
            "epoch 006/010, batch 001/002, step 0131/0496: loss=3.6326441764831543\n",
            "epoch 006/010, batch 001/002, step 0132/0496: loss=2.8563270568847656\n",
            "epoch 006/010, batch 001/002, step 0133/0496: loss=3.2921438217163086\n",
            "epoch 006/010, batch 001/002, step 0134/0496: loss=2.316920757293701\n",
            "epoch 006/010, batch 001/002, step 0135/0496: loss=6.190503120422363\n",
            "epoch 006/010, batch 001/002, step 0136/0496: loss=2.9392571449279785\n",
            "epoch 006/010, batch 001/002, step 0137/0496: loss=3.833662271499634\n",
            "epoch 006/010, batch 001/002, step 0138/0496: loss=2.724541664123535\n",
            "epoch 006/010, batch 001/002, step 0139/0496: loss=3.8267769813537598\n",
            "epoch 006/010, batch 001/002, step 0140/0496: loss=3.3003792762756348\n",
            "epoch 006/010, batch 001/002, step 0141/0496: loss=2.659027099609375\n",
            "epoch 006/010, batch 001/002, step 0142/0496: loss=2.727725028991699\n",
            "epoch 006/010, batch 001/002, step 0143/0496: loss=2.9736950397491455\n",
            "epoch 006/010, batch 001/002, step 0144/0496: loss=3.462409496307373\n",
            "epoch 006/010, batch 001/002, step 0145/0496: loss=3.5809173583984375\n",
            "epoch 006/010, batch 001/002, step 0146/0496: loss=3.7889509201049805\n",
            "epoch 006/010, batch 001/002, step 0147/0496: loss=2.3776583671569824\n",
            "epoch 006/010, batch 001/002, step 0148/0496: loss=2.542095899581909\n",
            "epoch 006/010, batch 001/002, step 0149/0496: loss=2.4046998023986816\n",
            "epoch 006/010, batch 001/002, step 0150/0496: loss=2.0901756286621094\n",
            "epoch 006/010, batch 001/002, step 0151/0496: loss=3.5479583740234375\n",
            "epoch 006/010, batch 001/002, step 0152/0496: loss=3.237281322479248\n",
            "epoch 006/010, batch 001/002, step 0153/0496: loss=2.8620426654815674\n",
            "epoch 006/010, batch 001/002, step 0154/0496: loss=6.67839241027832\n",
            "epoch 006/010, batch 001/002, step 0155/0496: loss=2.6954126358032227\n",
            "epoch 006/010, batch 001/002, step 0156/0496: loss=3.4950337409973145\n",
            "epoch 006/010, batch 001/002, step 0157/0496: loss=3.666013479232788\n",
            "epoch 006/010, batch 001/002, step 0158/0496: loss=4.614336967468262\n",
            "epoch 006/010, batch 001/002, step 0159/0496: loss=3.751404285430908\n",
            "epoch 006/010, batch 001/002, step 0160/0496: loss=4.477474689483643\n",
            "epoch 006/010, batch 001/002, step 0161/0496: loss=3.840360164642334\n",
            "epoch 006/010, batch 001/002, step 0162/0496: loss=4.520349502563477\n",
            "epoch 006/010, batch 001/002, step 0163/0496: loss=2.9776763916015625\n",
            "epoch 006/010, batch 001/002, step 0164/0496: loss=4.882489204406738\n",
            "epoch 006/010, batch 001/002, step 0165/0496: loss=2.7258071899414062\n",
            "epoch 006/010, batch 001/002, step 0166/0496: loss=5.26416015625\n",
            "epoch 006/010, batch 001/002, step 0167/0496: loss=3.4169700145721436\n",
            "epoch 006/010, batch 001/002, step 0168/0496: loss=6.53307580947876\n",
            "epoch 006/010, batch 001/002, step 0169/0496: loss=3.530641555786133\n",
            "epoch 006/010, batch 001/002, step 0170/0496: loss=5.68892765045166\n",
            "epoch 006/010, batch 001/002, step 0171/0496: loss=3.2315759658813477\n",
            "epoch 006/010, batch 001/002, step 0172/0496: loss=6.857489585876465\n",
            "epoch 006/010, batch 001/002, step 0173/0496: loss=2.9983603954315186\n",
            "epoch 006/010, batch 001/002, step 0174/0496: loss=8.44678783416748\n",
            "epoch 006/010, batch 001/002, step 0175/0496: loss=4.252069473266602\n",
            "epoch 006/010, batch 001/002, step 0176/0496: loss=5.420839309692383\n",
            "epoch 006/010, batch 001/002, step 0177/0496: loss=3.2568368911743164\n",
            "epoch 006/010, batch 001/002, step 0178/0496: loss=4.732369422912598\n",
            "epoch 006/010, batch 001/002, step 0179/0496: loss=3.8298401832580566\n",
            "epoch 006/010, batch 001/002, step 0180/0496: loss=2.7415521144866943\n",
            "epoch 006/010, batch 001/002, step 0181/0496: loss=4.18326997756958\n",
            "epoch 006/010, batch 001/002, step 0182/0496: loss=2.7263505458831787\n",
            "epoch 006/010, batch 001/002, step 0183/0496: loss=3.7893729209899902\n",
            "epoch 006/010, batch 001/002, step 0184/0496: loss=4.556839466094971\n",
            "epoch 006/010, batch 001/002, step 0185/0496: loss=3.235093593597412\n",
            "epoch 006/010, batch 001/002, step 0186/0496: loss=4.004612922668457\n",
            "epoch 006/010, batch 001/002, step 0187/0496: loss=5.0364251136779785\n",
            "epoch 006/010, batch 001/002, step 0188/0496: loss=8.57345199584961\n",
            "epoch 006/010, batch 001/002, step 0189/0496: loss=8.277835845947266\n",
            "epoch 006/010, batch 001/002, step 0190/0496: loss=3.615849018096924\n",
            "epoch 006/010, batch 001/002, step 0191/0496: loss=8.774654388427734\n",
            "epoch 006/010, batch 001/002, step 0192/0496: loss=4.726734161376953\n",
            "epoch 006/010, batch 001/002, step 0193/0496: loss=3.0171191692352295\n",
            "epoch 006/010, batch 001/002, step 0194/0496: loss=2.6419224739074707\n",
            "epoch 006/010, batch 001/002, step 0195/0496: loss=2.336636781692505\n",
            "epoch 006/010, batch 001/002, step 0196/0496: loss=3.2271087169647217\n",
            "epoch 006/010, batch 001/002, step 0197/0496: loss=2.9059970378875732\n",
            "epoch 006/010, batch 001/002, step 0198/0496: loss=2.96565318107605\n",
            "epoch 006/010, batch 001/002, step 0199/0496: loss=3.680079698562622\n",
            "epoch 006/010, batch 001/002, step 0200/0496: loss=2.3781113624572754\n",
            "epoch 006/010, batch 001/002, step 0201/0496: loss=2.8053789138793945\n",
            "epoch 006/010, batch 001/002, step 0202/0496: loss=3.196991443634033\n",
            "epoch 006/010, batch 001/002, step 0203/0496: loss=3.591531753540039\n",
            "epoch 006/010, batch 001/002, step 0204/0496: loss=2.8812808990478516\n",
            "epoch 006/010, batch 001/002, step 0205/0496: loss=2.7436561584472656\n",
            "epoch 006/010, batch 001/002, step 0206/0496: loss=3.1703412532806396\n",
            "epoch 006/010, batch 001/002, step 0207/0496: loss=3.21755051612854\n",
            "epoch 006/010, batch 001/002, step 0208/0496: loss=2.8789098262786865\n",
            "epoch 006/010, batch 001/002, step 0209/0496: loss=2.5975892543792725\n",
            "epoch 006/010, batch 001/002, step 0210/0496: loss=2.2624216079711914\n",
            "epoch 006/010, batch 001/002, step 0211/0496: loss=5.030202865600586\n",
            "epoch 006/010, batch 001/002, step 0212/0496: loss=2.5496459007263184\n",
            "epoch 006/010, batch 001/002, step 0213/0496: loss=2.203122854232788\n",
            "epoch 006/010, batch 001/002, step 0214/0496: loss=4.782225131988525\n",
            "epoch 006/010, batch 001/002, step 0215/0496: loss=3.053659677505493\n",
            "epoch 006/010, batch 001/002, step 0216/0496: loss=3.4735941886901855\n",
            "epoch 006/010, batch 001/002, step 0217/0496: loss=3.189452648162842\n",
            "epoch 006/010, batch 001/002, step 0218/0496: loss=4.802129745483398\n",
            "epoch 006/010, batch 001/002, step 0219/0496: loss=2.946859836578369\n",
            "epoch 006/010, batch 001/002, step 0220/0496: loss=4.0841264724731445\n",
            "epoch 006/010, batch 001/002, step 0221/0496: loss=3.3578553199768066\n",
            "epoch 006/010, batch 001/002, step 0222/0496: loss=3.772022247314453\n",
            "epoch 006/010, batch 001/002, step 0223/0496: loss=3.8249258995056152\n",
            "epoch 006/010, batch 001/002, step 0224/0496: loss=2.613456964492798\n",
            "epoch 006/010, batch 001/002, step 0225/0496: loss=3.834038734436035\n",
            "epoch 006/010, batch 001/002, step 0226/0496: loss=2.8758468627929688\n",
            "epoch 006/010, batch 001/002, step 0227/0496: loss=4.091275215148926\n",
            "epoch 006/010, batch 001/002, step 0228/0496: loss=3.0274927616119385\n",
            "epoch 006/010, batch 001/002, step 0229/0496: loss=4.16021728515625\n",
            "epoch 006/010, batch 001/002, step 0230/0496: loss=3.792301654815674\n",
            "epoch 006/010, batch 001/002, step 0231/0496: loss=6.57470178604126\n",
            "epoch 006/010, batch 001/002, step 0232/0496: loss=3.5287954807281494\n",
            "epoch 006/010, batch 001/002, step 0233/0496: loss=3.774705410003662\n",
            "epoch 006/010, batch 001/002, step 0234/0496: loss=2.5212724208831787\n",
            "epoch 006/010, batch 001/002, step 0235/0496: loss=4.063816070556641\n",
            "epoch 006/010, batch 001/002, step 0236/0496: loss=3.22210693359375\n",
            "epoch 006/010, batch 001/002, step 0237/0496: loss=3.3374571800231934\n",
            "epoch 006/010, batch 001/002, step 0238/0496: loss=3.499851703643799\n",
            "epoch 006/010, batch 001/002, step 0239/0496: loss=2.886431932449341\n",
            "epoch 006/010, batch 001/002, step 0240/0496: loss=3.787001132965088\n",
            "epoch 006/010, batch 001/002, step 0241/0496: loss=3.13826584815979\n",
            "epoch 006/010, batch 001/002, step 0242/0496: loss=3.1153359413146973\n",
            "epoch 006/010, batch 001/002, step 0243/0496: loss=3.043074131011963\n",
            "epoch 006/010, batch 001/002, step 0244/0496: loss=2.4863905906677246\n",
            "epoch 006/010, batch 001/002, step 0245/0496: loss=2.4966273307800293\n",
            "epoch 006/010, batch 001/002, step 0246/0496: loss=3.0206971168518066\n",
            "epoch 006/010, batch 001/002, step 0247/0496: loss=2.567898988723755\n",
            "epoch 006/010, batch 001/002, step 0248/0496: loss=3.335394859313965\n",
            "epoch 006/010, batch 001/002, step 0249/0496: loss=2.4608583450317383\n",
            "epoch 006/010, batch 001/002, step 0250/0496: loss=2.58752179145813\n",
            "epoch 006/010, batch 001/002, step 0251/0496: loss=2.991891622543335\n",
            "epoch 006/010, batch 001/002, step 0252/0496: loss=3.003514289855957\n",
            "epoch 006/010, batch 001/002, step 0253/0496: loss=3.6702733039855957\n",
            "epoch 006/010, batch 001/002, step 0254/0496: loss=3.137457847595215\n",
            "epoch 006/010, batch 001/002, step 0255/0496: loss=3.1884543895721436\n",
            "epoch 006/010, batch 001/002, step 0256/0496: loss=2.857016086578369\n",
            "epoch 006/010, batch 001/002, step 0257/0496: loss=4.839898109436035\n",
            "epoch 006/010, batch 001/002, step 0258/0496: loss=2.9258806705474854\n",
            "epoch 006/010, batch 001/002, step 0259/0496: loss=3.694371223449707\n",
            "epoch 006/010, batch 001/002, step 0260/0496: loss=2.814619541168213\n",
            "epoch 006/010, batch 001/002, step 0261/0496: loss=2.6586155891418457\n",
            "epoch 006/010, batch 001/002, step 0262/0496: loss=2.1493043899536133\n",
            "epoch 006/010, batch 001/002, step 0263/0496: loss=2.354193687438965\n",
            "epoch 006/010, batch 001/002, step 0264/0496: loss=2.2050952911376953\n",
            "epoch 006/010, batch 001/002, step 0265/0496: loss=3.465102195739746\n",
            "epoch 006/010, batch 001/002, step 0266/0496: loss=2.930335283279419\n",
            "epoch 006/010, batch 001/002, step 0267/0496: loss=4.398571014404297\n",
            "epoch 006/010, batch 001/002, step 0268/0496: loss=2.7605645656585693\n",
            "epoch 006/010, batch 001/002, step 0269/0496: loss=1.9839519262313843\n",
            "epoch 006/010, batch 001/002, step 0270/0496: loss=2.089200973510742\n",
            "epoch 006/010, batch 001/002, step 0271/0496: loss=2.52589750289917\n",
            "epoch 006/010, batch 001/002, step 0272/0496: loss=2.267821788787842\n",
            "epoch 006/010, batch 001/002, step 0273/0496: loss=2.607205390930176\n",
            "epoch 006/010, batch 001/002, step 0274/0496: loss=3.052048444747925\n",
            "epoch 006/010, batch 001/002, step 0275/0496: loss=3.42698335647583\n",
            "epoch 006/010, batch 001/002, step 0276/0496: loss=3.1162195205688477\n",
            "epoch 006/010, batch 001/002, step 0277/0496: loss=2.6138768196105957\n",
            "epoch 006/010, batch 001/002, step 0278/0496: loss=2.8346807956695557\n",
            "epoch 006/010, batch 001/002, step 0279/0496: loss=3.4462735652923584\n",
            "epoch 006/010, batch 001/002, step 0280/0496: loss=2.7356529235839844\n",
            "epoch 006/010, batch 001/002, step 0281/0496: loss=3.3254036903381348\n",
            "epoch 006/010, batch 001/002, step 0282/0496: loss=3.0148396492004395\n",
            "epoch 006/010, batch 001/002, step 0283/0496: loss=2.1319639682769775\n",
            "epoch 006/010, batch 001/002, step 0284/0496: loss=3.2478384971618652\n",
            "epoch 006/010, batch 001/002, step 0285/0496: loss=2.604459524154663\n",
            "epoch 006/010, batch 001/002, step 0286/0496: loss=2.2809975147247314\n",
            "epoch 006/010, batch 001/002, step 0287/0496: loss=2.1011035442352295\n",
            "epoch 006/010, batch 001/002, step 0288/0496: loss=2.476151466369629\n",
            "epoch 006/010, batch 001/002, step 0289/0496: loss=3.6584436893463135\n",
            "epoch 006/010, batch 001/002, step 0290/0496: loss=3.484419107437134\n",
            "epoch 006/010, batch 001/002, step 0291/0496: loss=2.196037769317627\n",
            "epoch 006/010, batch 001/002, step 0292/0496: loss=3.185328245162964\n",
            "epoch 006/010, batch 001/002, step 0293/0496: loss=2.266937732696533\n",
            "epoch 006/010, batch 001/002, step 0294/0496: loss=5.234402656555176\n",
            "epoch 006/010, batch 001/002, step 0295/0496: loss=2.50986385345459\n",
            "epoch 006/010, batch 001/002, step 0296/0496: loss=2.190877676010132\n",
            "epoch 006/010, batch 001/002, step 0297/0496: loss=3.252535343170166\n",
            "epoch 006/010, batch 001/002, step 0298/0496: loss=3.321744918823242\n",
            "epoch 006/010, batch 001/002, step 0299/0496: loss=2.0763797760009766\n",
            "epoch 006/010, batch 001/002, step 0300/0496: loss=2.42641019821167\n",
            "epoch 006/010, batch 001/002, step 0301/0496: loss=2.813300371170044\n",
            "epoch 006/010, batch 001/002, step 0302/0496: loss=2.464545726776123\n",
            "epoch 006/010, batch 001/002, step 0303/0496: loss=2.8791942596435547\n",
            "epoch 006/010, batch 001/002, step 0304/0496: loss=2.581035614013672\n",
            "epoch 006/010, batch 001/002, step 0305/0496: loss=3.3443431854248047\n",
            "epoch 006/010, batch 001/002, step 0306/0496: loss=2.907550811767578\n",
            "epoch 006/010, batch 001/002, step 0307/0496: loss=2.8294625282287598\n",
            "epoch 006/010, batch 001/002, step 0308/0496: loss=2.958070755004883\n",
            "epoch 006/010, batch 001/002, step 0309/0496: loss=2.8386921882629395\n",
            "epoch 006/010, batch 001/002, step 0310/0496: loss=2.985109329223633\n",
            "epoch 006/010, batch 001/002, step 0311/0496: loss=3.5718994140625\n",
            "epoch 006/010, batch 001/002, step 0312/0496: loss=2.5486433506011963\n",
            "epoch 006/010, batch 001/002, step 0313/0496: loss=4.112547874450684\n",
            "epoch 006/010, batch 001/002, step 0314/0496: loss=3.547774076461792\n",
            "epoch 006/010, batch 001/002, step 0315/0496: loss=2.8631577491760254\n",
            "epoch 006/010, batch 001/002, step 0316/0496: loss=2.173762083053589\n",
            "epoch 006/010, batch 001/002, step 0317/0496: loss=2.5373101234436035\n",
            "epoch 006/010, batch 001/002, step 0318/0496: loss=2.981606960296631\n",
            "epoch 006/010, batch 001/002, step 0319/0496: loss=2.242804527282715\n",
            "epoch 006/010, batch 001/002, step 0320/0496: loss=2.427959442138672\n",
            "epoch 006/010, batch 001/002, step 0321/0496: loss=2.7811336517333984\n",
            "epoch 006/010, batch 001/002, step 0322/0496: loss=2.8742196559906006\n",
            "epoch 006/010, batch 001/002, step 0323/0496: loss=2.4898931980133057\n",
            "epoch 006/010, batch 001/002, step 0324/0496: loss=2.8832345008850098\n",
            "epoch 006/010, batch 001/002, step 0325/0496: loss=2.454150915145874\n",
            "epoch 006/010, batch 001/002, step 0326/0496: loss=3.065742254257202\n",
            "epoch 006/010, batch 001/002, step 0327/0496: loss=2.1586551666259766\n",
            "epoch 006/010, batch 001/002, step 0328/0496: loss=2.617898464202881\n",
            "epoch 006/010, batch 001/002, step 0329/0496: loss=2.8274102210998535\n",
            "epoch 006/010, batch 001/002, step 0330/0496: loss=2.703273296356201\n",
            "epoch 006/010, batch 001/002, step 0331/0496: loss=3.112509250640869\n",
            "epoch 006/010, batch 001/002, step 0332/0496: loss=3.158940076828003\n",
            "epoch 006/010, batch 001/002, step 0333/0496: loss=2.625547409057617\n",
            "epoch 006/010, batch 001/002, step 0334/0496: loss=2.3505373001098633\n",
            "epoch 006/010, batch 001/002, step 0335/0496: loss=2.8040425777435303\n",
            "epoch 006/010, batch 001/002, step 0336/0496: loss=2.4933390617370605\n",
            "epoch 006/010, batch 001/002, step 0337/0496: loss=4.215563774108887\n",
            "epoch 006/010, batch 001/002, step 0338/0496: loss=2.711347818374634\n",
            "epoch 006/010, batch 001/002, step 0339/0496: loss=1.8314428329467773\n",
            "epoch 006/010, batch 001/002, step 0340/0496: loss=2.0368497371673584\n",
            "epoch 006/010, batch 001/002, step 0341/0496: loss=4.087263107299805\n",
            "epoch 006/010, batch 001/002, step 0342/0496: loss=2.8367528915405273\n",
            "epoch 006/010, batch 001/002, step 0343/0496: loss=2.199049949645996\n",
            "epoch 006/010, batch 001/002, step 0344/0496: loss=5.074225902557373\n",
            "epoch 006/010, batch 001/002, step 0345/0496: loss=2.5155413150787354\n",
            "epoch 006/010, batch 001/002, step 0346/0496: loss=3.1077499389648438\n",
            "epoch 006/010, batch 001/002, step 0347/0496: loss=2.3964731693267822\n",
            "epoch 006/010, batch 001/002, step 0348/0496: loss=2.664707899093628\n",
            "epoch 006/010, batch 001/002, step 0349/0496: loss=2.5137102603912354\n",
            "epoch 006/010, batch 001/002, step 0350/0496: loss=3.4241843223571777\n",
            "epoch 006/010, batch 001/002, step 0351/0496: loss=2.7859745025634766\n",
            "epoch 006/010, batch 001/002, step 0352/0496: loss=2.7251524925231934\n",
            "epoch 006/010, batch 001/002, step 0353/0496: loss=2.2740097045898438\n",
            "epoch 006/010, batch 001/002, step 0354/0496: loss=2.6019372940063477\n",
            "epoch 006/010, batch 001/002, step 0355/0496: loss=4.632555961608887\n",
            "epoch 006/010, batch 001/002, step 0356/0496: loss=2.2467598915100098\n",
            "epoch 006/010, batch 001/002, step 0357/0496: loss=2.3011012077331543\n",
            "epoch 006/010, batch 001/002, step 0358/0496: loss=2.4462785720825195\n",
            "epoch 006/010, batch 001/002, step 0359/0496: loss=2.5846660137176514\n",
            "epoch 006/010, batch 001/002, step 0360/0496: loss=2.765536069869995\n",
            "epoch 006/010, batch 001/002, step 0361/0496: loss=2.4601078033447266\n",
            "epoch 006/010, batch 001/002, step 0362/0496: loss=2.447911500930786\n",
            "epoch 006/010, batch 001/002, step 0363/0496: loss=2.5838258266448975\n",
            "epoch 006/010, batch 001/002, step 0364/0496: loss=2.2448439598083496\n",
            "epoch 006/010, batch 001/002, step 0365/0496: loss=2.4234259128570557\n",
            "epoch 006/010, batch 001/002, step 0366/0496: loss=3.2101051807403564\n",
            "epoch 006/010, batch 001/002, step 0367/0496: loss=2.325395107269287\n",
            "epoch 006/010, batch 001/002, step 0368/0496: loss=3.117647171020508\n",
            "epoch 006/010, batch 001/002, step 0369/0496: loss=2.0559661388397217\n",
            "epoch 006/010, batch 001/002, step 0370/0496: loss=2.4839982986450195\n",
            "epoch 006/010, batch 001/002, step 0371/0496: loss=3.0028457641601562\n",
            "epoch 006/010, batch 001/002, step 0372/0496: loss=3.151820421218872\n",
            "epoch 006/010, batch 001/002, step 0373/0496: loss=3.2029943466186523\n",
            "epoch 006/010, batch 001/002, step 0374/0496: loss=2.5054221153259277\n",
            "epoch 006/010, batch 001/002, step 0375/0496: loss=3.010650634765625\n",
            "epoch 006/010, batch 001/002, step 0376/0496: loss=2.845306158065796\n",
            "epoch 006/010, batch 001/002, step 0377/0496: loss=2.980844020843506\n",
            "epoch 006/010, batch 001/002, step 0378/0496: loss=2.6124486923217773\n",
            "epoch 006/010, batch 001/002, step 0379/0496: loss=2.4314773082733154\n",
            "epoch 006/010, batch 001/002, step 0380/0496: loss=1.7937740087509155\n",
            "epoch 006/010, batch 001/002, step 0381/0496: loss=2.706360340118408\n",
            "epoch 006/010, batch 001/002, step 0382/0496: loss=2.904501438140869\n",
            "epoch 006/010, batch 001/002, step 0383/0496: loss=2.1578755378723145\n",
            "epoch 006/010, batch 001/002, step 0384/0496: loss=1.9004199504852295\n",
            "epoch 006/010, batch 001/002, step 0385/0496: loss=2.3531646728515625\n",
            "epoch 006/010, batch 001/002, step 0386/0496: loss=1.8916131258010864\n",
            "epoch 006/010, batch 001/002, step 0387/0496: loss=1.8105212450027466\n",
            "epoch 006/010, batch 001/002, step 0388/0496: loss=4.686898231506348\n",
            "epoch 006/010, batch 001/002, step 0389/0496: loss=2.4446303844451904\n",
            "epoch 006/010, batch 001/002, step 0390/0496: loss=2.06962251663208\n",
            "epoch 006/010, batch 001/002, step 0391/0496: loss=2.782564640045166\n",
            "epoch 006/010, batch 001/002, step 0392/0496: loss=2.1485648155212402\n",
            "epoch 006/010, batch 001/002, step 0393/0496: loss=2.630103349685669\n",
            "epoch 006/010, batch 001/002, step 0394/0496: loss=2.1335501670837402\n",
            "epoch 006/010, batch 001/002, step 0395/0496: loss=2.765054225921631\n",
            "epoch 006/010, batch 001/002, step 0396/0496: loss=3.395197868347168\n",
            "epoch 006/010, batch 001/002, step 0397/0496: loss=2.3917832374572754\n",
            "epoch 006/010, batch 001/002, step 0398/0496: loss=1.915429711341858\n",
            "epoch 006/010, batch 001/002, step 0399/0496: loss=2.74422025680542\n",
            "epoch 006/010, batch 001/002, step 0400/0496: loss=2.4853415489196777\n",
            "epoch 006/010, batch 001/002, step 0401/0496: loss=2.1171483993530273\n",
            "epoch 006/010, batch 001/002, step 0402/0496: loss=1.7681427001953125\n",
            "epoch 006/010, batch 001/002, step 0403/0496: loss=1.768157720565796\n",
            "epoch 006/010, batch 001/002, step 0404/0496: loss=4.006889343261719\n",
            "epoch 006/010, batch 001/002, step 0405/0496: loss=3.503546714782715\n",
            "epoch 006/010, batch 001/002, step 0406/0496: loss=2.6425817012786865\n",
            "epoch 006/010, batch 001/002, step 0407/0496: loss=2.3040614128112793\n",
            "epoch 006/010, batch 001/002, step 0408/0496: loss=2.7364768981933594\n",
            "epoch 006/010, batch 001/002, step 0409/0496: loss=4.165899276733398\n",
            "epoch 006/010, batch 001/002, step 0410/0496: loss=2.07500958442688\n",
            "epoch 006/010, batch 001/002, step 0411/0496: loss=2.1359221935272217\n",
            "epoch 006/010, batch 001/002, step 0412/0496: loss=2.1986923217773438\n",
            "epoch 006/010, batch 001/002, step 0413/0496: loss=3.2674379348754883\n",
            "epoch 006/010, batch 001/002, step 0414/0496: loss=2.384369373321533\n",
            "epoch 006/010, batch 001/002, step 0415/0496: loss=2.379154682159424\n",
            "epoch 006/010, batch 001/002, step 0416/0496: loss=3.233597993850708\n",
            "epoch 006/010, batch 001/002, step 0417/0496: loss=2.7287943363189697\n",
            "epoch 006/010, batch 001/002, step 0418/0496: loss=2.403158187866211\n",
            "epoch 006/010, batch 001/002, step 0419/0496: loss=2.6114392280578613\n",
            "epoch 006/010, batch 001/002, step 0420/0496: loss=2.7562170028686523\n",
            "epoch 006/010, batch 001/002, step 0421/0496: loss=2.0233821868896484\n",
            "epoch 006/010, batch 001/002, step 0422/0496: loss=2.890817165374756\n",
            "epoch 006/010, batch 001/002, step 0423/0496: loss=2.396557331085205\n",
            "epoch 006/010, batch 001/002, step 0424/0496: loss=2.6006622314453125\n",
            "epoch 006/010, batch 001/002, step 0425/0496: loss=2.8750364780426025\n",
            "epoch 006/010, batch 001/002, step 0426/0496: loss=1.985856533050537\n",
            "epoch 006/010, batch 001/002, step 0427/0496: loss=3.3573718070983887\n",
            "epoch 006/010, batch 001/002, step 0428/0496: loss=2.7617886066436768\n",
            "epoch 006/010, batch 001/002, step 0429/0496: loss=2.6414031982421875\n",
            "epoch 006/010, batch 001/002, step 0430/0496: loss=2.488736152648926\n",
            "epoch 006/010, batch 001/002, step 0431/0496: loss=2.510500907897949\n",
            "epoch 006/010, batch 001/002, step 0432/0496: loss=2.620220899581909\n",
            "epoch 006/010, batch 001/002, step 0433/0496: loss=2.232719659805298\n",
            "epoch 006/010, batch 001/002, step 0434/0496: loss=2.7647359371185303\n",
            "epoch 006/010, batch 001/002, step 0435/0496: loss=2.5373964309692383\n",
            "epoch 006/010, batch 001/002, step 0436/0496: loss=2.3278331756591797\n",
            "epoch 006/010, batch 001/002, step 0437/0496: loss=3.7184929847717285\n",
            "epoch 006/010, batch 001/002, step 0438/0496: loss=2.4235024452209473\n",
            "epoch 006/010, batch 001/002, step 0439/0496: loss=3.4278907775878906\n",
            "epoch 006/010, batch 001/002, step 0440/0496: loss=2.773555278778076\n",
            "epoch 006/010, batch 001/002, step 0441/0496: loss=2.3173344135284424\n",
            "epoch 006/010, batch 001/002, step 0442/0496: loss=2.205888032913208\n",
            "epoch 006/010, batch 001/002, step 0443/0496: loss=3.4728870391845703\n",
            "epoch 006/010, batch 001/002, step 0444/0496: loss=3.0747647285461426\n",
            "epoch 006/010, batch 001/002, step 0445/0496: loss=2.788116693496704\n",
            "epoch 006/010, batch 001/002, step 0446/0496: loss=3.032878875732422\n",
            "epoch 006/010, batch 001/002, step 0447/0496: loss=2.823490858078003\n",
            "epoch 006/010, batch 001/002, step 0448/0496: loss=2.4055073261260986\n",
            "epoch 006/010, batch 001/002, step 0449/0496: loss=2.5295701026916504\n",
            "epoch 006/010, batch 001/002, step 0450/0496: loss=2.0935752391815186\n",
            "epoch 006/010, batch 001/002, step 0451/0496: loss=2.4008233547210693\n",
            "epoch 006/010, batch 001/002, step 0452/0496: loss=2.9586851596832275\n",
            "epoch 006/010, batch 001/002, step 0453/0496: loss=3.4077131748199463\n",
            "epoch 006/010, batch 001/002, step 0454/0496: loss=2.594674825668335\n",
            "epoch 006/010, batch 001/002, step 0455/0496: loss=3.0115227699279785\n",
            "epoch 006/010, batch 001/002, step 0456/0496: loss=2.258669137954712\n",
            "epoch 006/010, batch 001/002, step 0457/0496: loss=2.4705424308776855\n",
            "epoch 006/010, batch 001/002, step 0458/0496: loss=2.454164981842041\n",
            "epoch 006/010, batch 001/002, step 0459/0496: loss=2.0111491680145264\n",
            "epoch 006/010, batch 001/002, step 0460/0496: loss=4.060707092285156\n",
            "epoch 006/010, batch 001/002, step 0461/0496: loss=2.0488429069519043\n",
            "epoch 006/010, batch 001/002, step 0462/0496: loss=2.61012864112854\n",
            "epoch 006/010, batch 001/002, step 0463/0496: loss=2.219601631164551\n",
            "epoch 006/010, batch 001/002, step 0464/0496: loss=1.9693758487701416\n",
            "epoch 006/010, batch 001/002, step 0465/0496: loss=2.892435073852539\n",
            "epoch 006/010, batch 001/002, step 0466/0496: loss=1.717169165611267\n",
            "epoch 006/010, batch 001/002, step 0467/0496: loss=2.531928539276123\n",
            "epoch 006/010, batch 001/002, step 0468/0496: loss=2.245077133178711\n",
            "epoch 006/010, batch 001/002, step 0469/0496: loss=3.043050765991211\n",
            "epoch 006/010, batch 001/002, step 0470/0496: loss=2.712646484375\n",
            "epoch 006/010, batch 001/002, step 0471/0496: loss=2.3561909198760986\n",
            "epoch 006/010, batch 001/002, step 0472/0496: loss=2.9473297595977783\n",
            "epoch 006/010, batch 001/002, step 0473/0496: loss=2.282207727432251\n",
            "epoch 006/010, batch 001/002, step 0474/0496: loss=1.8487194776535034\n",
            "epoch 006/010, batch 001/002, step 0475/0496: loss=3.0076756477355957\n",
            "epoch 006/010, batch 001/002, step 0476/0496: loss=2.650935411453247\n",
            "epoch 006/010, batch 001/002, step 0477/0496: loss=3.1372666358947754\n",
            "epoch 006/010, batch 001/002, step 0478/0496: loss=3.09116530418396\n",
            "epoch 006/010, batch 001/002, step 0479/0496: loss=2.141796112060547\n",
            "epoch 006/010, batch 001/002, step 0480/0496: loss=2.396930694580078\n",
            "epoch 006/010, batch 001/002, step 0481/0496: loss=2.9413137435913086\n",
            "epoch 006/010, batch 001/002, step 0482/0496: loss=5.311528205871582\n",
            "epoch 006/010, batch 001/002, step 0483/0496: loss=3.493208885192871\n",
            "epoch 006/010, batch 001/002, step 0484/0496: loss=2.0829250812530518\n",
            "epoch 006/010, batch 001/002, step 0485/0496: loss=2.799962043762207\n",
            "epoch 006/010, batch 001/002, step 0486/0496: loss=2.3344576358795166\n",
            "epoch 006/010, batch 001/002, step 0487/0496: loss=3.0207552909851074\n",
            "epoch 006/010, batch 001/002, step 0488/0496: loss=2.6107096672058105\n",
            "epoch 006/010, batch 001/002, step 0489/0496: loss=2.402118682861328\n",
            "epoch 006/010, batch 001/002, step 0490/0496: loss=2.5451650619506836\n",
            "epoch 006/010, batch 001/002, step 0491/0496: loss=2.8543801307678223\n",
            "epoch 006/010, batch 001/002, step 0492/0496: loss=3.113344669342041\n",
            "epoch 006/010, batch 001/002, step 0493/0496: loss=2.7116355895996094\n",
            "epoch 006/010, batch 001/002, step 0494/0496: loss=2.4688806533813477\n",
            "epoch 006/010, batch 001/002, step 0495/0496: loss=3.810589075088501\n",
            "epoch 006/010, batch 001/002, step 0496/0496: loss=2.4203319549560547\n",
            "epoch 006/010, batch 002/002, step 0001/0496: loss=2.3641035556793213\n",
            "epoch 006/010, batch 002/002, step 0002/0496: loss=2.396001100540161\n",
            "epoch 006/010, batch 002/002, step 0003/0496: loss=2.1039438247680664\n",
            "epoch 006/010, batch 002/002, step 0004/0496: loss=2.3012170791625977\n",
            "epoch 006/010, batch 002/002, step 0005/0496: loss=1.9038434028625488\n",
            "epoch 006/010, batch 002/002, step 0006/0496: loss=3.0568408966064453\n",
            "epoch 006/010, batch 002/002, step 0007/0496: loss=2.5778698921203613\n",
            "epoch 006/010, batch 002/002, step 0008/0496: loss=2.370452404022217\n",
            "epoch 006/010, batch 002/002, step 0009/0496: loss=1.8982853889465332\n",
            "epoch 006/010, batch 002/002, step 0010/0496: loss=1.8522748947143555\n",
            "epoch 006/010, batch 002/002, step 0011/0496: loss=3.349597930908203\n",
            "epoch 006/010, batch 002/002, step 0012/0496: loss=2.142660140991211\n",
            "epoch 006/010, batch 002/002, step 0013/0496: loss=3.8297557830810547\n",
            "epoch 006/010, batch 002/002, step 0014/0496: loss=2.84731388092041\n",
            "epoch 006/010, batch 002/002, step 0015/0496: loss=2.3624911308288574\n",
            "epoch 006/010, batch 002/002, step 0016/0496: loss=2.3481462001800537\n",
            "epoch 006/010, batch 002/002, step 0017/0496: loss=2.7124996185302734\n",
            "epoch 006/010, batch 002/002, step 0018/0496: loss=2.2614808082580566\n",
            "epoch 006/010, batch 002/002, step 0019/0496: loss=2.145905017852783\n",
            "epoch 006/010, batch 002/002, step 0020/0496: loss=2.8854260444641113\n",
            "epoch 006/010, batch 002/002, step 0021/0496: loss=2.3979969024658203\n",
            "epoch 006/010, batch 002/002, step 0022/0496: loss=2.8623030185699463\n",
            "epoch 006/010, batch 002/002, step 0023/0496: loss=4.701595306396484\n",
            "epoch 006/010, batch 002/002, step 0024/0496: loss=2.4135756492614746\n",
            "epoch 006/010, batch 002/002, step 0025/0496: loss=3.2743043899536133\n",
            "epoch 006/010, batch 002/002, step 0026/0496: loss=2.3279058933258057\n",
            "epoch 006/010, batch 002/002, step 0027/0496: loss=3.539513111114502\n",
            "epoch 006/010, batch 002/002, step 0028/0496: loss=4.556909561157227\n",
            "epoch 006/010, batch 002/002, step 0029/0496: loss=3.617662191390991\n",
            "epoch 006/010, batch 002/002, step 0030/0496: loss=2.875722885131836\n",
            "epoch 006/010, batch 002/002, step 0031/0496: loss=2.82529878616333\n",
            "epoch 006/010, batch 002/002, step 0032/0496: loss=3.302737236022949\n",
            "epoch 006/010, batch 002/002, step 0033/0496: loss=2.9819796085357666\n",
            "epoch 006/010, batch 002/002, step 0034/0496: loss=2.4458553791046143\n",
            "epoch 006/010, batch 002/002, step 0035/0496: loss=4.96933650970459\n",
            "epoch 006/010, batch 002/002, step 0036/0496: loss=5.930380821228027\n",
            "epoch 006/010, batch 002/002, step 0037/0496: loss=3.100663185119629\n",
            "epoch 006/010, batch 002/002, step 0038/0496: loss=4.154531955718994\n",
            "epoch 006/010, batch 002/002, step 0039/0496: loss=4.608930587768555\n",
            "epoch 006/010, batch 002/002, step 0040/0496: loss=4.053847312927246\n",
            "epoch 006/010, batch 002/002, step 0041/0496: loss=4.463550090789795\n",
            "epoch 006/010, batch 002/002, step 0042/0496: loss=3.480069637298584\n",
            "epoch 006/010, batch 002/002, step 0043/0496: loss=4.075047492980957\n",
            "epoch 006/010, batch 002/002, step 0044/0496: loss=3.314971685409546\n",
            "epoch 006/010, batch 002/002, step 0045/0496: loss=3.4778435230255127\n",
            "epoch 006/010, batch 002/002, step 0046/0496: loss=2.67197322845459\n",
            "epoch 006/010, batch 002/002, step 0047/0496: loss=3.5669033527374268\n",
            "epoch 006/010, batch 002/002, step 0048/0496: loss=2.933427572250366\n",
            "epoch 006/010, batch 002/002, step 0049/0496: loss=4.457699775695801\n",
            "epoch 006/010, batch 002/002, step 0050/0496: loss=3.03987455368042\n",
            "epoch 006/010, batch 002/002, step 0051/0496: loss=3.8636395931243896\n",
            "epoch 006/010, batch 002/002, step 0052/0496: loss=2.9973719120025635\n",
            "epoch 006/010, batch 002/002, step 0053/0496: loss=3.314054489135742\n",
            "epoch 006/010, batch 002/002, step 0054/0496: loss=2.5682692527770996\n",
            "epoch 006/010, batch 002/002, step 0055/0496: loss=5.246795654296875\n",
            "epoch 006/010, batch 002/002, step 0056/0496: loss=2.904994010925293\n",
            "epoch 006/010, batch 002/002, step 0057/0496: loss=3.879293918609619\n",
            "epoch 006/010, batch 002/002, step 0058/0496: loss=2.563385486602783\n",
            "epoch 006/010, batch 002/002, step 0059/0496: loss=3.64900803565979\n",
            "epoch 006/010, batch 002/002, step 0060/0496: loss=2.326000928878784\n",
            "epoch 006/010, batch 002/002, step 0061/0496: loss=5.02193546295166\n",
            "epoch 006/010, batch 002/002, step 0062/0496: loss=3.4413695335388184\n",
            "epoch 006/010, batch 002/002, step 0063/0496: loss=3.9166038036346436\n",
            "epoch 006/010, batch 002/002, step 0064/0496: loss=3.0555899143218994\n",
            "epoch 006/010, batch 002/002, step 0065/0496: loss=3.0584869384765625\n",
            "epoch 006/010, batch 002/002, step 0066/0496: loss=2.3355493545532227\n",
            "epoch 006/010, batch 002/002, step 0067/0496: loss=2.181556224822998\n",
            "epoch 006/010, batch 002/002, step 0068/0496: loss=3.344081401824951\n",
            "epoch 006/010, batch 002/002, step 0069/0496: loss=3.2826669216156006\n",
            "epoch 006/010, batch 002/002, step 0070/0496: loss=2.8810184001922607\n",
            "epoch 006/010, batch 002/002, step 0071/0496: loss=2.6715011596679688\n",
            "epoch 006/010, batch 002/002, step 0072/0496: loss=4.246349811553955\n",
            "epoch 006/010, batch 002/002, step 0073/0496: loss=3.4932522773742676\n",
            "epoch 006/010, batch 002/002, step 0074/0496: loss=3.565969467163086\n",
            "epoch 006/010, batch 002/002, step 0075/0496: loss=2.4809556007385254\n",
            "epoch 006/010, batch 002/002, step 0076/0496: loss=3.5034942626953125\n",
            "epoch 006/010, batch 002/002, step 0077/0496: loss=2.2988150119781494\n",
            "epoch 006/010, batch 002/002, step 0078/0496: loss=2.3960866928100586\n",
            "epoch 006/010, batch 002/002, step 0079/0496: loss=9.728963851928711\n",
            "epoch 006/010, batch 002/002, step 0080/0496: loss=15.795633316040039\n",
            "epoch 006/010, batch 002/002, step 0081/0496: loss=9.568975448608398\n",
            "epoch 006/010, batch 002/002, step 0082/0496: loss=11.34337043762207\n",
            "epoch 006/010, batch 002/002, step 0083/0496: loss=3.926572322845459\n",
            "epoch 006/010, batch 002/002, step 0084/0496: loss=4.5608720779418945\n",
            "epoch 006/010, batch 002/002, step 0085/0496: loss=14.564767837524414\n",
            "epoch 006/010, batch 002/002, step 0086/0496: loss=6.584176540374756\n",
            "epoch 006/010, batch 002/002, step 0087/0496: loss=10.970046043395996\n",
            "epoch 006/010, batch 002/002, step 0088/0496: loss=7.442234992980957\n",
            "epoch 006/010, batch 002/002, step 0089/0496: loss=5.532372951507568\n",
            "epoch 006/010, batch 002/002, step 0090/0496: loss=4.505319595336914\n",
            "epoch 006/010, batch 002/002, step 0091/0496: loss=3.838244915008545\n",
            "epoch 006/010, batch 002/002, step 0092/0496: loss=5.560070037841797\n",
            "epoch 006/010, batch 002/002, step 0093/0496: loss=3.653050661087036\n",
            "epoch 006/010, batch 002/002, step 0094/0496: loss=3.1845974922180176\n",
            "epoch 006/010, batch 002/002, step 0095/0496: loss=5.6311821937561035\n",
            "epoch 006/010, batch 002/002, step 0096/0496: loss=13.218611717224121\n",
            "epoch 006/010, batch 002/002, step 0097/0496: loss=20.40249252319336\n",
            "epoch 006/010, batch 002/002, step 0098/0496: loss=7.270598411560059\n",
            "epoch 006/010, batch 002/002, step 0099/0496: loss=3.983959674835205\n",
            "epoch 006/010, batch 002/002, step 0100/0496: loss=9.466053009033203\n",
            "epoch 006/010, batch 002/002, step 0101/0496: loss=8.583703994750977\n",
            "epoch 006/010, batch 002/002, step 0102/0496: loss=13.210000991821289\n",
            "epoch 006/010, batch 002/002, step 0103/0496: loss=12.151022911071777\n",
            "epoch 006/010, batch 002/002, step 0104/0496: loss=5.510337829589844\n",
            "epoch 006/010, batch 002/002, step 0105/0496: loss=15.214130401611328\n",
            "epoch 006/010, batch 002/002, step 0106/0496: loss=3.9513354301452637\n",
            "epoch 006/010, batch 002/002, step 0107/0496: loss=13.557991981506348\n",
            "epoch 006/010, batch 002/002, step 0108/0496: loss=5.578653335571289\n",
            "epoch 006/010, batch 002/002, step 0109/0496: loss=4.746667385101318\n",
            "epoch 006/010, batch 002/002, step 0110/0496: loss=12.247998237609863\n",
            "epoch 006/010, batch 002/002, step 0111/0496: loss=3.39176082611084\n",
            "epoch 006/010, batch 002/002, step 0112/0496: loss=10.925146102905273\n",
            "epoch 006/010, batch 002/002, step 0113/0496: loss=3.695671796798706\n",
            "epoch 006/010, batch 002/002, step 0114/0496: loss=6.597068786621094\n",
            "epoch 006/010, batch 002/002, step 0115/0496: loss=7.118751525878906\n",
            "epoch 006/010, batch 002/002, step 0116/0496: loss=3.27866792678833\n",
            "epoch 006/010, batch 002/002, step 0117/0496: loss=9.503288269042969\n",
            "epoch 006/010, batch 002/002, step 0118/0496: loss=2.9256577491760254\n",
            "epoch 006/010, batch 002/002, step 0119/0496: loss=4.0481648445129395\n",
            "epoch 006/010, batch 002/002, step 0120/0496: loss=4.999608039855957\n",
            "epoch 006/010, batch 002/002, step 0121/0496: loss=2.1382453441619873\n",
            "epoch 006/010, batch 002/002, step 0122/0496: loss=7.678943157196045\n",
            "epoch 006/010, batch 002/002, step 0123/0496: loss=3.5286173820495605\n",
            "epoch 006/010, batch 002/002, step 0124/0496: loss=4.469784736633301\n",
            "epoch 006/010, batch 002/002, step 0125/0496: loss=3.9108762741088867\n",
            "epoch 006/010, batch 002/002, step 0126/0496: loss=3.664820432662964\n",
            "epoch 006/010, batch 002/002, step 0127/0496: loss=8.08603286743164\n",
            "epoch 006/010, batch 002/002, step 0128/0496: loss=4.2240447998046875\n",
            "epoch 006/010, batch 002/002, step 0129/0496: loss=2.535005569458008\n",
            "epoch 006/010, batch 002/002, step 0130/0496: loss=11.108173370361328\n",
            "epoch 006/010, batch 002/002, step 0131/0496: loss=4.4511237144470215\n",
            "epoch 006/010, batch 002/002, step 0132/0496: loss=12.240072250366211\n",
            "epoch 006/010, batch 002/002, step 0133/0496: loss=5.28197717666626\n",
            "epoch 006/010, batch 002/002, step 0134/0496: loss=6.345093727111816\n",
            "epoch 006/010, batch 002/002, step 0135/0496: loss=7.5633955001831055\n",
            "epoch 006/010, batch 002/002, step 0136/0496: loss=3.2458481788635254\n",
            "epoch 006/010, batch 002/002, step 0137/0496: loss=5.247387886047363\n",
            "epoch 006/010, batch 002/002, step 0138/0496: loss=11.243929862976074\n",
            "epoch 006/010, batch 002/002, step 0139/0496: loss=7.890595436096191\n",
            "epoch 006/010, batch 002/002, step 0140/0496: loss=12.62944507598877\n",
            "epoch 006/010, batch 002/002, step 0141/0496: loss=4.135064125061035\n",
            "epoch 006/010, batch 002/002, step 0142/0496: loss=10.896483421325684\n",
            "epoch 006/010, batch 002/002, step 0143/0496: loss=7.8934502601623535\n",
            "epoch 006/010, batch 002/002, step 0144/0496: loss=5.435448169708252\n",
            "epoch 006/010, batch 002/002, step 0145/0496: loss=5.8866286277771\n",
            "epoch 006/010, batch 002/002, step 0146/0496: loss=4.094815254211426\n",
            "epoch 006/010, batch 002/002, step 0147/0496: loss=5.085324764251709\n",
            "epoch 006/010, batch 002/002, step 0148/0496: loss=6.293654918670654\n",
            "epoch 006/010, batch 002/002, step 0149/0496: loss=2.6462643146514893\n",
            "epoch 006/010, batch 002/002, step 0150/0496: loss=6.076285362243652\n",
            "epoch 006/010, batch 002/002, step 0151/0496: loss=2.9803171157836914\n",
            "epoch 006/010, batch 002/002, step 0152/0496: loss=4.907163619995117\n",
            "epoch 006/010, batch 002/002, step 0153/0496: loss=5.040080547332764\n",
            "epoch 006/010, batch 002/002, step 0154/0496: loss=2.8206381797790527\n",
            "epoch 006/010, batch 002/002, step 0155/0496: loss=3.5096728801727295\n",
            "epoch 006/010, batch 002/002, step 0156/0496: loss=3.5381019115448\n",
            "epoch 006/010, batch 002/002, step 0157/0496: loss=3.887547492980957\n",
            "epoch 006/010, batch 002/002, step 0158/0496: loss=3.595115900039673\n",
            "epoch 006/010, batch 002/002, step 0159/0496: loss=4.062855243682861\n",
            "epoch 006/010, batch 002/002, step 0160/0496: loss=3.046733856201172\n",
            "epoch 006/010, batch 002/002, step 0161/0496: loss=3.207566261291504\n",
            "epoch 006/010, batch 002/002, step 0162/0496: loss=4.53059720993042\n",
            "epoch 006/010, batch 002/002, step 0163/0496: loss=3.3864850997924805\n",
            "epoch 006/010, batch 002/002, step 0164/0496: loss=3.807837963104248\n",
            "epoch 006/010, batch 002/002, step 0165/0496: loss=2.7427053451538086\n",
            "epoch 006/010, batch 002/002, step 0166/0496: loss=3.5383055210113525\n",
            "epoch 006/010, batch 002/002, step 0167/0496: loss=3.701669692993164\n",
            "epoch 006/010, batch 002/002, step 0168/0496: loss=4.530182361602783\n",
            "epoch 006/010, batch 002/002, step 0169/0496: loss=3.5396573543548584\n",
            "epoch 006/010, batch 002/002, step 0170/0496: loss=2.602909564971924\n",
            "epoch 006/010, batch 002/002, step 0171/0496: loss=2.7709178924560547\n",
            "epoch 006/010, batch 002/002, step 0172/0496: loss=3.7665605545043945\n",
            "epoch 006/010, batch 002/002, step 0173/0496: loss=2.850569725036621\n",
            "epoch 006/010, batch 002/002, step 0174/0496: loss=3.837648868560791\n",
            "epoch 006/010, batch 002/002, step 0175/0496: loss=3.7709031105041504\n",
            "epoch 006/010, batch 002/002, step 0176/0496: loss=3.2808709144592285\n",
            "epoch 006/010, batch 002/002, step 0177/0496: loss=3.452831745147705\n",
            "epoch 006/010, batch 002/002, step 0178/0496: loss=3.610182523727417\n",
            "epoch 006/010, batch 002/002, step 0179/0496: loss=2.8868250846862793\n",
            "epoch 006/010, batch 002/002, step 0180/0496: loss=2.2180161476135254\n",
            "epoch 006/010, batch 002/002, step 0181/0496: loss=4.664539337158203\n",
            "epoch 006/010, batch 002/002, step 0182/0496: loss=2.6341800689697266\n",
            "epoch 006/010, batch 002/002, step 0183/0496: loss=2.56569242477417\n",
            "epoch 006/010, batch 002/002, step 0184/0496: loss=2.1547062397003174\n",
            "epoch 006/010, batch 002/002, step 0185/0496: loss=3.9543423652648926\n",
            "epoch 006/010, batch 002/002, step 0186/0496: loss=4.5060577392578125\n",
            "epoch 006/010, batch 002/002, step 0187/0496: loss=3.2992258071899414\n",
            "epoch 006/010, batch 002/002, step 0188/0496: loss=2.725780487060547\n",
            "epoch 006/010, batch 002/002, step 0189/0496: loss=3.2875094413757324\n",
            "epoch 006/010, batch 002/002, step 0190/0496: loss=2.639087200164795\n",
            "epoch 006/010, batch 002/002, step 0191/0496: loss=1.903369665145874\n",
            "epoch 006/010, batch 002/002, step 0192/0496: loss=2.5746030807495117\n",
            "epoch 006/010, batch 002/002, step 0193/0496: loss=2.9709372520446777\n",
            "epoch 006/010, batch 002/002, step 0194/0496: loss=2.399364948272705\n",
            "epoch 006/010, batch 002/002, step 0195/0496: loss=2.847565174102783\n",
            "epoch 006/010, batch 002/002, step 0196/0496: loss=3.3173537254333496\n",
            "epoch 006/010, batch 002/002, step 0197/0496: loss=2.090028762817383\n",
            "epoch 006/010, batch 002/002, step 0198/0496: loss=3.7884504795074463\n",
            "epoch 006/010, batch 002/002, step 0199/0496: loss=2.4804296493530273\n",
            "epoch 006/010, batch 002/002, step 0200/0496: loss=2.7057743072509766\n",
            "epoch 006/010, batch 002/002, step 0201/0496: loss=2.874458074569702\n",
            "epoch 006/010, batch 002/002, step 0202/0496: loss=2.6752331256866455\n",
            "epoch 006/010, batch 002/002, step 0203/0496: loss=3.1550240516662598\n",
            "epoch 006/010, batch 002/002, step 0204/0496: loss=3.0000476837158203\n",
            "epoch 006/010, batch 002/002, step 0205/0496: loss=2.356555700302124\n",
            "epoch 006/010, batch 002/002, step 0206/0496: loss=3.1893157958984375\n",
            "epoch 006/010, batch 002/002, step 0207/0496: loss=2.7972452640533447\n",
            "epoch 006/010, batch 002/002, step 0208/0496: loss=2.088128089904785\n",
            "epoch 006/010, batch 002/002, step 0209/0496: loss=3.032588481903076\n",
            "epoch 006/010, batch 002/002, step 0210/0496: loss=2.818859338760376\n",
            "epoch 006/010, batch 002/002, step 0211/0496: loss=2.385575294494629\n",
            "epoch 006/010, batch 002/002, step 0212/0496: loss=2.1241421699523926\n",
            "epoch 006/010, batch 002/002, step 0213/0496: loss=2.0670480728149414\n",
            "epoch 006/010, batch 002/002, step 0214/0496: loss=3.2708241939544678\n",
            "epoch 006/010, batch 002/002, step 0215/0496: loss=2.7698519229888916\n",
            "epoch 006/010, batch 002/002, step 0216/0496: loss=2.58028244972229\n",
            "epoch 006/010, batch 002/002, step 0217/0496: loss=2.7892749309539795\n",
            "epoch 006/010, batch 002/002, step 0218/0496: loss=2.7496845722198486\n",
            "epoch 006/010, batch 002/002, step 0219/0496: loss=4.587896347045898\n",
            "epoch 006/010, batch 002/002, step 0220/0496: loss=1.9679843187332153\n",
            "epoch 006/010, batch 002/002, step 0221/0496: loss=3.2049546241760254\n",
            "epoch 006/010, batch 002/002, step 0222/0496: loss=2.6025335788726807\n",
            "epoch 006/010, batch 002/002, step 0223/0496: loss=2.9232280254364014\n",
            "epoch 006/010, batch 002/002, step 0224/0496: loss=2.052114963531494\n",
            "epoch 006/010, batch 002/002, step 0225/0496: loss=2.0900516510009766\n",
            "epoch 006/010, batch 002/002, step 0226/0496: loss=3.3097782135009766\n",
            "epoch 006/010, batch 002/002, step 0227/0496: loss=2.754373550415039\n",
            "epoch 006/010, batch 002/002, step 0228/0496: loss=3.3207790851593018\n",
            "epoch 006/010, batch 002/002, step 0229/0496: loss=2.5670552253723145\n",
            "epoch 006/010, batch 002/002, step 0230/0496: loss=2.697429656982422\n",
            "epoch 006/010, batch 002/002, step 0231/0496: loss=2.5624237060546875\n",
            "epoch 006/010, batch 002/002, step 0232/0496: loss=2.656057357788086\n",
            "epoch 006/010, batch 002/002, step 0233/0496: loss=2.6352157592773438\n",
            "epoch 006/010, batch 002/002, step 0234/0496: loss=2.6376075744628906\n",
            "epoch 006/010, batch 002/002, step 0235/0496: loss=2.236952543258667\n",
            "epoch 006/010, batch 002/002, step 0236/0496: loss=2.432410955429077\n",
            "epoch 006/010, batch 002/002, step 0237/0496: loss=2.779977798461914\n",
            "epoch 006/010, batch 002/002, step 0238/0496: loss=2.4026942253112793\n",
            "epoch 006/010, batch 002/002, step 0239/0496: loss=2.7485978603363037\n",
            "epoch 006/010, batch 002/002, step 0240/0496: loss=2.495671272277832\n",
            "epoch 006/010, batch 002/002, step 0241/0496: loss=2.852963447570801\n",
            "epoch 006/010, batch 002/002, step 0242/0496: loss=2.8999080657958984\n",
            "epoch 006/010, batch 002/002, step 0243/0496: loss=2.58590030670166\n",
            "epoch 006/010, batch 002/002, step 0244/0496: loss=2.786543846130371\n",
            "epoch 006/010, batch 002/002, step 0245/0496: loss=2.694728374481201\n",
            "epoch 006/010, batch 002/002, step 0246/0496: loss=3.5092110633850098\n",
            "epoch 006/010, batch 002/002, step 0247/0496: loss=2.1793835163116455\n",
            "epoch 006/010, batch 002/002, step 0248/0496: loss=2.324312448501587\n",
            "epoch 006/010, batch 002/002, step 0249/0496: loss=2.7563424110412598\n",
            "epoch 006/010, batch 002/002, step 0250/0496: loss=2.1323113441467285\n",
            "epoch 006/010, batch 002/002, step 0251/0496: loss=2.1052157878875732\n",
            "epoch 006/010, batch 002/002, step 0252/0496: loss=2.8877365589141846\n",
            "epoch 006/010, batch 002/002, step 0253/0496: loss=3.068903923034668\n",
            "epoch 006/010, batch 002/002, step 0254/0496: loss=2.6999669075012207\n",
            "epoch 006/010, batch 002/002, step 0255/0496: loss=2.9901843070983887\n",
            "epoch 006/010, batch 002/002, step 0256/0496: loss=2.0396933555603027\n",
            "epoch 006/010, batch 002/002, step 0257/0496: loss=4.430927276611328\n",
            "epoch 006/010, batch 002/002, step 0258/0496: loss=2.580395221710205\n",
            "epoch 006/010, batch 002/002, step 0259/0496: loss=2.5543065071105957\n",
            "epoch 006/010, batch 002/002, step 0260/0496: loss=3.139822244644165\n",
            "epoch 006/010, batch 002/002, step 0261/0496: loss=2.8758935928344727\n",
            "epoch 006/010, batch 002/002, step 0262/0496: loss=2.446371555328369\n",
            "epoch 006/010, batch 002/002, step 0263/0496: loss=2.051405906677246\n",
            "epoch 006/010, batch 002/002, step 0264/0496: loss=2.3381917476654053\n",
            "epoch 006/010, batch 002/002, step 0265/0496: loss=2.6197757720947266\n",
            "epoch 006/010, batch 002/002, step 0266/0496: loss=2.291860342025757\n",
            "epoch 006/010, batch 002/002, step 0267/0496: loss=2.7888879776000977\n",
            "epoch 006/010, batch 002/002, step 0268/0496: loss=2.8690478801727295\n",
            "epoch 006/010, batch 002/002, step 0269/0496: loss=2.67972993850708\n",
            "epoch 006/010, batch 002/002, step 0270/0496: loss=3.027270793914795\n",
            "epoch 006/010, batch 002/002, step 0271/0496: loss=2.343996047973633\n",
            "epoch 006/010, batch 002/002, step 0272/0496: loss=2.2879812717437744\n",
            "epoch 006/010, batch 002/002, step 0273/0496: loss=2.0103354454040527\n",
            "epoch 006/010, batch 002/002, step 0274/0496: loss=2.403299570083618\n",
            "epoch 006/010, batch 002/002, step 0275/0496: loss=2.262366771697998\n",
            "epoch 006/010, batch 002/002, step 0276/0496: loss=2.1301798820495605\n",
            "epoch 006/010, batch 002/002, step 0277/0496: loss=2.578677177429199\n",
            "epoch 006/010, batch 002/002, step 0278/0496: loss=2.5274500846862793\n",
            "epoch 006/010, batch 002/002, step 0279/0496: loss=2.9871528148651123\n",
            "epoch 006/010, batch 002/002, step 0280/0496: loss=2.110929012298584\n",
            "epoch 006/010, batch 002/002, step 0281/0496: loss=2.131725311279297\n",
            "epoch 006/010, batch 002/002, step 0282/0496: loss=2.1102325916290283\n",
            "epoch 006/010, batch 002/002, step 0283/0496: loss=1.8522348403930664\n",
            "epoch 006/010, batch 002/002, step 0284/0496: loss=3.0165200233459473\n",
            "epoch 006/010, batch 002/002, step 0285/0496: loss=2.2980589866638184\n",
            "epoch 006/010, batch 002/002, step 0286/0496: loss=1.8305065631866455\n",
            "epoch 006/010, batch 002/002, step 0287/0496: loss=2.352630376815796\n",
            "epoch 006/010, batch 002/002, step 0288/0496: loss=2.0409841537475586\n",
            "epoch 006/010, batch 002/002, step 0289/0496: loss=2.387526035308838\n",
            "epoch 006/010, batch 002/002, step 0290/0496: loss=2.1564605236053467\n",
            "epoch 006/010, batch 002/002, step 0291/0496: loss=1.9373292922973633\n",
            "epoch 006/010, batch 002/002, step 0292/0496: loss=3.969203472137451\n",
            "epoch 006/010, batch 002/002, step 0293/0496: loss=2.5501785278320312\n",
            "epoch 006/010, batch 002/002, step 0294/0496: loss=2.143312931060791\n",
            "epoch 006/010, batch 002/002, step 0295/0496: loss=2.655285596847534\n",
            "epoch 006/010, batch 002/002, step 0296/0496: loss=2.3301279544830322\n",
            "epoch 006/010, batch 002/002, step 0297/0496: loss=2.5633621215820312\n",
            "epoch 006/010, batch 002/002, step 0298/0496: loss=2.1836016178131104\n",
            "epoch 006/010, batch 002/002, step 0299/0496: loss=2.66287899017334\n",
            "epoch 006/010, batch 002/002, step 0300/0496: loss=2.4148151874542236\n",
            "epoch 006/010, batch 002/002, step 0301/0496: loss=1.7004915475845337\n",
            "epoch 006/010, batch 002/002, step 0302/0496: loss=2.2976367473602295\n",
            "epoch 006/010, batch 002/002, step 0303/0496: loss=2.357696056365967\n",
            "epoch 006/010, batch 002/002, step 0304/0496: loss=3.0546586513519287\n",
            "epoch 006/010, batch 002/002, step 0305/0496: loss=2.371718406677246\n",
            "epoch 006/010, batch 002/002, step 0306/0496: loss=2.563619613647461\n",
            "epoch 006/010, batch 002/002, step 0307/0496: loss=1.7975231409072876\n",
            "epoch 006/010, batch 002/002, step 0308/0496: loss=2.3197014331817627\n",
            "epoch 006/010, batch 002/002, step 0309/0496: loss=1.938537836074829\n",
            "epoch 006/010, batch 002/002, step 0310/0496: loss=2.656928539276123\n",
            "epoch 006/010, batch 002/002, step 0311/0496: loss=3.832247257232666\n",
            "epoch 006/010, batch 002/002, step 0312/0496: loss=2.4173264503479004\n",
            "epoch 006/010, batch 002/002, step 0313/0496: loss=2.7670164108276367\n",
            "epoch 006/010, batch 002/002, step 0314/0496: loss=2.3698348999023438\n",
            "epoch 006/010, batch 002/002, step 0315/0496: loss=2.2511324882507324\n",
            "epoch 006/010, batch 002/002, step 0316/0496: loss=2.584655284881592\n",
            "epoch 006/010, batch 002/002, step 0317/0496: loss=2.5124197006225586\n",
            "epoch 006/010, batch 002/002, step 0318/0496: loss=2.192603588104248\n",
            "epoch 006/010, batch 002/002, step 0319/0496: loss=2.412708044052124\n",
            "epoch 006/010, batch 002/002, step 0320/0496: loss=2.5743558406829834\n",
            "epoch 006/010, batch 002/002, step 0321/0496: loss=2.0789244174957275\n",
            "epoch 006/010, batch 002/002, step 0322/0496: loss=2.415029525756836\n",
            "epoch 006/010, batch 002/002, step 0323/0496: loss=3.482159376144409\n",
            "epoch 006/010, batch 002/002, step 0324/0496: loss=2.6471285820007324\n",
            "epoch 006/010, batch 002/002, step 0325/0496: loss=1.9157084226608276\n",
            "epoch 006/010, batch 002/002, step 0326/0496: loss=2.2258338928222656\n",
            "epoch 006/010, batch 002/002, step 0327/0496: loss=2.876349925994873\n",
            "epoch 006/010, batch 002/002, step 0328/0496: loss=2.1045007705688477\n",
            "epoch 006/010, batch 002/002, step 0329/0496: loss=2.3384878635406494\n",
            "epoch 006/010, batch 002/002, step 0330/0496: loss=3.0462374687194824\n",
            "epoch 006/010, batch 002/002, step 0331/0496: loss=2.9630956649780273\n",
            "epoch 006/010, batch 002/002, step 0332/0496: loss=2.388059139251709\n",
            "epoch 006/010, batch 002/002, step 0333/0496: loss=2.908339023590088\n",
            "epoch 006/010, batch 002/002, step 0334/0496: loss=2.5075693130493164\n",
            "epoch 006/010, batch 002/002, step 0335/0496: loss=1.9433808326721191\n",
            "epoch 006/010, batch 002/002, step 0336/0496: loss=2.8671715259552\n",
            "epoch 006/010, batch 002/002, step 0337/0496: loss=1.9040172100067139\n",
            "epoch 006/010, batch 002/002, step 0338/0496: loss=2.8413920402526855\n",
            "epoch 006/010, batch 002/002, step 0339/0496: loss=2.5072381496429443\n",
            "epoch 006/010, batch 002/002, step 0340/0496: loss=2.6493189334869385\n",
            "epoch 006/010, batch 002/002, step 0341/0496: loss=2.5430684089660645\n",
            "epoch 006/010, batch 002/002, step 0342/0496: loss=2.0219316482543945\n",
            "epoch 006/010, batch 002/002, step 0343/0496: loss=2.5367558002471924\n",
            "epoch 006/010, batch 002/002, step 0344/0496: loss=1.7089576721191406\n",
            "epoch 006/010, batch 002/002, step 0345/0496: loss=2.277900457382202\n",
            "epoch 006/010, batch 002/002, step 0346/0496: loss=3.0614044666290283\n",
            "epoch 006/010, batch 002/002, step 0347/0496: loss=1.9181537628173828\n",
            "epoch 006/010, batch 002/002, step 0348/0496: loss=2.3539581298828125\n",
            "epoch 006/010, batch 002/002, step 0349/0496: loss=2.0595710277557373\n",
            "epoch 006/010, batch 002/002, step 0350/0496: loss=4.560920238494873\n",
            "epoch 006/010, batch 002/002, step 0351/0496: loss=2.458441734313965\n",
            "epoch 006/010, batch 002/002, step 0352/0496: loss=2.0961172580718994\n",
            "epoch 006/010, batch 002/002, step 0353/0496: loss=2.4776735305786133\n",
            "epoch 006/010, batch 002/002, step 0354/0496: loss=2.3528902530670166\n",
            "epoch 006/010, batch 002/002, step 0355/0496: loss=2.769904375076294\n",
            "epoch 006/010, batch 002/002, step 0356/0496: loss=2.5263352394104004\n",
            "epoch 006/010, batch 002/002, step 0357/0496: loss=2.3263912200927734\n",
            "epoch 006/010, batch 002/002, step 0358/0496: loss=1.7820395231246948\n",
            "epoch 006/010, batch 002/002, step 0359/0496: loss=1.878059983253479\n",
            "epoch 006/010, batch 002/002, step 0360/0496: loss=2.7177233695983887\n",
            "epoch 006/010, batch 002/002, step 0361/0496: loss=2.343559980392456\n",
            "epoch 006/010, batch 002/002, step 0362/0496: loss=2.4406564235687256\n",
            "epoch 006/010, batch 002/002, step 0363/0496: loss=2.795172691345215\n",
            "epoch 006/010, batch 002/002, step 0364/0496: loss=2.828575611114502\n",
            "epoch 006/010, batch 002/002, step 0365/0496: loss=2.4484915733337402\n",
            "epoch 006/010, batch 002/002, step 0366/0496: loss=2.7835333347320557\n",
            "epoch 006/010, batch 002/002, step 0367/0496: loss=2.5748744010925293\n",
            "epoch 006/010, batch 002/002, step 0368/0496: loss=2.7771804332733154\n",
            "epoch 006/010, batch 002/002, step 0369/0496: loss=2.169813632965088\n",
            "epoch 006/010, batch 002/002, step 0370/0496: loss=2.2545671463012695\n",
            "epoch 006/010, batch 002/002, step 0371/0496: loss=2.3857336044311523\n",
            "epoch 006/010, batch 002/002, step 0372/0496: loss=2.1689951419830322\n",
            "epoch 006/010, batch 002/002, step 0373/0496: loss=2.7631382942199707\n",
            "epoch 006/010, batch 002/002, step 0374/0496: loss=1.8687732219696045\n",
            "epoch 006/010, batch 002/002, step 0375/0496: loss=2.448997974395752\n",
            "epoch 006/010, batch 002/002, step 0376/0496: loss=1.830264925956726\n",
            "epoch 006/010, batch 002/002, step 0377/0496: loss=2.6507201194763184\n",
            "epoch 006/010, batch 002/002, step 0378/0496: loss=2.4190070629119873\n",
            "epoch 006/010, batch 002/002, step 0379/0496: loss=2.495835781097412\n",
            "epoch 006/010, batch 002/002, step 0380/0496: loss=2.0419445037841797\n",
            "epoch 006/010, batch 002/002, step 0381/0496: loss=2.3277997970581055\n",
            "epoch 006/010, batch 002/002, step 0382/0496: loss=2.1723732948303223\n",
            "epoch 006/010, batch 002/002, step 0383/0496: loss=3.2466084957122803\n",
            "epoch 006/010, batch 002/002, step 0384/0496: loss=1.8984692096710205\n",
            "epoch 006/010, batch 002/002, step 0385/0496: loss=2.3313417434692383\n",
            "epoch 006/010, batch 002/002, step 0386/0496: loss=2.303607225418091\n",
            "epoch 006/010, batch 002/002, step 0387/0496: loss=1.8901488780975342\n",
            "epoch 006/010, batch 002/002, step 0388/0496: loss=1.935732126235962\n",
            "epoch 006/010, batch 002/002, step 0389/0496: loss=2.9333887100219727\n",
            "epoch 006/010, batch 002/002, step 0390/0496: loss=2.8367652893066406\n",
            "epoch 006/010, batch 002/002, step 0391/0496: loss=2.434530258178711\n",
            "epoch 006/010, batch 002/002, step 0392/0496: loss=2.6078310012817383\n",
            "epoch 006/010, batch 002/002, step 0393/0496: loss=2.235757350921631\n",
            "epoch 006/010, batch 002/002, step 0394/0496: loss=1.9512988328933716\n",
            "epoch 006/010, batch 002/002, step 0395/0496: loss=1.8836643695831299\n",
            "epoch 006/010, batch 002/002, step 0396/0496: loss=2.151780605316162\n",
            "epoch 006/010, batch 002/002, step 0397/0496: loss=1.9101603031158447\n",
            "epoch 006/010, batch 002/002, step 0398/0496: loss=2.619398832321167\n",
            "epoch 006/010, batch 002/002, step 0399/0496: loss=2.203505039215088\n",
            "epoch 006/010, batch 002/002, step 0400/0496: loss=1.7940083742141724\n",
            "epoch 006/010, batch 002/002, step 0401/0496: loss=2.073672294616699\n",
            "epoch 006/010, batch 002/002, step 0402/0496: loss=3.2539477348327637\n",
            "epoch 006/010, batch 002/002, step 0403/0496: loss=2.285305976867676\n",
            "epoch 006/010, batch 002/002, step 0404/0496: loss=1.862762689590454\n",
            "epoch 006/010, batch 002/002, step 0405/0496: loss=2.417613983154297\n",
            "epoch 006/010, batch 002/002, step 0406/0496: loss=2.4517269134521484\n",
            "epoch 006/010, batch 002/002, step 0407/0496: loss=2.4079246520996094\n",
            "epoch 006/010, batch 002/002, step 0408/0496: loss=2.2483339309692383\n",
            "epoch 006/010, batch 002/002, step 0409/0496: loss=2.2431628704071045\n",
            "epoch 006/010, batch 002/002, step 0410/0496: loss=3.1896309852600098\n",
            "epoch 006/010, batch 002/002, step 0411/0496: loss=2.3163440227508545\n",
            "epoch 006/010, batch 002/002, step 0412/0496: loss=2.1077866554260254\n",
            "epoch 006/010, batch 002/002, step 0413/0496: loss=2.148458242416382\n",
            "epoch 006/010, batch 002/002, step 0414/0496: loss=2.4980924129486084\n",
            "epoch 006/010, batch 002/002, step 0415/0496: loss=2.903946876525879\n",
            "epoch 006/010, batch 002/002, step 0416/0496: loss=3.0467305183410645\n",
            "epoch 006/010, batch 002/002, step 0417/0496: loss=2.330063819885254\n",
            "epoch 006/010, batch 002/002, step 0418/0496: loss=2.927178144454956\n",
            "epoch 006/010, batch 002/002, step 0419/0496: loss=2.782078981399536\n",
            "epoch 006/010, batch 002/002, step 0420/0496: loss=2.5088179111480713\n",
            "epoch 006/010, batch 002/002, step 0421/0496: loss=2.1950230598449707\n",
            "epoch 006/010, batch 002/002, step 0422/0496: loss=2.8814353942871094\n",
            "epoch 006/010, batch 002/002, step 0423/0496: loss=2.604149341583252\n",
            "epoch 006/010, batch 002/002, step 0424/0496: loss=4.207823753356934\n",
            "epoch 006/010, batch 002/002, step 0425/0496: loss=2.3174309730529785\n",
            "epoch 006/010, batch 002/002, step 0426/0496: loss=2.5162582397460938\n",
            "epoch 006/010, batch 002/002, step 0427/0496: loss=2.4697632789611816\n",
            "epoch 006/010, batch 002/002, step 0428/0496: loss=2.74729061126709\n",
            "epoch 006/010, batch 002/002, step 0429/0496: loss=2.1091361045837402\n",
            "epoch 006/010, batch 002/002, step 0430/0496: loss=2.2714438438415527\n",
            "epoch 006/010, batch 002/002, step 0431/0496: loss=2.2313294410705566\n",
            "epoch 006/010, batch 002/002, step 0432/0496: loss=2.4483273029327393\n",
            "epoch 006/010, batch 002/002, step 0433/0496: loss=2.1507370471954346\n",
            "epoch 006/010, batch 002/002, step 0434/0496: loss=2.5939719676971436\n",
            "epoch 006/010, batch 002/002, step 0435/0496: loss=2.0268869400024414\n",
            "epoch 006/010, batch 002/002, step 0436/0496: loss=2.4155499935150146\n",
            "epoch 006/010, batch 002/002, step 0437/0496: loss=3.1396231651306152\n",
            "epoch 006/010, batch 002/002, step 0438/0496: loss=1.7050895690917969\n",
            "epoch 006/010, batch 002/002, step 0439/0496: loss=2.105940580368042\n",
            "epoch 006/010, batch 002/002, step 0440/0496: loss=2.0971274375915527\n",
            "epoch 006/010, batch 002/002, step 0441/0496: loss=2.6039257049560547\n",
            "epoch 006/010, batch 002/002, step 0442/0496: loss=1.9845319986343384\n",
            "epoch 006/010, batch 002/002, step 0443/0496: loss=2.715503215789795\n",
            "epoch 006/010, batch 002/002, step 0444/0496: loss=2.8611812591552734\n",
            "epoch 006/010, batch 002/002, step 0445/0496: loss=3.0421881675720215\n",
            "epoch 006/010, batch 002/002, step 0446/0496: loss=2.18493914604187\n",
            "epoch 006/010, batch 002/002, step 0447/0496: loss=2.8240222930908203\n",
            "epoch 006/010, batch 002/002, step 0448/0496: loss=2.7906951904296875\n",
            "epoch 006/010, batch 002/002, step 0449/0496: loss=2.1768150329589844\n",
            "epoch 006/010, batch 002/002, step 0450/0496: loss=2.812408447265625\n",
            "epoch 006/010, batch 002/002, step 0451/0496: loss=4.052715301513672\n",
            "epoch 006/010, batch 002/002, step 0452/0496: loss=2.2512054443359375\n",
            "epoch 006/010, batch 002/002, step 0453/0496: loss=2.6627211570739746\n",
            "epoch 006/010, batch 002/002, step 0454/0496: loss=2.4289982318878174\n",
            "epoch 006/010, batch 002/002, step 0455/0496: loss=2.1049044132232666\n",
            "epoch 006/010, batch 002/002, step 0456/0496: loss=1.813738226890564\n",
            "epoch 006/010, batch 002/002, step 0457/0496: loss=2.446180820465088\n",
            "epoch 006/010, batch 002/002, step 0458/0496: loss=2.102261543273926\n",
            "epoch 006/010, batch 002/002, step 0459/0496: loss=2.763033866882324\n",
            "epoch 006/010, batch 002/002, step 0460/0496: loss=2.713712215423584\n",
            "epoch 006/010, batch 002/002, step 0461/0496: loss=2.820888042449951\n",
            "epoch 006/010, batch 002/002, step 0462/0496: loss=2.5864315032958984\n",
            "epoch 006/010, batch 002/002, step 0463/0496: loss=1.8004332780838013\n",
            "epoch 006/010, batch 002/002, step 0464/0496: loss=2.5520920753479004\n",
            "epoch 006/010, batch 002/002, step 0465/0496: loss=1.921985387802124\n",
            "epoch 006/010, batch 002/002, step 0466/0496: loss=2.214016914367676\n",
            "epoch 006/010, batch 002/002, step 0467/0496: loss=2.0236315727233887\n",
            "epoch 006/010, batch 002/002, step 0468/0496: loss=1.9558244943618774\n",
            "epoch 006/010, batch 002/002, step 0469/0496: loss=2.5800650119781494\n",
            "epoch 006/010, batch 002/002, step 0470/0496: loss=2.7049503326416016\n",
            "epoch 006/010, batch 002/002, step 0471/0496: loss=4.285610198974609\n",
            "epoch 006/010, batch 002/002, step 0472/0496: loss=2.3357114791870117\n",
            "epoch 006/010, batch 002/002, step 0473/0496: loss=2.102241277694702\n",
            "epoch 006/010, batch 002/002, step 0474/0496: loss=1.9431943893432617\n",
            "epoch 006/010, batch 002/002, step 0475/0496: loss=1.61258864402771\n",
            "epoch 006/010, batch 002/002, step 0476/0496: loss=2.758023738861084\n",
            "epoch 006/010, batch 002/002, step 0477/0496: loss=3.1176815032958984\n",
            "epoch 006/010, batch 002/002, step 0478/0496: loss=2.8946309089660645\n",
            "epoch 006/010, batch 002/002, step 0479/0496: loss=1.8921211957931519\n",
            "epoch 006/010, batch 002/002, step 0480/0496: loss=2.626209259033203\n",
            "epoch 006/010, batch 002/002, step 0481/0496: loss=2.4080514907836914\n",
            "epoch 006/010, batch 002/002, step 0482/0496: loss=2.583902359008789\n",
            "epoch 006/010, batch 002/002, step 0483/0496: loss=2.17333984375\n",
            "epoch 006/010, batch 002/002, step 0484/0496: loss=1.8703670501708984\n",
            "epoch 006/010, batch 002/002, step 0485/0496: loss=2.178356170654297\n",
            "epoch 006/010, batch 002/002, step 0486/0496: loss=2.034097671508789\n",
            "epoch 006/010, batch 002/002, step 0487/0496: loss=2.6289572715759277\n",
            "epoch 006/010, batch 002/002, step 0488/0496: loss=2.614917755126953\n",
            "epoch 006/010, batch 002/002, step 0489/0496: loss=2.4754374027252197\n",
            "epoch 006/010, batch 002/002, step 0490/0496: loss=1.7571189403533936\n",
            "epoch 006/010, batch 002/002, step 0491/0496: loss=2.7040982246398926\n",
            "epoch 006/010, batch 002/002, step 0492/0496: loss=2.366361618041992\n",
            "epoch 006/010, batch 002/002, step 0493/0496: loss=2.4055490493774414\n",
            "epoch 006/010, batch 002/002, step 0494/0496: loss=2.2080719470977783\n",
            "epoch 006/010, batch 002/002, step 0495/0496: loss=2.173419237136841\n",
            "epoch 006/010, batch 002/002, step 0496/0496: loss=2.493727684020996\n",
            "epoch 007/010, batch 001/002, step 0001/0496: loss=2.1018576622009277\n",
            "epoch 007/010, batch 001/002, step 0002/0496: loss=1.6846165657043457\n",
            "epoch 007/010, batch 001/002, step 0003/0496: loss=2.3876545429229736\n",
            "epoch 007/010, batch 001/002, step 0004/0496: loss=2.5037717819213867\n",
            "epoch 007/010, batch 001/002, step 0005/0496: loss=2.934934139251709\n",
            "epoch 007/010, batch 001/002, step 0006/0496: loss=2.373920202255249\n",
            "epoch 007/010, batch 001/002, step 0007/0496: loss=2.2416090965270996\n",
            "epoch 007/010, batch 001/002, step 0008/0496: loss=2.9909746646881104\n",
            "epoch 007/010, batch 001/002, step 0009/0496: loss=2.984994888305664\n",
            "epoch 007/010, batch 001/002, step 0010/0496: loss=2.4747791290283203\n",
            "epoch 007/010, batch 001/002, step 0011/0496: loss=2.3079559803009033\n",
            "epoch 007/010, batch 001/002, step 0012/0496: loss=2.7647061347961426\n",
            "epoch 007/010, batch 001/002, step 0013/0496: loss=1.907195806503296\n",
            "epoch 007/010, batch 001/002, step 0014/0496: loss=1.7996952533721924\n",
            "epoch 007/010, batch 001/002, step 0015/0496: loss=2.373096466064453\n",
            "epoch 007/010, batch 001/002, step 0016/0496: loss=2.851301908493042\n",
            "epoch 007/010, batch 001/002, step 0017/0496: loss=2.420295238494873\n",
            "epoch 007/010, batch 001/002, step 0018/0496: loss=2.4427380561828613\n",
            "epoch 007/010, batch 001/002, step 0019/0496: loss=2.772171974182129\n",
            "epoch 007/010, batch 001/002, step 0020/0496: loss=2.182337760925293\n",
            "epoch 007/010, batch 001/002, step 0021/0496: loss=2.498983860015869\n",
            "epoch 007/010, batch 001/002, step 0022/0496: loss=1.8139653205871582\n",
            "epoch 007/010, batch 001/002, step 0023/0496: loss=2.3621184825897217\n",
            "epoch 007/010, batch 001/002, step 0024/0496: loss=1.801875114440918\n",
            "epoch 007/010, batch 001/002, step 0025/0496: loss=2.9345216751098633\n",
            "epoch 007/010, batch 001/002, step 0026/0496: loss=1.764143705368042\n",
            "epoch 007/010, batch 001/002, step 0027/0496: loss=2.076178550720215\n",
            "epoch 007/010, batch 001/002, step 0028/0496: loss=2.742424249649048\n",
            "epoch 007/010, batch 001/002, step 0029/0496: loss=2.0745773315429688\n",
            "epoch 007/010, batch 001/002, step 0030/0496: loss=2.0056138038635254\n",
            "epoch 007/010, batch 001/002, step 0031/0496: loss=1.8786370754241943\n",
            "epoch 007/010, batch 001/002, step 0032/0496: loss=2.5842576026916504\n",
            "epoch 007/010, batch 001/002, step 0033/0496: loss=2.1965270042419434\n",
            "epoch 007/010, batch 001/002, step 0034/0496: loss=2.3380815982818604\n",
            "epoch 007/010, batch 001/002, step 0035/0496: loss=2.2347826957702637\n",
            "epoch 007/010, batch 001/002, step 0036/0496: loss=2.4928746223449707\n",
            "epoch 007/010, batch 001/002, step 0037/0496: loss=2.2774696350097656\n",
            "epoch 007/010, batch 001/002, step 0038/0496: loss=2.8110432624816895\n",
            "epoch 007/010, batch 001/002, step 0039/0496: loss=2.039860248565674\n",
            "epoch 007/010, batch 001/002, step 0040/0496: loss=2.0913732051849365\n",
            "epoch 007/010, batch 001/002, step 0041/0496: loss=2.6381895542144775\n",
            "epoch 007/010, batch 001/002, step 0042/0496: loss=2.360365867614746\n",
            "epoch 007/010, batch 001/002, step 0043/0496: loss=1.8083398342132568\n",
            "epoch 007/010, batch 001/002, step 0044/0496: loss=2.674978733062744\n",
            "epoch 007/010, batch 001/002, step 0045/0496: loss=2.0442967414855957\n",
            "epoch 007/010, batch 001/002, step 0046/0496: loss=2.7151694297790527\n",
            "epoch 007/010, batch 001/002, step 0047/0496: loss=2.3854892253875732\n",
            "epoch 007/010, batch 001/002, step 0048/0496: loss=4.255797386169434\n",
            "epoch 007/010, batch 001/002, step 0049/0496: loss=2.0265328884124756\n",
            "epoch 007/010, batch 001/002, step 0050/0496: loss=2.4733729362487793\n",
            "epoch 007/010, batch 001/002, step 0051/0496: loss=2.759172201156616\n",
            "epoch 007/010, batch 001/002, step 0052/0496: loss=2.336139440536499\n",
            "epoch 007/010, batch 001/002, step 0053/0496: loss=1.9362545013427734\n",
            "epoch 007/010, batch 001/002, step 0054/0496: loss=1.9427317380905151\n",
            "epoch 007/010, batch 001/002, step 0055/0496: loss=2.0393521785736084\n",
            "epoch 007/010, batch 001/002, step 0056/0496: loss=3.699557304382324\n",
            "epoch 007/010, batch 001/002, step 0057/0496: loss=2.531245231628418\n",
            "epoch 007/010, batch 001/002, step 0058/0496: loss=2.051449775695801\n",
            "epoch 007/010, batch 001/002, step 0059/0496: loss=2.1794722080230713\n",
            "epoch 007/010, batch 001/002, step 0060/0496: loss=2.628403663635254\n",
            "epoch 007/010, batch 001/002, step 0061/0496: loss=2.431818723678589\n",
            "epoch 007/010, batch 001/002, step 0062/0496: loss=2.5234928131103516\n",
            "epoch 007/010, batch 001/002, step 0063/0496: loss=2.5207114219665527\n",
            "epoch 007/010, batch 001/002, step 0064/0496: loss=1.636488676071167\n",
            "epoch 007/010, batch 001/002, step 0065/0496: loss=4.097200870513916\n",
            "epoch 007/010, batch 001/002, step 0066/0496: loss=1.9325153827667236\n",
            "epoch 007/010, batch 001/002, step 0067/0496: loss=2.3721163272857666\n",
            "epoch 007/010, batch 001/002, step 0068/0496: loss=1.8890331983566284\n",
            "epoch 007/010, batch 001/002, step 0069/0496: loss=4.379027843475342\n",
            "epoch 007/010, batch 001/002, step 0070/0496: loss=2.2827329635620117\n",
            "epoch 007/010, batch 001/002, step 0071/0496: loss=4.152673721313477\n",
            "epoch 007/010, batch 001/002, step 0072/0496: loss=1.7972437143325806\n",
            "epoch 007/010, batch 001/002, step 0073/0496: loss=2.7637743949890137\n",
            "epoch 007/010, batch 001/002, step 0074/0496: loss=2.4232101440429688\n",
            "epoch 007/010, batch 001/002, step 0075/0496: loss=2.7036571502685547\n",
            "epoch 007/010, batch 001/002, step 0076/0496: loss=1.5302059650421143\n",
            "epoch 007/010, batch 001/002, step 0077/0496: loss=4.054629802703857\n",
            "epoch 007/010, batch 001/002, step 0078/0496: loss=2.137108325958252\n",
            "epoch 007/010, batch 001/002, step 0079/0496: loss=2.0804476737976074\n",
            "epoch 007/010, batch 001/002, step 0080/0496: loss=2.289094924926758\n",
            "epoch 007/010, batch 001/002, step 0081/0496: loss=3.019524097442627\n",
            "epoch 007/010, batch 001/002, step 0082/0496: loss=2.616119384765625\n",
            "epoch 007/010, batch 001/002, step 0083/0496: loss=2.7155394554138184\n",
            "epoch 007/010, batch 001/002, step 0084/0496: loss=2.879450559616089\n",
            "epoch 007/010, batch 001/002, step 0085/0496: loss=2.377274990081787\n",
            "epoch 007/010, batch 001/002, step 0086/0496: loss=2.1766812801361084\n",
            "epoch 007/010, batch 001/002, step 0087/0496: loss=2.5418076515197754\n",
            "epoch 007/010, batch 001/002, step 0088/0496: loss=3.640573501586914\n",
            "epoch 007/010, batch 001/002, step 0089/0496: loss=3.413256883621216\n",
            "epoch 007/010, batch 001/002, step 0090/0496: loss=2.720975875854492\n",
            "epoch 007/010, batch 001/002, step 0091/0496: loss=2.0852606296539307\n",
            "epoch 007/010, batch 001/002, step 0092/0496: loss=3.0516858100891113\n",
            "epoch 007/010, batch 001/002, step 0093/0496: loss=2.028761148452759\n",
            "epoch 007/010, batch 001/002, step 0094/0496: loss=2.457070827484131\n",
            "epoch 007/010, batch 001/002, step 0095/0496: loss=2.427380084991455\n",
            "epoch 007/010, batch 001/002, step 0096/0496: loss=2.6346535682678223\n",
            "epoch 007/010, batch 001/002, step 0097/0496: loss=2.8805737495422363\n",
            "epoch 007/010, batch 001/002, step 0098/0496: loss=1.8599745035171509\n",
            "epoch 007/010, batch 001/002, step 0099/0496: loss=2.318033218383789\n",
            "epoch 007/010, batch 001/002, step 0100/0496: loss=1.914054274559021\n",
            "epoch 007/010, batch 001/002, step 0101/0496: loss=3.101775646209717\n",
            "epoch 007/010, batch 001/002, step 0102/0496: loss=2.355073928833008\n",
            "epoch 007/010, batch 001/002, step 0103/0496: loss=2.2376813888549805\n",
            "epoch 007/010, batch 001/002, step 0104/0496: loss=2.082080602645874\n",
            "epoch 007/010, batch 001/002, step 0105/0496: loss=2.020435333251953\n",
            "epoch 007/010, batch 001/002, step 0106/0496: loss=1.9992519617080688\n",
            "epoch 007/010, batch 001/002, step 0107/0496: loss=2.3683815002441406\n",
            "epoch 007/010, batch 001/002, step 0108/0496: loss=2.8427672386169434\n",
            "epoch 007/010, batch 001/002, step 0109/0496: loss=2.20004940032959\n",
            "epoch 007/010, batch 001/002, step 0110/0496: loss=2.2801616191864014\n",
            "epoch 007/010, batch 001/002, step 0111/0496: loss=2.566732168197632\n",
            "epoch 007/010, batch 001/002, step 0112/0496: loss=2.133496046066284\n",
            "epoch 007/010, batch 001/002, step 0113/0496: loss=1.8400278091430664\n",
            "epoch 007/010, batch 001/002, step 0114/0496: loss=2.135847568511963\n",
            "epoch 007/010, batch 001/002, step 0115/0496: loss=1.9317405223846436\n",
            "epoch 007/010, batch 001/002, step 0116/0496: loss=2.875077247619629\n",
            "epoch 007/010, batch 001/002, step 0117/0496: loss=2.49178409576416\n",
            "epoch 007/010, batch 001/002, step 0118/0496: loss=2.0373499393463135\n",
            "epoch 007/010, batch 001/002, step 0119/0496: loss=2.0699634552001953\n",
            "epoch 007/010, batch 001/002, step 0120/0496: loss=1.9055588245391846\n",
            "epoch 007/010, batch 001/002, step 0121/0496: loss=2.3265085220336914\n",
            "epoch 007/010, batch 001/002, step 0122/0496: loss=1.794951319694519\n",
            "epoch 007/010, batch 001/002, step 0123/0496: loss=2.3508968353271484\n",
            "epoch 007/010, batch 001/002, step 0124/0496: loss=1.8372523784637451\n",
            "epoch 007/010, batch 001/002, step 0125/0496: loss=2.6292929649353027\n",
            "epoch 007/010, batch 001/002, step 0126/0496: loss=2.2212183475494385\n",
            "epoch 007/010, batch 001/002, step 0127/0496: loss=1.9438364505767822\n",
            "epoch 007/010, batch 001/002, step 0128/0496: loss=1.9872729778289795\n",
            "epoch 007/010, batch 001/002, step 0129/0496: loss=2.4561498165130615\n",
            "epoch 007/010, batch 001/002, step 0130/0496: loss=2.523864269256592\n",
            "epoch 007/010, batch 001/002, step 0131/0496: loss=2.226623058319092\n",
            "epoch 007/010, batch 001/002, step 0132/0496: loss=2.107090950012207\n",
            "epoch 007/010, batch 001/002, step 0133/0496: loss=2.1007015705108643\n",
            "epoch 007/010, batch 001/002, step 0134/0496: loss=1.7520415782928467\n",
            "epoch 007/010, batch 001/002, step 0135/0496: loss=1.6322624683380127\n",
            "epoch 007/010, batch 001/002, step 0136/0496: loss=2.705411911010742\n",
            "epoch 007/010, batch 001/002, step 0137/0496: loss=2.604043960571289\n",
            "epoch 007/010, batch 001/002, step 0138/0496: loss=1.781333088874817\n",
            "epoch 007/010, batch 001/002, step 0139/0496: loss=1.691373348236084\n",
            "epoch 007/010, batch 001/002, step 0140/0496: loss=2.1101977825164795\n",
            "epoch 007/010, batch 001/002, step 0141/0496: loss=2.506117105484009\n",
            "epoch 007/010, batch 001/002, step 0142/0496: loss=3.36971378326416\n",
            "epoch 007/010, batch 001/002, step 0143/0496: loss=2.209197521209717\n",
            "epoch 007/010, batch 001/002, step 0144/0496: loss=2.0470004081726074\n",
            "epoch 007/010, batch 001/002, step 0145/0496: loss=2.901054859161377\n",
            "epoch 007/010, batch 001/002, step 0146/0496: loss=2.177644729614258\n",
            "epoch 007/010, batch 001/002, step 0147/0496: loss=2.8266897201538086\n",
            "epoch 007/010, batch 001/002, step 0148/0496: loss=2.3330883979797363\n",
            "epoch 007/010, batch 001/002, step 0149/0496: loss=1.8954486846923828\n",
            "epoch 007/010, batch 001/002, step 0150/0496: loss=2.3955092430114746\n",
            "epoch 007/010, batch 001/002, step 0151/0496: loss=2.3743274211883545\n",
            "epoch 007/010, batch 001/002, step 0152/0496: loss=3.9377806186676025\n",
            "epoch 007/010, batch 001/002, step 0153/0496: loss=4.190102577209473\n",
            "epoch 007/010, batch 001/002, step 0154/0496: loss=2.6359314918518066\n",
            "epoch 007/010, batch 001/002, step 0155/0496: loss=2.105544328689575\n",
            "epoch 007/010, batch 001/002, step 0156/0496: loss=2.3131790161132812\n",
            "epoch 007/010, batch 001/002, step 0157/0496: loss=2.5920028686523438\n",
            "epoch 007/010, batch 001/002, step 0158/0496: loss=1.9103198051452637\n",
            "epoch 007/010, batch 001/002, step 0159/0496: loss=2.052867889404297\n",
            "epoch 007/010, batch 001/002, step 0160/0496: loss=2.2699813842773438\n",
            "epoch 007/010, batch 001/002, step 0161/0496: loss=1.9774196147918701\n",
            "epoch 007/010, batch 001/002, step 0162/0496: loss=1.8666205406188965\n",
            "epoch 007/010, batch 001/002, step 0163/0496: loss=1.7818963527679443\n",
            "epoch 007/010, batch 001/002, step 0164/0496: loss=2.0535693168640137\n",
            "epoch 007/010, batch 001/002, step 0165/0496: loss=1.9045771360397339\n",
            "epoch 007/010, batch 001/002, step 0166/0496: loss=2.035278081893921\n",
            "epoch 007/010, batch 001/002, step 0167/0496: loss=2.2057390213012695\n",
            "epoch 007/010, batch 001/002, step 0168/0496: loss=3.399951457977295\n",
            "epoch 007/010, batch 001/002, step 0169/0496: loss=2.413127899169922\n",
            "epoch 007/010, batch 001/002, step 0170/0496: loss=3.9205613136291504\n",
            "epoch 007/010, batch 001/002, step 0171/0496: loss=1.874277949333191\n",
            "epoch 007/010, batch 001/002, step 0172/0496: loss=3.630789279937744\n",
            "epoch 007/010, batch 001/002, step 0173/0496: loss=4.370401382446289\n",
            "epoch 007/010, batch 001/002, step 0174/0496: loss=2.5767931938171387\n",
            "epoch 007/010, batch 001/002, step 0175/0496: loss=2.4616293907165527\n",
            "epoch 007/010, batch 001/002, step 0176/0496: loss=2.6955857276916504\n",
            "epoch 007/010, batch 001/002, step 0177/0496: loss=2.6292929649353027\n",
            "epoch 007/010, batch 001/002, step 0178/0496: loss=2.061702251434326\n",
            "epoch 007/010, batch 001/002, step 0179/0496: loss=2.7438783645629883\n",
            "epoch 007/010, batch 001/002, step 0180/0496: loss=2.6407039165496826\n",
            "epoch 007/010, batch 001/002, step 0181/0496: loss=1.6234756708145142\n",
            "epoch 007/010, batch 001/002, step 0182/0496: loss=2.2203757762908936\n",
            "epoch 007/010, batch 001/002, step 0183/0496: loss=2.2728376388549805\n",
            "epoch 007/010, batch 001/002, step 0184/0496: loss=2.153402805328369\n",
            "epoch 007/010, batch 001/002, step 0185/0496: loss=2.5491490364074707\n",
            "epoch 007/010, batch 001/002, step 0186/0496: loss=2.7859692573547363\n",
            "epoch 007/010, batch 001/002, step 0187/0496: loss=2.3023877143859863\n",
            "epoch 007/010, batch 001/002, step 0188/0496: loss=2.160785675048828\n",
            "epoch 007/010, batch 001/002, step 0189/0496: loss=2.3523499965667725\n",
            "epoch 007/010, batch 001/002, step 0190/0496: loss=2.7271780967712402\n",
            "epoch 007/010, batch 001/002, step 0191/0496: loss=2.0953943729400635\n",
            "epoch 007/010, batch 001/002, step 0192/0496: loss=3.6598260402679443\n",
            "epoch 007/010, batch 001/002, step 0193/0496: loss=2.4504685401916504\n",
            "epoch 007/010, batch 001/002, step 0194/0496: loss=2.4329888820648193\n",
            "epoch 007/010, batch 001/002, step 0195/0496: loss=2.744813919067383\n",
            "epoch 007/010, batch 001/002, step 0196/0496: loss=2.9084348678588867\n",
            "epoch 007/010, batch 001/002, step 0197/0496: loss=2.103896141052246\n",
            "epoch 007/010, batch 001/002, step 0198/0496: loss=3.180419683456421\n",
            "epoch 007/010, batch 001/002, step 0199/0496: loss=3.2724668979644775\n",
            "epoch 007/010, batch 001/002, step 0200/0496: loss=1.9085195064544678\n",
            "epoch 007/010, batch 001/002, step 0201/0496: loss=2.3735861778259277\n",
            "epoch 007/010, batch 001/002, step 0202/0496: loss=2.3380565643310547\n",
            "epoch 007/010, batch 001/002, step 0203/0496: loss=3.7388949394226074\n",
            "epoch 007/010, batch 001/002, step 0204/0496: loss=2.9255857467651367\n",
            "epoch 007/010, batch 001/002, step 0205/0496: loss=1.9006845951080322\n",
            "epoch 007/010, batch 001/002, step 0206/0496: loss=4.331223487854004\n",
            "epoch 007/010, batch 001/002, step 0207/0496: loss=3.7496018409729004\n",
            "epoch 007/010, batch 001/002, step 0208/0496: loss=2.7795896530151367\n",
            "epoch 007/010, batch 001/002, step 0209/0496: loss=3.5107007026672363\n",
            "epoch 007/010, batch 001/002, step 0210/0496: loss=2.828968048095703\n",
            "epoch 007/010, batch 001/002, step 0211/0496: loss=3.456037998199463\n",
            "epoch 007/010, batch 001/002, step 0212/0496: loss=2.737851619720459\n",
            "epoch 007/010, batch 001/002, step 0213/0496: loss=2.7409801483154297\n",
            "epoch 007/010, batch 001/002, step 0214/0496: loss=2.468881368637085\n",
            "epoch 007/010, batch 001/002, step 0215/0496: loss=3.507784843444824\n",
            "epoch 007/010, batch 001/002, step 0216/0496: loss=2.041027545928955\n",
            "epoch 007/010, batch 001/002, step 0217/0496: loss=1.9499725103378296\n",
            "epoch 007/010, batch 001/002, step 0218/0496: loss=2.6690382957458496\n",
            "epoch 007/010, batch 001/002, step 0219/0496: loss=2.464951515197754\n",
            "epoch 007/010, batch 001/002, step 0220/0496: loss=3.088102340698242\n",
            "epoch 007/010, batch 001/002, step 0221/0496: loss=3.9519083499908447\n",
            "epoch 007/010, batch 001/002, step 0222/0496: loss=3.01049542427063\n",
            "epoch 007/010, batch 001/002, step 0223/0496: loss=2.802147388458252\n",
            "epoch 007/010, batch 001/002, step 0224/0496: loss=2.7771663665771484\n",
            "epoch 007/010, batch 001/002, step 0225/0496: loss=2.9135282039642334\n",
            "epoch 007/010, batch 001/002, step 0226/0496: loss=2.523545742034912\n",
            "epoch 007/010, batch 001/002, step 0227/0496: loss=2.6772680282592773\n",
            "epoch 007/010, batch 001/002, step 0228/0496: loss=1.8610183000564575\n",
            "epoch 007/010, batch 001/002, step 0229/0496: loss=2.0549569129943848\n",
            "epoch 007/010, batch 001/002, step 0230/0496: loss=2.4649343490600586\n",
            "epoch 007/010, batch 001/002, step 0231/0496: loss=2.2260000705718994\n",
            "epoch 007/010, batch 001/002, step 0232/0496: loss=2.727391242980957\n",
            "epoch 007/010, batch 001/002, step 0233/0496: loss=2.5473320484161377\n",
            "epoch 007/010, batch 001/002, step 0234/0496: loss=2.3046412467956543\n",
            "epoch 007/010, batch 001/002, step 0235/0496: loss=2.661220073699951\n",
            "epoch 007/010, batch 001/002, step 0236/0496: loss=2.1495039463043213\n",
            "epoch 007/010, batch 001/002, step 0237/0496: loss=2.7546377182006836\n",
            "epoch 007/010, batch 001/002, step 0238/0496: loss=1.6875921487808228\n",
            "epoch 007/010, batch 001/002, step 0239/0496: loss=1.971216082572937\n",
            "epoch 007/010, batch 001/002, step 0240/0496: loss=1.9464082717895508\n",
            "epoch 007/010, batch 001/002, step 0241/0496: loss=3.0730082988739014\n",
            "epoch 007/010, batch 001/002, step 0242/0496: loss=2.834106683731079\n",
            "epoch 007/010, batch 001/002, step 0243/0496: loss=3.4144301414489746\n",
            "epoch 007/010, batch 001/002, step 0244/0496: loss=2.5241141319274902\n",
            "epoch 007/010, batch 001/002, step 0245/0496: loss=3.104870080947876\n",
            "epoch 007/010, batch 001/002, step 0246/0496: loss=2.656186580657959\n",
            "epoch 007/010, batch 001/002, step 0247/0496: loss=2.1464247703552246\n",
            "epoch 007/010, batch 001/002, step 0248/0496: loss=2.14322566986084\n",
            "epoch 007/010, batch 001/002, step 0249/0496: loss=2.5560410022735596\n",
            "epoch 007/010, batch 001/002, step 0250/0496: loss=2.3210887908935547\n",
            "epoch 007/010, batch 001/002, step 0251/0496: loss=3.4554455280303955\n",
            "epoch 007/010, batch 001/002, step 0252/0496: loss=2.3834168910980225\n",
            "epoch 007/010, batch 001/002, step 0253/0496: loss=2.4843945503234863\n",
            "epoch 007/010, batch 001/002, step 0254/0496: loss=2.563383102416992\n",
            "epoch 007/010, batch 001/002, step 0255/0496: loss=2.358774185180664\n",
            "epoch 007/010, batch 001/002, step 0256/0496: loss=1.665116548538208\n",
            "epoch 007/010, batch 001/002, step 0257/0496: loss=3.4620418548583984\n",
            "epoch 007/010, batch 001/002, step 0258/0496: loss=2.0557444095611572\n",
            "epoch 007/010, batch 001/002, step 0259/0496: loss=2.6073660850524902\n",
            "epoch 007/010, batch 001/002, step 0260/0496: loss=2.292491912841797\n",
            "epoch 007/010, batch 001/002, step 0261/0496: loss=2.1135053634643555\n",
            "epoch 007/010, batch 001/002, step 0262/0496: loss=1.9571528434753418\n",
            "epoch 007/010, batch 001/002, step 0263/0496: loss=2.4908580780029297\n",
            "epoch 007/010, batch 001/002, step 0264/0496: loss=3.693636417388916\n",
            "epoch 007/010, batch 001/002, step 0265/0496: loss=2.460176944732666\n",
            "epoch 007/010, batch 001/002, step 0266/0496: loss=2.565927505493164\n",
            "epoch 007/010, batch 001/002, step 0267/0496: loss=2.6299946308135986\n",
            "epoch 007/010, batch 001/002, step 0268/0496: loss=2.0837841033935547\n",
            "epoch 007/010, batch 001/002, step 0269/0496: loss=2.3766753673553467\n",
            "epoch 007/010, batch 001/002, step 0270/0496: loss=2.742164134979248\n",
            "epoch 007/010, batch 001/002, step 0271/0496: loss=2.9126834869384766\n",
            "epoch 007/010, batch 001/002, step 0272/0496: loss=2.9272890090942383\n",
            "epoch 007/010, batch 001/002, step 0273/0496: loss=1.994200587272644\n",
            "epoch 007/010, batch 001/002, step 0274/0496: loss=2.336009979248047\n",
            "epoch 007/010, batch 001/002, step 0275/0496: loss=1.7753914594650269\n",
            "epoch 007/010, batch 001/002, step 0276/0496: loss=2.5981342792510986\n",
            "epoch 007/010, batch 001/002, step 0277/0496: loss=1.8791619539260864\n",
            "epoch 007/010, batch 001/002, step 0278/0496: loss=2.061880588531494\n",
            "epoch 007/010, batch 001/002, step 0279/0496: loss=2.462522029876709\n",
            "epoch 007/010, batch 001/002, step 0280/0496: loss=2.242770195007324\n",
            "epoch 007/010, batch 001/002, step 0281/0496: loss=2.832273006439209\n",
            "epoch 007/010, batch 001/002, step 0282/0496: loss=2.4217581748962402\n",
            "epoch 007/010, batch 001/002, step 0283/0496: loss=2.485051155090332\n",
            "epoch 007/010, batch 001/002, step 0284/0496: loss=2.2813796997070312\n",
            "epoch 007/010, batch 001/002, step 0285/0496: loss=2.0648460388183594\n",
            "epoch 007/010, batch 001/002, step 0286/0496: loss=2.1771671772003174\n",
            "epoch 007/010, batch 001/002, step 0287/0496: loss=1.9784036874771118\n",
            "epoch 007/010, batch 001/002, step 0288/0496: loss=3.073014259338379\n",
            "epoch 007/010, batch 001/002, step 0289/0496: loss=2.6270718574523926\n",
            "epoch 007/010, batch 001/002, step 0290/0496: loss=1.924825668334961\n",
            "epoch 007/010, batch 001/002, step 0291/0496: loss=2.126502513885498\n",
            "epoch 007/010, batch 001/002, step 0292/0496: loss=2.5279431343078613\n",
            "epoch 007/010, batch 001/002, step 0293/0496: loss=2.752340078353882\n",
            "epoch 007/010, batch 001/002, step 0294/0496: loss=1.7622668743133545\n",
            "epoch 007/010, batch 001/002, step 0295/0496: loss=1.9687668085098267\n",
            "epoch 007/010, batch 001/002, step 0296/0496: loss=2.83730411529541\n",
            "epoch 007/010, batch 001/002, step 0297/0496: loss=2.295423984527588\n",
            "epoch 007/010, batch 001/002, step 0298/0496: loss=2.282762050628662\n",
            "epoch 007/010, batch 001/002, step 0299/0496: loss=2.4711484909057617\n",
            "epoch 007/010, batch 001/002, step 0300/0496: loss=1.8130159378051758\n",
            "epoch 007/010, batch 001/002, step 0301/0496: loss=2.1890666484832764\n",
            "epoch 007/010, batch 001/002, step 0302/0496: loss=1.9078234434127808\n",
            "epoch 007/010, batch 001/002, step 0303/0496: loss=1.9684776067733765\n",
            "epoch 007/010, batch 001/002, step 0304/0496: loss=2.7945570945739746\n",
            "epoch 007/010, batch 001/002, step 0305/0496: loss=2.347723960876465\n",
            "epoch 007/010, batch 001/002, step 0306/0496: loss=1.8323826789855957\n",
            "epoch 007/010, batch 001/002, step 0307/0496: loss=2.2901344299316406\n",
            "epoch 007/010, batch 001/002, step 0308/0496: loss=1.8420052528381348\n",
            "epoch 007/010, batch 001/002, step 0309/0496: loss=2.1025118827819824\n",
            "epoch 007/010, batch 001/002, step 0310/0496: loss=1.8585525751113892\n",
            "epoch 007/010, batch 001/002, step 0311/0496: loss=2.6781575679779053\n",
            "epoch 007/010, batch 001/002, step 0312/0496: loss=1.618622064590454\n",
            "epoch 007/010, batch 001/002, step 0313/0496: loss=2.271174192428589\n",
            "epoch 007/010, batch 001/002, step 0314/0496: loss=1.8323193788528442\n",
            "epoch 007/010, batch 001/002, step 0315/0496: loss=3.6412811279296875\n",
            "epoch 007/010, batch 001/002, step 0316/0496: loss=2.80391001701355\n",
            "epoch 007/010, batch 001/002, step 0317/0496: loss=1.95451021194458\n",
            "epoch 007/010, batch 001/002, step 0318/0496: loss=1.8347727060317993\n",
            "epoch 007/010, batch 001/002, step 0319/0496: loss=2.5217528343200684\n",
            "epoch 007/010, batch 001/002, step 0320/0496: loss=2.4230942726135254\n",
            "epoch 007/010, batch 001/002, step 0321/0496: loss=2.3576736450195312\n",
            "epoch 007/010, batch 001/002, step 0322/0496: loss=2.086876392364502\n",
            "epoch 007/010, batch 001/002, step 0323/0496: loss=2.8010404109954834\n",
            "epoch 007/010, batch 001/002, step 0324/0496: loss=2.5639166831970215\n",
            "epoch 007/010, batch 001/002, step 0325/0496: loss=2.4321093559265137\n",
            "epoch 007/010, batch 001/002, step 0326/0496: loss=2.1442294120788574\n",
            "epoch 007/010, batch 001/002, step 0327/0496: loss=3.2362279891967773\n",
            "epoch 007/010, batch 001/002, step 0328/0496: loss=2.0678861141204834\n",
            "epoch 007/010, batch 001/002, step 0329/0496: loss=1.7457950115203857\n",
            "epoch 007/010, batch 001/002, step 0330/0496: loss=2.0053811073303223\n",
            "epoch 007/010, batch 001/002, step 0331/0496: loss=1.6134662628173828\n",
            "epoch 007/010, batch 001/002, step 0332/0496: loss=2.3996410369873047\n",
            "epoch 007/010, batch 001/002, step 0333/0496: loss=1.9908546209335327\n",
            "epoch 007/010, batch 001/002, step 0334/0496: loss=2.1974682807922363\n",
            "epoch 007/010, batch 001/002, step 0335/0496: loss=2.3778371810913086\n",
            "epoch 007/010, batch 001/002, step 0336/0496: loss=2.225121021270752\n",
            "epoch 007/010, batch 001/002, step 0337/0496: loss=2.497103691101074\n",
            "epoch 007/010, batch 001/002, step 0338/0496: loss=2.354815721511841\n",
            "epoch 007/010, batch 001/002, step 0339/0496: loss=2.9149630069732666\n",
            "epoch 007/010, batch 001/002, step 0340/0496: loss=2.7430505752563477\n",
            "epoch 007/010, batch 001/002, step 0341/0496: loss=4.140226364135742\n",
            "epoch 007/010, batch 001/002, step 0342/0496: loss=2.2002859115600586\n",
            "epoch 007/010, batch 001/002, step 0343/0496: loss=1.609459638595581\n",
            "epoch 007/010, batch 001/002, step 0344/0496: loss=1.8381116390228271\n",
            "epoch 007/010, batch 001/002, step 0345/0496: loss=2.5749704837799072\n",
            "epoch 007/010, batch 001/002, step 0346/0496: loss=2.588881492614746\n",
            "epoch 007/010, batch 001/002, step 0347/0496: loss=2.1185925006866455\n",
            "epoch 007/010, batch 001/002, step 0348/0496: loss=2.4040133953094482\n",
            "epoch 007/010, batch 001/002, step 0349/0496: loss=2.1043167114257812\n",
            "epoch 007/010, batch 001/002, step 0350/0496: loss=1.9838218688964844\n",
            "epoch 007/010, batch 001/002, step 0351/0496: loss=2.3032126426696777\n",
            "epoch 007/010, batch 001/002, step 0352/0496: loss=1.9262995719909668\n",
            "epoch 007/010, batch 001/002, step 0353/0496: loss=1.9378857612609863\n",
            "epoch 007/010, batch 001/002, step 0354/0496: loss=2.2211966514587402\n",
            "epoch 007/010, batch 001/002, step 0355/0496: loss=1.6657965183258057\n",
            "epoch 007/010, batch 001/002, step 0356/0496: loss=2.817485809326172\n",
            "epoch 007/010, batch 001/002, step 0357/0496: loss=1.8712259531021118\n",
            "epoch 007/010, batch 001/002, step 0358/0496: loss=2.1660232543945312\n",
            "epoch 007/010, batch 001/002, step 0359/0496: loss=1.9908405542373657\n",
            "epoch 007/010, batch 001/002, step 0360/0496: loss=2.60479736328125\n",
            "epoch 007/010, batch 001/002, step 0361/0496: loss=1.95496666431427\n",
            "epoch 007/010, batch 001/002, step 0362/0496: loss=2.8500750064849854\n",
            "epoch 007/010, batch 001/002, step 0363/0496: loss=4.283889293670654\n",
            "epoch 007/010, batch 001/002, step 0364/0496: loss=2.186964511871338\n",
            "epoch 007/010, batch 001/002, step 0365/0496: loss=1.9073519706726074\n",
            "epoch 007/010, batch 001/002, step 0366/0496: loss=3.1191341876983643\n",
            "epoch 007/010, batch 001/002, step 0367/0496: loss=3.3492178916931152\n",
            "epoch 007/010, batch 001/002, step 0368/0496: loss=3.8547520637512207\n",
            "epoch 007/010, batch 001/002, step 0369/0496: loss=2.621664047241211\n",
            "epoch 007/010, batch 001/002, step 0370/0496: loss=1.8724571466445923\n",
            "epoch 007/010, batch 001/002, step 0371/0496: loss=2.912101984024048\n",
            "epoch 007/010, batch 001/002, step 0372/0496: loss=3.346245765686035\n",
            "epoch 007/010, batch 001/002, step 0373/0496: loss=3.2359235286712646\n",
            "epoch 007/010, batch 001/002, step 0374/0496: loss=2.274116277694702\n",
            "epoch 007/010, batch 001/002, step 0375/0496: loss=2.1895904541015625\n",
            "epoch 007/010, batch 001/002, step 0376/0496: loss=2.9353644847869873\n",
            "epoch 007/010, batch 001/002, step 0377/0496: loss=2.472730875015259\n",
            "epoch 007/010, batch 001/002, step 0378/0496: loss=3.153092622756958\n",
            "epoch 007/010, batch 001/002, step 0379/0496: loss=2.665879249572754\n",
            "epoch 007/010, batch 001/002, step 0380/0496: loss=2.138033390045166\n",
            "epoch 007/010, batch 001/002, step 0381/0496: loss=2.5221469402313232\n",
            "epoch 007/010, batch 001/002, step 0382/0496: loss=2.2298407554626465\n",
            "epoch 007/010, batch 001/002, step 0383/0496: loss=2.3585000038146973\n",
            "epoch 007/010, batch 001/002, step 0384/0496: loss=1.776017189025879\n",
            "epoch 007/010, batch 001/002, step 0385/0496: loss=2.4498417377471924\n",
            "epoch 007/010, batch 001/002, step 0386/0496: loss=2.270874500274658\n",
            "epoch 007/010, batch 001/002, step 0387/0496: loss=2.8958444595336914\n",
            "epoch 007/010, batch 001/002, step 0388/0496: loss=2.7454216480255127\n",
            "epoch 007/010, batch 001/002, step 0389/0496: loss=1.6577731370925903\n",
            "epoch 007/010, batch 001/002, step 0390/0496: loss=3.023573160171509\n",
            "epoch 007/010, batch 001/002, step 0391/0496: loss=1.9683198928833008\n",
            "epoch 007/010, batch 001/002, step 0392/0496: loss=2.5268306732177734\n",
            "epoch 007/010, batch 001/002, step 0393/0496: loss=1.881784439086914\n",
            "epoch 007/010, batch 001/002, step 0394/0496: loss=2.755425214767456\n",
            "epoch 007/010, batch 001/002, step 0395/0496: loss=2.8956804275512695\n",
            "epoch 007/010, batch 001/002, step 0396/0496: loss=2.62579083442688\n",
            "epoch 007/010, batch 001/002, step 0397/0496: loss=1.9242748022079468\n",
            "epoch 007/010, batch 001/002, step 0398/0496: loss=3.51961612701416\n",
            "epoch 007/010, batch 001/002, step 0399/0496: loss=2.0468454360961914\n",
            "epoch 007/010, batch 001/002, step 0400/0496: loss=1.9565116167068481\n",
            "epoch 007/010, batch 001/002, step 0401/0496: loss=1.9621484279632568\n",
            "epoch 007/010, batch 001/002, step 0402/0496: loss=4.3684539794921875\n",
            "epoch 007/010, batch 001/002, step 0403/0496: loss=2.469276189804077\n",
            "epoch 007/010, batch 001/002, step 0404/0496: loss=2.7590482234954834\n",
            "epoch 007/010, batch 001/002, step 0405/0496: loss=3.2694578170776367\n",
            "epoch 007/010, batch 001/002, step 0406/0496: loss=2.933133363723755\n",
            "epoch 007/010, batch 001/002, step 0407/0496: loss=2.4589221477508545\n",
            "epoch 007/010, batch 001/002, step 0408/0496: loss=2.4903228282928467\n",
            "epoch 007/010, batch 001/002, step 0409/0496: loss=2.111893653869629\n",
            "epoch 007/010, batch 001/002, step 0410/0496: loss=1.8797963857650757\n",
            "epoch 007/010, batch 001/002, step 0411/0496: loss=2.297614574432373\n",
            "epoch 007/010, batch 001/002, step 0412/0496: loss=3.9270896911621094\n",
            "epoch 007/010, batch 001/002, step 0413/0496: loss=2.529498338699341\n",
            "epoch 007/010, batch 001/002, step 0414/0496: loss=2.5632948875427246\n",
            "epoch 007/010, batch 001/002, step 0415/0496: loss=1.8214572668075562\n",
            "epoch 007/010, batch 001/002, step 0416/0496: loss=1.7072560787200928\n",
            "epoch 007/010, batch 001/002, step 0417/0496: loss=2.600261688232422\n",
            "epoch 007/010, batch 001/002, step 0418/0496: loss=1.8681145906448364\n",
            "epoch 007/010, batch 001/002, step 0419/0496: loss=2.1877965927124023\n",
            "epoch 007/010, batch 001/002, step 0420/0496: loss=1.8198384046554565\n",
            "epoch 007/010, batch 001/002, step 0421/0496: loss=1.844017744064331\n",
            "epoch 007/010, batch 001/002, step 0422/0496: loss=2.0867884159088135\n",
            "epoch 007/010, batch 001/002, step 0423/0496: loss=2.595782995223999\n",
            "epoch 007/010, batch 001/002, step 0424/0496: loss=2.292896270751953\n",
            "epoch 007/010, batch 001/002, step 0425/0496: loss=2.314523220062256\n",
            "epoch 007/010, batch 001/002, step 0426/0496: loss=2.5679290294647217\n",
            "epoch 007/010, batch 001/002, step 0427/0496: loss=2.367173433303833\n",
            "epoch 007/010, batch 001/002, step 0428/0496: loss=2.069938898086548\n",
            "epoch 007/010, batch 001/002, step 0429/0496: loss=2.0288689136505127\n",
            "epoch 007/010, batch 001/002, step 0430/0496: loss=1.888624668121338\n",
            "epoch 007/010, batch 001/002, step 0431/0496: loss=2.7977099418640137\n",
            "epoch 007/010, batch 001/002, step 0432/0496: loss=2.882195472717285\n",
            "epoch 007/010, batch 001/002, step 0433/0496: loss=1.9160897731781006\n",
            "epoch 007/010, batch 001/002, step 0434/0496: loss=2.342339515686035\n",
            "epoch 007/010, batch 001/002, step 0435/0496: loss=1.7842910289764404\n",
            "epoch 007/010, batch 001/002, step 0436/0496: loss=1.9932023286819458\n",
            "epoch 007/010, batch 001/002, step 0437/0496: loss=2.23422908782959\n",
            "epoch 007/010, batch 001/002, step 0438/0496: loss=1.7825875282287598\n",
            "epoch 007/010, batch 001/002, step 0439/0496: loss=2.003715991973877\n",
            "epoch 007/010, batch 001/002, step 0440/0496: loss=1.8450183868408203\n",
            "epoch 007/010, batch 001/002, step 0441/0496: loss=1.7531601190567017\n",
            "epoch 007/010, batch 001/002, step 0442/0496: loss=2.850663185119629\n",
            "epoch 007/010, batch 001/002, step 0443/0496: loss=1.610051155090332\n",
            "epoch 007/010, batch 001/002, step 0444/0496: loss=2.515516519546509\n",
            "epoch 007/010, batch 001/002, step 0445/0496: loss=2.0626420974731445\n",
            "epoch 007/010, batch 001/002, step 0446/0496: loss=2.396522045135498\n",
            "epoch 007/010, batch 001/002, step 0447/0496: loss=2.489272117614746\n",
            "epoch 007/010, batch 001/002, step 0448/0496: loss=2.221637487411499\n",
            "epoch 007/010, batch 001/002, step 0449/0496: loss=2.1068217754364014\n",
            "epoch 007/010, batch 001/002, step 0450/0496: loss=3.9030301570892334\n",
            "epoch 007/010, batch 001/002, step 0451/0496: loss=1.892656922340393\n",
            "epoch 007/010, batch 001/002, step 0452/0496: loss=1.8203601837158203\n",
            "epoch 007/010, batch 001/002, step 0453/0496: loss=2.218632936477661\n",
            "epoch 007/010, batch 001/002, step 0454/0496: loss=2.4268178939819336\n",
            "epoch 007/010, batch 001/002, step 0455/0496: loss=2.199540615081787\n",
            "epoch 007/010, batch 001/002, step 0456/0496: loss=2.125718355178833\n",
            "epoch 007/010, batch 001/002, step 0457/0496: loss=1.5033965110778809\n",
            "epoch 007/010, batch 001/002, step 0458/0496: loss=2.3940935134887695\n",
            "epoch 007/010, batch 001/002, step 0459/0496: loss=1.916642665863037\n",
            "epoch 007/010, batch 001/002, step 0460/0496: loss=1.61372971534729\n",
            "epoch 007/010, batch 001/002, step 0461/0496: loss=2.340116262435913\n",
            "epoch 007/010, batch 001/002, step 0462/0496: loss=2.950530529022217\n",
            "epoch 007/010, batch 001/002, step 0463/0496: loss=2.376469135284424\n",
            "epoch 007/010, batch 001/002, step 0464/0496: loss=2.51540470123291\n",
            "epoch 007/010, batch 001/002, step 0465/0496: loss=1.726313829421997\n",
            "epoch 007/010, batch 001/002, step 0466/0496: loss=1.6982026100158691\n",
            "epoch 007/010, batch 001/002, step 0467/0496: loss=1.740149974822998\n",
            "epoch 007/010, batch 001/002, step 0468/0496: loss=1.962610125541687\n",
            "epoch 007/010, batch 001/002, step 0469/0496: loss=2.1368257999420166\n",
            "epoch 007/010, batch 001/002, step 0470/0496: loss=2.066157817840576\n",
            "epoch 007/010, batch 001/002, step 0471/0496: loss=2.5149545669555664\n",
            "epoch 007/010, batch 001/002, step 0472/0496: loss=2.0198564529418945\n",
            "epoch 007/010, batch 001/002, step 0473/0496: loss=1.9077980518341064\n",
            "epoch 007/010, batch 001/002, step 0474/0496: loss=2.418593406677246\n",
            "epoch 007/010, batch 001/002, step 0475/0496: loss=2.5817155838012695\n",
            "epoch 007/010, batch 001/002, step 0476/0496: loss=2.2889933586120605\n",
            "epoch 007/010, batch 001/002, step 0477/0496: loss=2.2590041160583496\n",
            "epoch 007/010, batch 001/002, step 0478/0496: loss=2.5130159854888916\n",
            "epoch 007/010, batch 001/002, step 0479/0496: loss=1.7590200901031494\n",
            "epoch 007/010, batch 001/002, step 0480/0496: loss=1.6395121812820435\n",
            "epoch 007/010, batch 001/002, step 0481/0496: loss=1.4786568880081177\n",
            "epoch 007/010, batch 001/002, step 0482/0496: loss=1.9778019189834595\n",
            "epoch 007/010, batch 001/002, step 0483/0496: loss=2.478771686553955\n",
            "epoch 007/010, batch 001/002, step 0484/0496: loss=2.444593906402588\n",
            "epoch 007/010, batch 001/002, step 0485/0496: loss=2.0708067417144775\n",
            "epoch 007/010, batch 001/002, step 0486/0496: loss=2.6264309883117676\n",
            "epoch 007/010, batch 001/002, step 0487/0496: loss=2.6985864639282227\n",
            "epoch 007/010, batch 001/002, step 0488/0496: loss=3.83322811126709\n",
            "epoch 007/010, batch 001/002, step 0489/0496: loss=2.220317840576172\n",
            "epoch 007/010, batch 001/002, step 0490/0496: loss=2.4663443565368652\n",
            "epoch 007/010, batch 001/002, step 0491/0496: loss=2.015502452850342\n",
            "epoch 007/010, batch 001/002, step 0492/0496: loss=2.4637935161590576\n",
            "epoch 007/010, batch 001/002, step 0493/0496: loss=2.252955913543701\n",
            "epoch 007/010, batch 001/002, step 0494/0496: loss=1.8412151336669922\n",
            "epoch 007/010, batch 001/002, step 0495/0496: loss=2.812056064605713\n",
            "epoch 007/010, batch 001/002, step 0496/0496: loss=1.8314179182052612\n",
            "epoch 007/010, batch 002/002, step 0001/0496: loss=2.470160961151123\n",
            "epoch 007/010, batch 002/002, step 0002/0496: loss=2.2137298583984375\n",
            "epoch 007/010, batch 002/002, step 0003/0496: loss=1.730870008468628\n",
            "epoch 007/010, batch 002/002, step 0004/0496: loss=2.4700467586517334\n",
            "epoch 007/010, batch 002/002, step 0005/0496: loss=1.9968585968017578\n",
            "epoch 007/010, batch 002/002, step 0006/0496: loss=2.690152168273926\n",
            "epoch 007/010, batch 002/002, step 0007/0496: loss=2.0682690143585205\n",
            "epoch 007/010, batch 002/002, step 0008/0496: loss=2.52166748046875\n",
            "epoch 007/010, batch 002/002, step 0009/0496: loss=1.9797444343566895\n",
            "epoch 007/010, batch 002/002, step 0010/0496: loss=2.4757072925567627\n",
            "epoch 007/010, batch 002/002, step 0011/0496: loss=2.5791211128234863\n",
            "epoch 007/010, batch 002/002, step 0012/0496: loss=2.1762263774871826\n",
            "epoch 007/010, batch 002/002, step 0013/0496: loss=2.5948033332824707\n",
            "epoch 007/010, batch 002/002, step 0014/0496: loss=2.0018656253814697\n",
            "epoch 007/010, batch 002/002, step 0015/0496: loss=2.0173137187957764\n",
            "epoch 007/010, batch 002/002, step 0016/0496: loss=2.613434314727783\n",
            "epoch 007/010, batch 002/002, step 0017/0496: loss=4.088030815124512\n",
            "epoch 007/010, batch 002/002, step 0018/0496: loss=1.5557732582092285\n",
            "epoch 007/010, batch 002/002, step 0019/0496: loss=2.355713367462158\n",
            "epoch 007/010, batch 002/002, step 0020/0496: loss=2.259592294692993\n",
            "epoch 007/010, batch 002/002, step 0021/0496: loss=4.088513374328613\n",
            "epoch 007/010, batch 002/002, step 0022/0496: loss=2.143467426300049\n",
            "epoch 007/010, batch 002/002, step 0023/0496: loss=2.2401576042175293\n",
            "epoch 007/010, batch 002/002, step 0024/0496: loss=2.6300048828125\n",
            "epoch 007/010, batch 002/002, step 0025/0496: loss=1.962282419204712\n",
            "epoch 007/010, batch 002/002, step 0026/0496: loss=2.6877410411834717\n",
            "epoch 007/010, batch 002/002, step 0027/0496: loss=2.318355083465576\n",
            "epoch 007/010, batch 002/002, step 0028/0496: loss=2.9488086700439453\n",
            "epoch 007/010, batch 002/002, step 0029/0496: loss=1.7396705150604248\n",
            "epoch 007/010, batch 002/002, step 0030/0496: loss=2.328531503677368\n",
            "epoch 007/010, batch 002/002, step 0031/0496: loss=2.462005376815796\n",
            "epoch 007/010, batch 002/002, step 0032/0496: loss=2.0561699867248535\n",
            "epoch 007/010, batch 002/002, step 0033/0496: loss=2.1309781074523926\n",
            "epoch 007/010, batch 002/002, step 0034/0496: loss=1.9498552083969116\n",
            "epoch 007/010, batch 002/002, step 0035/0496: loss=2.331028938293457\n",
            "epoch 007/010, batch 002/002, step 0036/0496: loss=2.0444858074188232\n",
            "epoch 007/010, batch 002/002, step 0037/0496: loss=2.3513336181640625\n",
            "epoch 007/010, batch 002/002, step 0038/0496: loss=2.5827410221099854\n",
            "epoch 007/010, batch 002/002, step 0039/0496: loss=2.5896711349487305\n",
            "epoch 007/010, batch 002/002, step 0040/0496: loss=1.9133172035217285\n",
            "epoch 007/010, batch 002/002, step 0041/0496: loss=2.293400764465332\n",
            "epoch 007/010, batch 002/002, step 0042/0496: loss=1.972708821296692\n",
            "epoch 007/010, batch 002/002, step 0043/0496: loss=1.623106598854065\n",
            "epoch 007/010, batch 002/002, step 0044/0496: loss=1.9364538192749023\n",
            "epoch 007/010, batch 002/002, step 0045/0496: loss=1.7316783666610718\n",
            "epoch 007/010, batch 002/002, step 0046/0496: loss=1.9906954765319824\n",
            "epoch 007/010, batch 002/002, step 0047/0496: loss=1.8188650608062744\n",
            "epoch 007/010, batch 002/002, step 0048/0496: loss=3.624575138092041\n",
            "epoch 007/010, batch 002/002, step 0049/0496: loss=1.9757187366485596\n",
            "epoch 007/010, batch 002/002, step 0050/0496: loss=2.408511161804199\n",
            "epoch 007/010, batch 002/002, step 0051/0496: loss=1.7887885570526123\n",
            "epoch 007/010, batch 002/002, step 0052/0496: loss=2.0965566635131836\n",
            "epoch 007/010, batch 002/002, step 0053/0496: loss=1.8874549865722656\n",
            "epoch 007/010, batch 002/002, step 0054/0496: loss=2.0723984241485596\n",
            "epoch 007/010, batch 002/002, step 0055/0496: loss=1.716016411781311\n",
            "epoch 007/010, batch 002/002, step 0056/0496: loss=1.5248124599456787\n",
            "epoch 007/010, batch 002/002, step 0057/0496: loss=1.9558987617492676\n",
            "epoch 007/010, batch 002/002, step 0058/0496: loss=2.4463157653808594\n",
            "epoch 007/010, batch 002/002, step 0059/0496: loss=2.0606329441070557\n",
            "epoch 007/010, batch 002/002, step 0060/0496: loss=2.317876100540161\n",
            "epoch 007/010, batch 002/002, step 0061/0496: loss=2.602227210998535\n",
            "epoch 007/010, batch 002/002, step 0062/0496: loss=2.129551887512207\n",
            "epoch 007/010, batch 002/002, step 0063/0496: loss=2.0985171794891357\n",
            "epoch 007/010, batch 002/002, step 0064/0496: loss=2.080333948135376\n",
            "epoch 007/010, batch 002/002, step 0065/0496: loss=3.0353963375091553\n",
            "epoch 007/010, batch 002/002, step 0066/0496: loss=1.7687220573425293\n",
            "epoch 007/010, batch 002/002, step 0067/0496: loss=3.077645778656006\n",
            "epoch 007/010, batch 002/002, step 0068/0496: loss=2.2670772075653076\n",
            "epoch 007/010, batch 002/002, step 0069/0496: loss=1.827214241027832\n",
            "epoch 007/010, batch 002/002, step 0070/0496: loss=2.0366082191467285\n",
            "epoch 007/010, batch 002/002, step 0071/0496: loss=1.6014423370361328\n",
            "epoch 007/010, batch 002/002, step 0072/0496: loss=2.0657005310058594\n",
            "epoch 007/010, batch 002/002, step 0073/0496: loss=1.7238473892211914\n",
            "epoch 007/010, batch 002/002, step 0074/0496: loss=1.829903244972229\n",
            "epoch 007/010, batch 002/002, step 0075/0496: loss=1.6246774196624756\n",
            "epoch 007/010, batch 002/002, step 0076/0496: loss=1.9302363395690918\n",
            "epoch 007/010, batch 002/002, step 0077/0496: loss=2.366194725036621\n",
            "epoch 007/010, batch 002/002, step 0078/0496: loss=2.3730642795562744\n",
            "epoch 007/010, batch 002/002, step 0079/0496: loss=1.8988306522369385\n",
            "epoch 007/010, batch 002/002, step 0080/0496: loss=2.263878107070923\n",
            "epoch 007/010, batch 002/002, step 0081/0496: loss=1.9695775508880615\n",
            "epoch 007/010, batch 002/002, step 0082/0496: loss=1.8436853885650635\n",
            "epoch 007/010, batch 002/002, step 0083/0496: loss=2.0620827674865723\n",
            "epoch 007/010, batch 002/002, step 0084/0496: loss=2.29793643951416\n",
            "epoch 007/010, batch 002/002, step 0085/0496: loss=2.1820051670074463\n",
            "epoch 007/010, batch 002/002, step 0086/0496: loss=1.999320387840271\n",
            "epoch 007/010, batch 002/002, step 0087/0496: loss=1.7105731964111328\n",
            "epoch 007/010, batch 002/002, step 0088/0496: loss=1.9190013408660889\n",
            "epoch 007/010, batch 002/002, step 0089/0496: loss=2.08250093460083\n",
            "epoch 007/010, batch 002/002, step 0090/0496: loss=2.05912446975708\n",
            "epoch 007/010, batch 002/002, step 0091/0496: loss=2.125976085662842\n",
            "epoch 007/010, batch 002/002, step 0092/0496: loss=2.427924156188965\n",
            "epoch 007/010, batch 002/002, step 0093/0496: loss=1.8241645097732544\n",
            "epoch 007/010, batch 002/002, step 0094/0496: loss=2.7177627086639404\n",
            "epoch 007/010, batch 002/002, step 0095/0496: loss=2.1325430870056152\n",
            "epoch 007/010, batch 002/002, step 0096/0496: loss=2.679680347442627\n",
            "epoch 007/010, batch 002/002, step 0097/0496: loss=2.2801690101623535\n",
            "epoch 007/010, batch 002/002, step 0098/0496: loss=3.0102200508117676\n",
            "epoch 007/010, batch 002/002, step 0099/0496: loss=2.2539920806884766\n",
            "epoch 007/010, batch 002/002, step 0100/0496: loss=2.191824436187744\n",
            "epoch 007/010, batch 002/002, step 0101/0496: loss=2.1289944648742676\n",
            "epoch 007/010, batch 002/002, step 0102/0496: loss=1.9263997077941895\n",
            "epoch 007/010, batch 002/002, step 0103/0496: loss=1.9510505199432373\n",
            "epoch 007/010, batch 002/002, step 0104/0496: loss=2.3167381286621094\n",
            "epoch 007/010, batch 002/002, step 0105/0496: loss=3.3342721462249756\n",
            "epoch 007/010, batch 002/002, step 0106/0496: loss=2.214524745941162\n",
            "epoch 007/010, batch 002/002, step 0107/0496: loss=2.132667064666748\n",
            "epoch 007/010, batch 002/002, step 0108/0496: loss=2.887051820755005\n",
            "epoch 007/010, batch 002/002, step 0109/0496: loss=2.473637104034424\n",
            "epoch 007/010, batch 002/002, step 0110/0496: loss=1.9797685146331787\n",
            "epoch 007/010, batch 002/002, step 0111/0496: loss=2.359041690826416\n",
            "epoch 007/010, batch 002/002, step 0112/0496: loss=2.189531087875366\n",
            "epoch 007/010, batch 002/002, step 0113/0496: loss=2.7618489265441895\n",
            "epoch 007/010, batch 002/002, step 0114/0496: loss=2.9257917404174805\n",
            "epoch 007/010, batch 002/002, step 0115/0496: loss=1.6878451108932495\n",
            "epoch 007/010, batch 002/002, step 0116/0496: loss=2.3584651947021484\n",
            "epoch 007/010, batch 002/002, step 0117/0496: loss=1.8629711866378784\n",
            "epoch 007/010, batch 002/002, step 0118/0496: loss=2.0139079093933105\n",
            "epoch 007/010, batch 002/002, step 0119/0496: loss=3.856191396713257\n",
            "epoch 007/010, batch 002/002, step 0120/0496: loss=2.097472667694092\n",
            "epoch 007/010, batch 002/002, step 0121/0496: loss=2.4818029403686523\n",
            "epoch 007/010, batch 002/002, step 0122/0496: loss=2.2246553897857666\n",
            "epoch 007/010, batch 002/002, step 0123/0496: loss=2.172562837600708\n",
            "epoch 007/010, batch 002/002, step 0124/0496: loss=2.0131425857543945\n",
            "epoch 007/010, batch 002/002, step 0125/0496: loss=2.2734532356262207\n",
            "epoch 007/010, batch 002/002, step 0126/0496: loss=2.836087703704834\n",
            "epoch 007/010, batch 002/002, step 0127/0496: loss=2.020834445953369\n",
            "epoch 007/010, batch 002/002, step 0128/0496: loss=2.2228753566741943\n",
            "epoch 007/010, batch 002/002, step 0129/0496: loss=2.112459182739258\n",
            "epoch 007/010, batch 002/002, step 0130/0496: loss=1.9180312156677246\n",
            "epoch 007/010, batch 002/002, step 0131/0496: loss=2.1921956539154053\n",
            "epoch 007/010, batch 002/002, step 0132/0496: loss=2.3413240909576416\n",
            "epoch 007/010, batch 002/002, step 0133/0496: loss=1.9981162548065186\n",
            "epoch 007/010, batch 002/002, step 0134/0496: loss=2.0404582023620605\n",
            "epoch 007/010, batch 002/002, step 0135/0496: loss=1.4298492670059204\n",
            "epoch 007/010, batch 002/002, step 0136/0496: loss=2.6373026371002197\n",
            "epoch 007/010, batch 002/002, step 0137/0496: loss=1.9141703844070435\n",
            "epoch 007/010, batch 002/002, step 0138/0496: loss=2.303028106689453\n",
            "epoch 007/010, batch 002/002, step 0139/0496: loss=2.5279223918914795\n",
            "epoch 007/010, batch 002/002, step 0140/0496: loss=2.0842061042785645\n",
            "epoch 007/010, batch 002/002, step 0141/0496: loss=3.9659500122070312\n",
            "epoch 007/010, batch 002/002, step 0142/0496: loss=2.297877073287964\n",
            "epoch 007/010, batch 002/002, step 0143/0496: loss=2.0470774173736572\n",
            "epoch 007/010, batch 002/002, step 0144/0496: loss=2.565887928009033\n",
            "epoch 007/010, batch 002/002, step 0145/0496: loss=2.1709089279174805\n",
            "epoch 007/010, batch 002/002, step 0146/0496: loss=2.054431676864624\n",
            "epoch 007/010, batch 002/002, step 0147/0496: loss=2.222552537918091\n",
            "epoch 007/010, batch 002/002, step 0148/0496: loss=2.5961084365844727\n",
            "epoch 007/010, batch 002/002, step 0149/0496: loss=1.994563102722168\n",
            "epoch 007/010, batch 002/002, step 0150/0496: loss=2.369776964187622\n",
            "epoch 007/010, batch 002/002, step 0151/0496: loss=2.581536054611206\n",
            "epoch 007/010, batch 002/002, step 0152/0496: loss=3.5993845462799072\n",
            "epoch 007/010, batch 002/002, step 0153/0496: loss=2.431213855743408\n",
            "epoch 007/010, batch 002/002, step 0154/0496: loss=2.1290295124053955\n",
            "epoch 007/010, batch 002/002, step 0155/0496: loss=2.5422592163085938\n",
            "epoch 007/010, batch 002/002, step 0156/0496: loss=3.1876938343048096\n",
            "epoch 007/010, batch 002/002, step 0157/0496: loss=2.4727272987365723\n",
            "epoch 007/010, batch 002/002, step 0158/0496: loss=3.0660126209259033\n",
            "epoch 007/010, batch 002/002, step 0159/0496: loss=2.7078990936279297\n",
            "epoch 007/010, batch 002/002, step 0160/0496: loss=3.73012113571167\n",
            "epoch 007/010, batch 002/002, step 0161/0496: loss=2.4697682857513428\n",
            "epoch 007/010, batch 002/002, step 0162/0496: loss=2.957193613052368\n",
            "epoch 007/010, batch 002/002, step 0163/0496: loss=2.6127967834472656\n",
            "epoch 007/010, batch 002/002, step 0164/0496: loss=2.6316637992858887\n",
            "epoch 007/010, batch 002/002, step 0165/0496: loss=2.1915860176086426\n",
            "epoch 007/010, batch 002/002, step 0166/0496: loss=2.869504928588867\n",
            "epoch 007/010, batch 002/002, step 0167/0496: loss=2.195265293121338\n",
            "epoch 007/010, batch 002/002, step 0168/0496: loss=2.6860527992248535\n",
            "epoch 007/010, batch 002/002, step 0169/0496: loss=2.4268152713775635\n",
            "epoch 007/010, batch 002/002, step 0170/0496: loss=2.6465845108032227\n",
            "epoch 007/010, batch 002/002, step 0171/0496: loss=1.5673623085021973\n",
            "epoch 007/010, batch 002/002, step 0172/0496: loss=2.031951427459717\n",
            "epoch 007/010, batch 002/002, step 0173/0496: loss=1.731606125831604\n",
            "epoch 007/010, batch 002/002, step 0174/0496: loss=2.3807151317596436\n",
            "epoch 007/010, batch 002/002, step 0175/0496: loss=2.679447650909424\n",
            "epoch 007/010, batch 002/002, step 0176/0496: loss=2.357218027114868\n",
            "epoch 007/010, batch 002/002, step 0177/0496: loss=1.9860594272613525\n",
            "epoch 007/010, batch 002/002, step 0178/0496: loss=1.6300623416900635\n",
            "epoch 007/010, batch 002/002, step 0179/0496: loss=4.772237777709961\n",
            "epoch 007/010, batch 002/002, step 0180/0496: loss=2.112211227416992\n",
            "epoch 007/010, batch 002/002, step 0181/0496: loss=2.2070932388305664\n",
            "epoch 007/010, batch 002/002, step 0182/0496: loss=1.7557765245437622\n",
            "epoch 007/010, batch 002/002, step 0183/0496: loss=1.7890657186508179\n",
            "epoch 007/010, batch 002/002, step 0184/0496: loss=2.640991687774658\n",
            "epoch 007/010, batch 002/002, step 0185/0496: loss=2.248242139816284\n",
            "epoch 007/010, batch 002/002, step 0186/0496: loss=2.1412863731384277\n",
            "epoch 007/010, batch 002/002, step 0187/0496: loss=1.8058769702911377\n",
            "epoch 007/010, batch 002/002, step 0188/0496: loss=1.8763401508331299\n",
            "epoch 007/010, batch 002/002, step 0189/0496: loss=1.9474531412124634\n",
            "epoch 007/010, batch 002/002, step 0190/0496: loss=2.6461453437805176\n",
            "epoch 007/010, batch 002/002, step 0191/0496: loss=2.1607680320739746\n",
            "epoch 007/010, batch 002/002, step 0192/0496: loss=1.9315050840377808\n",
            "epoch 007/010, batch 002/002, step 0193/0496: loss=2.871541976928711\n",
            "epoch 007/010, batch 002/002, step 0194/0496: loss=2.7718465328216553\n",
            "epoch 007/010, batch 002/002, step 0195/0496: loss=1.9600002765655518\n",
            "epoch 007/010, batch 002/002, step 0196/0496: loss=1.9414013624191284\n",
            "epoch 007/010, batch 002/002, step 0197/0496: loss=1.6394569873809814\n",
            "epoch 007/010, batch 002/002, step 0198/0496: loss=1.6842150688171387\n",
            "epoch 007/010, batch 002/002, step 0199/0496: loss=2.083876609802246\n",
            "epoch 007/010, batch 002/002, step 0200/0496: loss=1.781205654144287\n",
            "epoch 007/010, batch 002/002, step 0201/0496: loss=2.3119864463806152\n",
            "epoch 007/010, batch 002/002, step 0202/0496: loss=1.8779957294464111\n",
            "epoch 007/010, batch 002/002, step 0203/0496: loss=1.5801937580108643\n",
            "epoch 007/010, batch 002/002, step 0204/0496: loss=3.882490396499634\n",
            "epoch 007/010, batch 002/002, step 0205/0496: loss=2.038111448287964\n",
            "epoch 007/010, batch 002/002, step 0206/0496: loss=1.8783494234085083\n",
            "epoch 007/010, batch 002/002, step 0207/0496: loss=2.3153011798858643\n",
            "epoch 007/010, batch 002/002, step 0208/0496: loss=2.30928373336792\n",
            "epoch 007/010, batch 002/002, step 0209/0496: loss=1.9429649114608765\n",
            "epoch 007/010, batch 002/002, step 0210/0496: loss=2.0587656497955322\n",
            "epoch 007/010, batch 002/002, step 0211/0496: loss=2.336177349090576\n",
            "epoch 007/010, batch 002/002, step 0212/0496: loss=2.210477828979492\n",
            "epoch 007/010, batch 002/002, step 0213/0496: loss=1.84230637550354\n",
            "epoch 007/010, batch 002/002, step 0214/0496: loss=1.778741717338562\n",
            "epoch 007/010, batch 002/002, step 0215/0496: loss=2.2155723571777344\n",
            "epoch 007/010, batch 002/002, step 0216/0496: loss=2.3597726821899414\n",
            "epoch 007/010, batch 002/002, step 0217/0496: loss=1.6846331357955933\n",
            "epoch 007/010, batch 002/002, step 0218/0496: loss=2.5779967308044434\n",
            "epoch 007/010, batch 002/002, step 0219/0496: loss=3.5844058990478516\n",
            "epoch 007/010, batch 002/002, step 0220/0496: loss=2.383772373199463\n",
            "epoch 007/010, batch 002/002, step 0221/0496: loss=2.180321216583252\n",
            "epoch 007/010, batch 002/002, step 0222/0496: loss=2.5815770626068115\n",
            "epoch 007/010, batch 002/002, step 0223/0496: loss=2.2205991744995117\n",
            "epoch 007/010, batch 002/002, step 0224/0496: loss=2.250178337097168\n",
            "epoch 007/010, batch 002/002, step 0225/0496: loss=2.567671537399292\n",
            "epoch 007/010, batch 002/002, step 0226/0496: loss=2.3801722526550293\n",
            "epoch 007/010, batch 002/002, step 0227/0496: loss=2.094397783279419\n",
            "epoch 007/010, batch 002/002, step 0228/0496: loss=1.9472160339355469\n",
            "epoch 007/010, batch 002/002, step 0229/0496: loss=1.9282140731811523\n",
            "epoch 007/010, batch 002/002, step 0230/0496: loss=2.2890470027923584\n",
            "epoch 007/010, batch 002/002, step 0231/0496: loss=1.5597110986709595\n",
            "epoch 007/010, batch 002/002, step 0232/0496: loss=2.526608467102051\n",
            "epoch 007/010, batch 002/002, step 0233/0496: loss=2.0038132667541504\n",
            "epoch 007/010, batch 002/002, step 0234/0496: loss=1.9755041599273682\n",
            "epoch 007/010, batch 002/002, step 0235/0496: loss=3.1058573722839355\n",
            "epoch 007/010, batch 002/002, step 0236/0496: loss=3.630852222442627\n",
            "epoch 007/010, batch 002/002, step 0237/0496: loss=2.110013961791992\n",
            "epoch 007/010, batch 002/002, step 0238/0496: loss=2.477051258087158\n",
            "epoch 007/010, batch 002/002, step 0239/0496: loss=2.1064093112945557\n",
            "epoch 007/010, batch 002/002, step 0240/0496: loss=2.345956325531006\n",
            "epoch 007/010, batch 002/002, step 0241/0496: loss=1.8414130210876465\n",
            "epoch 007/010, batch 002/002, step 0242/0496: loss=1.8108110427856445\n",
            "epoch 007/010, batch 002/002, step 0243/0496: loss=2.5781242847442627\n",
            "epoch 007/010, batch 002/002, step 0244/0496: loss=1.9392330646514893\n",
            "epoch 007/010, batch 002/002, step 0245/0496: loss=1.6234744787216187\n",
            "epoch 007/010, batch 002/002, step 0246/0496: loss=1.8085218667984009\n",
            "epoch 007/010, batch 002/002, step 0247/0496: loss=2.339496612548828\n",
            "epoch 007/010, batch 002/002, step 0248/0496: loss=2.47401762008667\n",
            "epoch 007/010, batch 002/002, step 0249/0496: loss=2.3404526710510254\n",
            "epoch 007/010, batch 002/002, step 0250/0496: loss=2.636073350906372\n",
            "epoch 007/010, batch 002/002, step 0251/0496: loss=1.5397744178771973\n",
            "epoch 007/010, batch 002/002, step 0252/0496: loss=1.536438226699829\n",
            "epoch 007/010, batch 002/002, step 0253/0496: loss=2.120638132095337\n",
            "epoch 007/010, batch 002/002, step 0254/0496: loss=1.6965733766555786\n",
            "epoch 007/010, batch 002/002, step 0255/0496: loss=2.302382469177246\n",
            "epoch 007/010, batch 002/002, step 0256/0496: loss=1.8311750888824463\n",
            "epoch 007/010, batch 002/002, step 0257/0496: loss=2.129868268966675\n",
            "epoch 007/010, batch 002/002, step 0258/0496: loss=2.067171335220337\n",
            "epoch 007/010, batch 002/002, step 0259/0496: loss=3.481051445007324\n",
            "epoch 007/010, batch 002/002, step 0260/0496: loss=2.0850210189819336\n",
            "epoch 007/010, batch 002/002, step 0261/0496: loss=1.8191158771514893\n",
            "epoch 007/010, batch 002/002, step 0262/0496: loss=1.8678913116455078\n",
            "epoch 007/010, batch 002/002, step 0263/0496: loss=1.9215126037597656\n",
            "epoch 007/010, batch 002/002, step 0264/0496: loss=2.715212345123291\n",
            "epoch 007/010, batch 002/002, step 0265/0496: loss=2.0261073112487793\n",
            "epoch 007/010, batch 002/002, step 0266/0496: loss=2.1908321380615234\n",
            "epoch 007/010, batch 002/002, step 0267/0496: loss=2.653651237487793\n",
            "epoch 007/010, batch 002/002, step 0268/0496: loss=1.6494121551513672\n",
            "epoch 007/010, batch 002/002, step 0269/0496: loss=2.249600410461426\n",
            "epoch 007/010, batch 002/002, step 0270/0496: loss=2.1065335273742676\n",
            "epoch 007/010, batch 002/002, step 0271/0496: loss=2.780353546142578\n",
            "epoch 007/010, batch 002/002, step 0272/0496: loss=2.401472330093384\n",
            "epoch 007/010, batch 002/002, step 0273/0496: loss=1.654753565788269\n",
            "epoch 007/010, batch 002/002, step 0274/0496: loss=1.8194701671600342\n",
            "epoch 007/010, batch 002/002, step 0275/0496: loss=1.3438189029693604\n",
            "epoch 007/010, batch 002/002, step 0276/0496: loss=1.8815598487854004\n",
            "epoch 007/010, batch 002/002, step 0277/0496: loss=2.0415782928466797\n",
            "epoch 007/010, batch 002/002, step 0278/0496: loss=1.9952008724212646\n",
            "epoch 007/010, batch 002/002, step 0279/0496: loss=2.040099620819092\n",
            "epoch 007/010, batch 002/002, step 0280/0496: loss=1.6835317611694336\n",
            "epoch 007/010, batch 002/002, step 0281/0496: loss=2.388745069503784\n",
            "epoch 007/010, batch 002/002, step 0282/0496: loss=1.5938535928726196\n",
            "epoch 007/010, batch 002/002, step 0283/0496: loss=1.6205512285232544\n",
            "epoch 007/010, batch 002/002, step 0284/0496: loss=1.9067810773849487\n",
            "epoch 007/010, batch 002/002, step 0285/0496: loss=2.3193535804748535\n",
            "epoch 007/010, batch 002/002, step 0286/0496: loss=1.7746179103851318\n",
            "epoch 007/010, batch 002/002, step 0287/0496: loss=1.8812453746795654\n",
            "epoch 007/010, batch 002/002, step 0288/0496: loss=1.54901123046875\n",
            "epoch 007/010, batch 002/002, step 0289/0496: loss=2.1592206954956055\n",
            "epoch 007/010, batch 002/002, step 0290/0496: loss=2.658829689025879\n",
            "epoch 007/010, batch 002/002, step 0291/0496: loss=2.541688919067383\n",
            "epoch 007/010, batch 002/002, step 0292/0496: loss=2.2671070098876953\n",
            "epoch 007/010, batch 002/002, step 0293/0496: loss=2.2559728622436523\n",
            "epoch 007/010, batch 002/002, step 0294/0496: loss=1.941308617591858\n",
            "epoch 007/010, batch 002/002, step 0295/0496: loss=1.9888521432876587\n",
            "epoch 007/010, batch 002/002, step 0296/0496: loss=2.2039523124694824\n",
            "epoch 007/010, batch 002/002, step 0297/0496: loss=2.4611964225769043\n",
            "epoch 007/010, batch 002/002, step 0298/0496: loss=1.974226474761963\n",
            "epoch 007/010, batch 002/002, step 0299/0496: loss=2.2707393169403076\n",
            "epoch 007/010, batch 002/002, step 0300/0496: loss=1.9298803806304932\n",
            "epoch 007/010, batch 002/002, step 0301/0496: loss=1.7778371572494507\n",
            "epoch 007/010, batch 002/002, step 0302/0496: loss=2.374659538269043\n",
            "epoch 007/010, batch 002/002, step 0303/0496: loss=4.053705215454102\n",
            "epoch 007/010, batch 002/002, step 0304/0496: loss=2.9163222312927246\n",
            "epoch 007/010, batch 002/002, step 0305/0496: loss=2.572998523712158\n",
            "epoch 007/010, batch 002/002, step 0306/0496: loss=2.5715670585632324\n",
            "epoch 007/010, batch 002/002, step 0307/0496: loss=2.546268939971924\n",
            "epoch 007/010, batch 002/002, step 0308/0496: loss=1.8361456394195557\n",
            "epoch 007/010, batch 002/002, step 0309/0496: loss=1.9828687906265259\n",
            "epoch 007/010, batch 002/002, step 0310/0496: loss=2.9671921730041504\n",
            "epoch 007/010, batch 002/002, step 0311/0496: loss=2.2474985122680664\n",
            "epoch 007/010, batch 002/002, step 0312/0496: loss=1.9386895895004272\n",
            "epoch 007/010, batch 002/002, step 0313/0496: loss=2.535263776779175\n",
            "epoch 007/010, batch 002/002, step 0314/0496: loss=2.7796552181243896\n",
            "epoch 007/010, batch 002/002, step 0315/0496: loss=2.330735921859741\n",
            "epoch 007/010, batch 002/002, step 0316/0496: loss=2.572235107421875\n",
            "epoch 007/010, batch 002/002, step 0317/0496: loss=2.1933538913726807\n",
            "epoch 007/010, batch 002/002, step 0318/0496: loss=2.7252907752990723\n",
            "epoch 007/010, batch 002/002, step 0319/0496: loss=2.0848941802978516\n",
            "epoch 007/010, batch 002/002, step 0320/0496: loss=1.6644068956375122\n",
            "epoch 007/010, batch 002/002, step 0321/0496: loss=2.069720506668091\n",
            "epoch 007/010, batch 002/002, step 0322/0496: loss=2.1035633087158203\n",
            "epoch 007/010, batch 002/002, step 0323/0496: loss=3.075955867767334\n",
            "epoch 007/010, batch 002/002, step 0324/0496: loss=2.659088611602783\n",
            "epoch 007/010, batch 002/002, step 0325/0496: loss=3.134307861328125\n",
            "epoch 007/010, batch 002/002, step 0326/0496: loss=2.204821825027466\n",
            "epoch 007/010, batch 002/002, step 0327/0496: loss=2.615967035293579\n",
            "epoch 007/010, batch 002/002, step 0328/0496: loss=1.941272258758545\n",
            "epoch 007/010, batch 002/002, step 0329/0496: loss=1.885326623916626\n",
            "epoch 007/010, batch 002/002, step 0330/0496: loss=2.0215344429016113\n",
            "epoch 007/010, batch 002/002, step 0331/0496: loss=1.6668494939804077\n",
            "epoch 007/010, batch 002/002, step 0332/0496: loss=3.711881160736084\n",
            "epoch 007/010, batch 002/002, step 0333/0496: loss=5.494668960571289\n",
            "epoch 007/010, batch 002/002, step 0334/0496: loss=2.118905782699585\n",
            "epoch 007/010, batch 002/002, step 0335/0496: loss=5.508713722229004\n",
            "epoch 007/010, batch 002/002, step 0336/0496: loss=4.400542259216309\n",
            "epoch 007/010, batch 002/002, step 0337/0496: loss=9.958658218383789\n",
            "epoch 007/010, batch 002/002, step 0338/0496: loss=2.8141493797302246\n",
            "epoch 007/010, batch 002/002, step 0339/0496: loss=13.56254768371582\n",
            "epoch 007/010, batch 002/002, step 0340/0496: loss=4.518551826477051\n",
            "epoch 007/010, batch 002/002, step 0341/0496: loss=17.37646484375\n",
            "epoch 007/010, batch 002/002, step 0342/0496: loss=20.481178283691406\n",
            "epoch 007/010, batch 002/002, step 0343/0496: loss=12.626559257507324\n",
            "epoch 007/010, batch 002/002, step 0344/0496: loss=22.73944854736328\n",
            "epoch 007/010, batch 002/002, step 0345/0496: loss=8.075045585632324\n",
            "epoch 007/010, batch 002/002, step 0346/0496: loss=7.8379597663879395\n",
            "epoch 007/010, batch 002/002, step 0347/0496: loss=20.540159225463867\n",
            "epoch 007/010, batch 002/002, step 0348/0496: loss=6.504149436950684\n",
            "epoch 007/010, batch 002/002, step 0349/0496: loss=26.5611629486084\n",
            "epoch 007/010, batch 002/002, step 0350/0496: loss=4.951847553253174\n",
            "epoch 007/010, batch 002/002, step 0351/0496: loss=22.932865142822266\n",
            "epoch 007/010, batch 002/002, step 0352/0496: loss=5.362037658691406\n",
            "epoch 007/010, batch 002/002, step 0353/0496: loss=14.940672874450684\n",
            "epoch 007/010, batch 002/002, step 0354/0496: loss=4.5820488929748535\n",
            "epoch 007/010, batch 002/002, step 0355/0496: loss=11.305068969726562\n",
            "epoch 007/010, batch 002/002, step 0356/0496: loss=6.6383161544799805\n",
            "epoch 007/010, batch 002/002, step 0357/0496: loss=8.642127990722656\n",
            "epoch 007/010, batch 002/002, step 0358/0496: loss=18.49166488647461\n",
            "epoch 007/010, batch 002/002, step 0359/0496: loss=15.488880157470703\n",
            "epoch 007/010, batch 002/002, step 0360/0496: loss=14.881059646606445\n",
            "epoch 007/010, batch 002/002, step 0361/0496: loss=5.981929779052734\n",
            "epoch 007/010, batch 002/002, step 0362/0496: loss=13.822481155395508\n",
            "epoch 007/010, batch 002/002, step 0363/0496: loss=2.9896345138549805\n",
            "epoch 007/010, batch 002/002, step 0364/0496: loss=5.7860331535339355\n",
            "epoch 007/010, batch 002/002, step 0365/0496: loss=9.976434707641602\n",
            "epoch 007/010, batch 002/002, step 0366/0496: loss=3.3805534839630127\n",
            "epoch 007/010, batch 002/002, step 0367/0496: loss=9.255130767822266\n",
            "epoch 007/010, batch 002/002, step 0368/0496: loss=4.6417975425720215\n",
            "epoch 007/010, batch 002/002, step 0369/0496: loss=9.70792007446289\n",
            "epoch 007/010, batch 002/002, step 0370/0496: loss=5.251428127288818\n",
            "epoch 007/010, batch 002/002, step 0371/0496: loss=3.951615810394287\n",
            "epoch 007/010, batch 002/002, step 0372/0496: loss=4.649031639099121\n",
            "epoch 007/010, batch 002/002, step 0373/0496: loss=5.567744255065918\n",
            "epoch 007/010, batch 002/002, step 0374/0496: loss=2.4088783264160156\n",
            "epoch 007/010, batch 002/002, step 0375/0496: loss=4.469496250152588\n",
            "epoch 007/010, batch 002/002, step 0376/0496: loss=4.697322368621826\n",
            "epoch 007/010, batch 002/002, step 0377/0496: loss=3.404191493988037\n",
            "epoch 007/010, batch 002/002, step 0378/0496: loss=4.341078758239746\n",
            "epoch 007/010, batch 002/002, step 0379/0496: loss=2.9233553409576416\n",
            "epoch 007/010, batch 002/002, step 0380/0496: loss=3.225459575653076\n",
            "epoch 007/010, batch 002/002, step 0381/0496: loss=3.7036001682281494\n",
            "epoch 007/010, batch 002/002, step 0382/0496: loss=3.9336018562316895\n",
            "epoch 007/010, batch 002/002, step 0383/0496: loss=3.4368340969085693\n",
            "epoch 007/010, batch 002/002, step 0384/0496: loss=3.3210058212280273\n",
            "epoch 007/010, batch 002/002, step 0385/0496: loss=3.593001365661621\n",
            "epoch 007/010, batch 002/002, step 0386/0496: loss=3.2029130458831787\n",
            "epoch 007/010, batch 002/002, step 0387/0496: loss=4.035461902618408\n",
            "epoch 007/010, batch 002/002, step 0388/0496: loss=2.6576149463653564\n",
            "epoch 007/010, batch 002/002, step 0389/0496: loss=2.0396173000335693\n",
            "epoch 007/010, batch 002/002, step 0390/0496: loss=3.3921780586242676\n",
            "epoch 007/010, batch 002/002, step 0391/0496: loss=2.3572463989257812\n",
            "epoch 007/010, batch 002/002, step 0392/0496: loss=2.377007007598877\n",
            "epoch 007/010, batch 002/002, step 0393/0496: loss=2.711780548095703\n",
            "epoch 007/010, batch 002/002, step 0394/0496: loss=2.5342555046081543\n",
            "epoch 007/010, batch 002/002, step 0395/0496: loss=2.092322826385498\n",
            "epoch 007/010, batch 002/002, step 0396/0496: loss=1.8340270519256592\n",
            "epoch 007/010, batch 002/002, step 0397/0496: loss=2.584019184112549\n",
            "epoch 007/010, batch 002/002, step 0398/0496: loss=2.3865480422973633\n",
            "epoch 007/010, batch 002/002, step 0399/0496: loss=2.122296094894409\n",
            "epoch 007/010, batch 002/002, step 0400/0496: loss=2.2224364280700684\n",
            "epoch 007/010, batch 002/002, step 0401/0496: loss=2.4331254959106445\n",
            "epoch 007/010, batch 002/002, step 0402/0496: loss=3.043473243713379\n",
            "epoch 007/010, batch 002/002, step 0403/0496: loss=2.9256935119628906\n",
            "epoch 007/010, batch 002/002, step 0404/0496: loss=1.9824103116989136\n",
            "epoch 007/010, batch 002/002, step 0405/0496: loss=3.4318013191223145\n",
            "epoch 007/010, batch 002/002, step 0406/0496: loss=1.7521816492080688\n",
            "epoch 007/010, batch 002/002, step 0407/0496: loss=2.406172037124634\n",
            "epoch 007/010, batch 002/002, step 0408/0496: loss=2.3056581020355225\n",
            "epoch 007/010, batch 002/002, step 0409/0496: loss=2.368051528930664\n",
            "epoch 007/010, batch 002/002, step 0410/0496: loss=2.0160698890686035\n",
            "epoch 007/010, batch 002/002, step 0411/0496: loss=3.1677091121673584\n",
            "epoch 007/010, batch 002/002, step 0412/0496: loss=3.141396999359131\n",
            "epoch 007/010, batch 002/002, step 0413/0496: loss=2.6177704334259033\n",
            "epoch 007/010, batch 002/002, step 0414/0496: loss=2.3115439414978027\n",
            "epoch 007/010, batch 002/002, step 0415/0496: loss=2.09431791305542\n",
            "epoch 007/010, batch 002/002, step 0416/0496: loss=1.9521164894104004\n",
            "epoch 007/010, batch 002/002, step 0417/0496: loss=2.2553389072418213\n",
            "epoch 007/010, batch 002/002, step 0418/0496: loss=2.257312297821045\n",
            "epoch 007/010, batch 002/002, step 0419/0496: loss=1.9706207513809204\n",
            "epoch 007/010, batch 002/002, step 0420/0496: loss=1.8595037460327148\n",
            "epoch 007/010, batch 002/002, step 0421/0496: loss=2.165863513946533\n",
            "epoch 007/010, batch 002/002, step 0422/0496: loss=1.5776896476745605\n",
            "epoch 007/010, batch 002/002, step 0423/0496: loss=2.272974729537964\n",
            "epoch 007/010, batch 002/002, step 0424/0496: loss=2.119137763977051\n",
            "epoch 007/010, batch 002/002, step 0425/0496: loss=2.535651206970215\n",
            "epoch 007/010, batch 002/002, step 0426/0496: loss=2.0418238639831543\n",
            "epoch 007/010, batch 002/002, step 0427/0496: loss=2.510298728942871\n",
            "epoch 007/010, batch 002/002, step 0428/0496: loss=1.6104403734207153\n",
            "epoch 007/010, batch 002/002, step 0429/0496: loss=1.5066698789596558\n",
            "epoch 007/010, batch 002/002, step 0430/0496: loss=1.944838047027588\n",
            "epoch 007/010, batch 002/002, step 0431/0496: loss=2.317094564437866\n",
            "epoch 007/010, batch 002/002, step 0432/0496: loss=1.8598445653915405\n",
            "epoch 007/010, batch 002/002, step 0433/0496: loss=1.960681676864624\n",
            "epoch 007/010, batch 002/002, step 0434/0496: loss=2.7037649154663086\n",
            "epoch 007/010, batch 002/002, step 0435/0496: loss=2.061549425125122\n",
            "epoch 007/010, batch 002/002, step 0436/0496: loss=1.6212619543075562\n",
            "epoch 007/010, batch 002/002, step 0437/0496: loss=2.6628823280334473\n",
            "epoch 007/010, batch 002/002, step 0438/0496: loss=1.8808796405792236\n",
            "epoch 007/010, batch 002/002, step 0439/0496: loss=2.0814921855926514\n",
            "epoch 007/010, batch 002/002, step 0440/0496: loss=2.343616008758545\n",
            "epoch 007/010, batch 002/002, step 0441/0496: loss=1.7125046253204346\n",
            "epoch 007/010, batch 002/002, step 0442/0496: loss=1.662935733795166\n",
            "epoch 007/010, batch 002/002, step 0443/0496: loss=2.2089943885803223\n",
            "epoch 007/010, batch 002/002, step 0444/0496: loss=2.3975088596343994\n",
            "epoch 007/010, batch 002/002, step 0445/0496: loss=1.6740070581436157\n",
            "epoch 007/010, batch 002/002, step 0446/0496: loss=1.7987875938415527\n",
            "epoch 007/010, batch 002/002, step 0447/0496: loss=1.644949197769165\n",
            "epoch 007/010, batch 002/002, step 0448/0496: loss=1.5822157859802246\n",
            "epoch 007/010, batch 002/002, step 0449/0496: loss=2.9060120582580566\n",
            "epoch 007/010, batch 002/002, step 0450/0496: loss=2.332625389099121\n",
            "epoch 007/010, batch 002/002, step 0451/0496: loss=2.1774468421936035\n",
            "epoch 007/010, batch 002/002, step 0452/0496: loss=2.095137357711792\n",
            "epoch 007/010, batch 002/002, step 0453/0496: loss=1.6658308506011963\n",
            "epoch 007/010, batch 002/002, step 0454/0496: loss=2.42816162109375\n",
            "epoch 007/010, batch 002/002, step 0455/0496: loss=2.280928611755371\n",
            "epoch 007/010, batch 002/002, step 0456/0496: loss=2.041280746459961\n",
            "epoch 007/010, batch 002/002, step 0457/0496: loss=2.301117181777954\n",
            "epoch 007/010, batch 002/002, step 0458/0496: loss=3.2500975131988525\n",
            "epoch 007/010, batch 002/002, step 0459/0496: loss=2.02152419090271\n",
            "epoch 007/010, batch 002/002, step 0460/0496: loss=1.7361236810684204\n",
            "epoch 007/010, batch 002/002, step 0461/0496: loss=1.9524507522583008\n",
            "epoch 007/010, batch 002/002, step 0462/0496: loss=3.4034504890441895\n",
            "epoch 007/010, batch 002/002, step 0463/0496: loss=2.3211522102355957\n",
            "epoch 007/010, batch 002/002, step 0464/0496: loss=2.158109188079834\n",
            "epoch 007/010, batch 002/002, step 0465/0496: loss=2.0182688236236572\n",
            "epoch 007/010, batch 002/002, step 0466/0496: loss=1.692667007446289\n",
            "epoch 007/010, batch 002/002, step 0467/0496: loss=2.0277762413024902\n",
            "epoch 007/010, batch 002/002, step 0468/0496: loss=2.2451560497283936\n",
            "epoch 007/010, batch 002/002, step 0469/0496: loss=1.8842048645019531\n",
            "epoch 007/010, batch 002/002, step 0470/0496: loss=1.7061026096343994\n",
            "epoch 007/010, batch 002/002, step 0471/0496: loss=2.0687880516052246\n",
            "epoch 007/010, batch 002/002, step 0472/0496: loss=1.9915823936462402\n",
            "epoch 007/010, batch 002/002, step 0473/0496: loss=2.1109395027160645\n",
            "epoch 007/010, batch 002/002, step 0474/0496: loss=2.2927980422973633\n",
            "epoch 007/010, batch 002/002, step 0475/0496: loss=2.113511085510254\n",
            "epoch 007/010, batch 002/002, step 0476/0496: loss=2.472731590270996\n",
            "epoch 007/010, batch 002/002, step 0477/0496: loss=1.6368005275726318\n",
            "epoch 007/010, batch 002/002, step 0478/0496: loss=1.6957272291183472\n",
            "epoch 007/010, batch 002/002, step 0479/0496: loss=1.4705756902694702\n",
            "epoch 007/010, batch 002/002, step 0480/0496: loss=2.40108585357666\n",
            "epoch 007/010, batch 002/002, step 0481/0496: loss=1.7307214736938477\n",
            "epoch 007/010, batch 002/002, step 0482/0496: loss=2.226761817932129\n",
            "epoch 007/010, batch 002/002, step 0483/0496: loss=2.3629844188690186\n",
            "epoch 007/010, batch 002/002, step 0484/0496: loss=2.1234962940216064\n",
            "epoch 007/010, batch 002/002, step 0485/0496: loss=1.993058681488037\n",
            "epoch 007/010, batch 002/002, step 0486/0496: loss=1.9550302028656006\n",
            "epoch 007/010, batch 002/002, step 0487/0496: loss=2.6312103271484375\n",
            "epoch 007/010, batch 002/002, step 0488/0496: loss=1.9354302883148193\n",
            "epoch 007/010, batch 002/002, step 0489/0496: loss=2.525998115539551\n",
            "epoch 007/010, batch 002/002, step 0490/0496: loss=2.7535054683685303\n",
            "epoch 007/010, batch 002/002, step 0491/0496: loss=1.5852694511413574\n",
            "epoch 007/010, batch 002/002, step 0492/0496: loss=2.0926718711853027\n",
            "epoch 007/010, batch 002/002, step 0493/0496: loss=1.8900251388549805\n",
            "epoch 007/010, batch 002/002, step 0494/0496: loss=2.12125825881958\n",
            "epoch 007/010, batch 002/002, step 0495/0496: loss=2.2579026222229004\n",
            "epoch 007/010, batch 002/002, step 0496/0496: loss=2.1550168991088867\n",
            "epoch 008/010, batch 001/002, step 0001/0496: loss=1.8876672983169556\n",
            "epoch 008/010, batch 001/002, step 0002/0496: loss=1.9557011127471924\n",
            "epoch 008/010, batch 001/002, step 0003/0496: loss=1.729309320449829\n",
            "epoch 008/010, batch 001/002, step 0004/0496: loss=1.8800095319747925\n",
            "epoch 008/010, batch 001/002, step 0005/0496: loss=2.404952049255371\n",
            "epoch 008/010, batch 001/002, step 0006/0496: loss=2.4060401916503906\n",
            "epoch 008/010, batch 001/002, step 0007/0496: loss=1.9816375970840454\n",
            "epoch 008/010, batch 001/002, step 0008/0496: loss=2.056816339492798\n",
            "epoch 008/010, batch 001/002, step 0009/0496: loss=1.7417871952056885\n",
            "epoch 008/010, batch 001/002, step 0010/0496: loss=2.3751792907714844\n",
            "epoch 008/010, batch 001/002, step 0011/0496: loss=2.0845675468444824\n",
            "epoch 008/010, batch 001/002, step 0012/0496: loss=2.2478604316711426\n",
            "epoch 008/010, batch 001/002, step 0013/0496: loss=2.1467037200927734\n",
            "epoch 008/010, batch 001/002, step 0014/0496: loss=1.9761202335357666\n",
            "epoch 008/010, batch 001/002, step 0015/0496: loss=2.0281379222869873\n",
            "epoch 008/010, batch 001/002, step 0016/0496: loss=3.347836494445801\n",
            "epoch 008/010, batch 001/002, step 0017/0496: loss=2.010791540145874\n",
            "epoch 008/010, batch 001/002, step 0018/0496: loss=1.9069151878356934\n",
            "epoch 008/010, batch 001/002, step 0019/0496: loss=2.6182053089141846\n",
            "epoch 008/010, batch 001/002, step 0020/0496: loss=2.1431150436401367\n",
            "epoch 008/010, batch 001/002, step 0021/0496: loss=1.8847169876098633\n",
            "epoch 008/010, batch 001/002, step 0022/0496: loss=1.9124021530151367\n",
            "epoch 008/010, batch 001/002, step 0023/0496: loss=1.7902339696884155\n",
            "epoch 008/010, batch 001/002, step 0024/0496: loss=2.367763042449951\n",
            "epoch 008/010, batch 001/002, step 0025/0496: loss=1.6726562976837158\n",
            "epoch 008/010, batch 001/002, step 0026/0496: loss=1.9235903024673462\n",
            "epoch 008/010, batch 001/002, step 0027/0496: loss=1.8186414241790771\n",
            "epoch 008/010, batch 001/002, step 0028/0496: loss=1.7235112190246582\n",
            "epoch 008/010, batch 001/002, step 0029/0496: loss=2.165463447570801\n",
            "epoch 008/010, batch 001/002, step 0030/0496: loss=2.249953031539917\n",
            "epoch 008/010, batch 001/002, step 0031/0496: loss=1.7418546676635742\n",
            "epoch 008/010, batch 001/002, step 0032/0496: loss=2.6368587017059326\n",
            "epoch 008/010, batch 001/002, step 0033/0496: loss=2.157367706298828\n",
            "epoch 008/010, batch 001/002, step 0034/0496: loss=1.8776127099990845\n",
            "epoch 008/010, batch 001/002, step 0035/0496: loss=1.9421026706695557\n",
            "epoch 008/010, batch 001/002, step 0036/0496: loss=1.6038343906402588\n",
            "epoch 008/010, batch 001/002, step 0037/0496: loss=1.9127280712127686\n",
            "epoch 008/010, batch 001/002, step 0038/0496: loss=1.7034720182418823\n",
            "epoch 008/010, batch 001/002, step 0039/0496: loss=1.8719544410705566\n",
            "epoch 008/010, batch 001/002, step 0040/0496: loss=2.226210117340088\n",
            "epoch 008/010, batch 001/002, step 0041/0496: loss=2.0172336101531982\n",
            "epoch 008/010, batch 001/002, step 0042/0496: loss=2.0252621173858643\n",
            "epoch 008/010, batch 001/002, step 0043/0496: loss=2.244981288909912\n",
            "epoch 008/010, batch 001/002, step 0044/0496: loss=1.646867275238037\n",
            "epoch 008/010, batch 001/002, step 0045/0496: loss=1.7614398002624512\n",
            "epoch 008/010, batch 001/002, step 0046/0496: loss=3.757628917694092\n",
            "epoch 008/010, batch 001/002, step 0047/0496: loss=1.6108568906784058\n",
            "epoch 008/010, batch 001/002, step 0048/0496: loss=1.8337821960449219\n",
            "epoch 008/010, batch 001/002, step 0049/0496: loss=2.3480942249298096\n",
            "epoch 008/010, batch 001/002, step 0050/0496: loss=1.8378407955169678\n",
            "epoch 008/010, batch 001/002, step 0051/0496: loss=1.7088541984558105\n",
            "epoch 008/010, batch 001/002, step 0052/0496: loss=1.8667654991149902\n",
            "epoch 008/010, batch 001/002, step 0053/0496: loss=1.9721721410751343\n",
            "epoch 008/010, batch 001/002, step 0054/0496: loss=2.22395658493042\n",
            "epoch 008/010, batch 001/002, step 0055/0496: loss=2.1169357299804688\n",
            "epoch 008/010, batch 001/002, step 0056/0496: loss=1.5625629425048828\n",
            "epoch 008/010, batch 001/002, step 0057/0496: loss=1.6483008861541748\n",
            "epoch 008/010, batch 001/002, step 0058/0496: loss=2.1854913234710693\n",
            "epoch 008/010, batch 001/002, step 0059/0496: loss=2.263607978820801\n",
            "epoch 008/010, batch 001/002, step 0060/0496: loss=1.7001361846923828\n",
            "epoch 008/010, batch 001/002, step 0061/0496: loss=1.7232550382614136\n",
            "epoch 008/010, batch 001/002, step 0062/0496: loss=1.567785382270813\n",
            "epoch 008/010, batch 001/002, step 0063/0496: loss=1.785871982574463\n",
            "epoch 008/010, batch 001/002, step 0064/0496: loss=1.563586950302124\n",
            "epoch 008/010, batch 001/002, step 0065/0496: loss=2.062943935394287\n",
            "epoch 008/010, batch 001/002, step 0066/0496: loss=2.031913995742798\n",
            "epoch 008/010, batch 001/002, step 0067/0496: loss=2.0034985542297363\n",
            "epoch 008/010, batch 001/002, step 0068/0496: loss=1.63694167137146\n",
            "epoch 008/010, batch 001/002, step 0069/0496: loss=1.9440219402313232\n",
            "epoch 008/010, batch 001/002, step 0070/0496: loss=1.5824956893920898\n",
            "epoch 008/010, batch 001/002, step 0071/0496: loss=2.001856803894043\n",
            "epoch 008/010, batch 001/002, step 0072/0496: loss=1.995732307434082\n",
            "epoch 008/010, batch 001/002, step 0073/0496: loss=1.9207298755645752\n",
            "epoch 008/010, batch 001/002, step 0074/0496: loss=1.7587807178497314\n",
            "epoch 008/010, batch 001/002, step 0075/0496: loss=2.0934762954711914\n",
            "epoch 008/010, batch 001/002, step 0076/0496: loss=1.674432635307312\n",
            "epoch 008/010, batch 001/002, step 0077/0496: loss=2.0201683044433594\n",
            "epoch 008/010, batch 001/002, step 0078/0496: loss=2.1213979721069336\n",
            "epoch 008/010, batch 001/002, step 0079/0496: loss=2.2712881565093994\n",
            "epoch 008/010, batch 001/002, step 0080/0496: loss=1.60914945602417\n",
            "epoch 008/010, batch 001/002, step 0081/0496: loss=1.823277473449707\n",
            "epoch 008/010, batch 001/002, step 0082/0496: loss=1.899390459060669\n",
            "epoch 008/010, batch 001/002, step 0083/0496: loss=1.7376115322113037\n",
            "epoch 008/010, batch 001/002, step 0084/0496: loss=2.4939846992492676\n",
            "epoch 008/010, batch 001/002, step 0085/0496: loss=2.2090649604797363\n",
            "epoch 008/010, batch 001/002, step 0086/0496: loss=1.7447073459625244\n",
            "epoch 008/010, batch 001/002, step 0087/0496: loss=1.66400146484375\n",
            "epoch 008/010, batch 001/002, step 0088/0496: loss=1.793151617050171\n",
            "epoch 008/010, batch 001/002, step 0089/0496: loss=2.6265854835510254\n",
            "epoch 008/010, batch 001/002, step 0090/0496: loss=2.016047477722168\n",
            "epoch 008/010, batch 001/002, step 0091/0496: loss=1.8220322132110596\n",
            "epoch 008/010, batch 001/002, step 0092/0496: loss=2.158050060272217\n",
            "epoch 008/010, batch 001/002, step 0093/0496: loss=2.192126989364624\n",
            "epoch 008/010, batch 001/002, step 0094/0496: loss=1.8994648456573486\n",
            "epoch 008/010, batch 001/002, step 0095/0496: loss=1.4627091884613037\n",
            "epoch 008/010, batch 001/002, step 0096/0496: loss=1.822180986404419\n",
            "epoch 008/010, batch 001/002, step 0097/0496: loss=1.8549656867980957\n",
            "epoch 008/010, batch 001/002, step 0098/0496: loss=1.6292822360992432\n",
            "epoch 008/010, batch 001/002, step 0099/0496: loss=1.592188835144043\n",
            "epoch 008/010, batch 001/002, step 0100/0496: loss=1.8664741516113281\n",
            "epoch 008/010, batch 001/002, step 0101/0496: loss=1.9251770973205566\n",
            "epoch 008/010, batch 001/002, step 0102/0496: loss=3.1613657474517822\n",
            "epoch 008/010, batch 001/002, step 0103/0496: loss=2.371361017227173\n",
            "epoch 008/010, batch 001/002, step 0104/0496: loss=2.0400750637054443\n",
            "epoch 008/010, batch 001/002, step 0105/0496: loss=2.014660358428955\n",
            "epoch 008/010, batch 001/002, step 0106/0496: loss=1.9732091426849365\n",
            "epoch 008/010, batch 001/002, step 0107/0496: loss=1.871434211730957\n",
            "epoch 008/010, batch 001/002, step 0108/0496: loss=2.3045310974121094\n",
            "epoch 008/010, batch 001/002, step 0109/0496: loss=2.0070295333862305\n",
            "epoch 008/010, batch 001/002, step 0110/0496: loss=2.2634494304656982\n",
            "epoch 008/010, batch 001/002, step 0111/0496: loss=1.8701472282409668\n",
            "epoch 008/010, batch 001/002, step 0112/0496: loss=2.1122779846191406\n",
            "epoch 008/010, batch 001/002, step 0113/0496: loss=1.814740777015686\n",
            "epoch 008/010, batch 001/002, step 0114/0496: loss=1.6214241981506348\n",
            "epoch 008/010, batch 001/002, step 0115/0496: loss=2.2002644538879395\n",
            "epoch 008/010, batch 001/002, step 0116/0496: loss=2.292478561401367\n",
            "epoch 008/010, batch 001/002, step 0117/0496: loss=1.6021873950958252\n",
            "epoch 008/010, batch 001/002, step 0118/0496: loss=2.947770118713379\n",
            "epoch 008/010, batch 001/002, step 0119/0496: loss=1.689451813697815\n",
            "epoch 008/010, batch 001/002, step 0120/0496: loss=2.0370800495147705\n",
            "epoch 008/010, batch 001/002, step 0121/0496: loss=1.9361222982406616\n",
            "epoch 008/010, batch 001/002, step 0122/0496: loss=1.5629115104675293\n",
            "epoch 008/010, batch 001/002, step 0123/0496: loss=2.0533084869384766\n",
            "epoch 008/010, batch 001/002, step 0124/0496: loss=2.11594295501709\n",
            "epoch 008/010, batch 001/002, step 0125/0496: loss=1.5041673183441162\n",
            "epoch 008/010, batch 001/002, step 0126/0496: loss=1.9742703437805176\n",
            "epoch 008/010, batch 001/002, step 0127/0496: loss=1.933768391609192\n",
            "epoch 008/010, batch 001/002, step 0128/0496: loss=2.584730386734009\n",
            "epoch 008/010, batch 001/002, step 0129/0496: loss=1.8256711959838867\n",
            "epoch 008/010, batch 001/002, step 0130/0496: loss=1.7317216396331787\n",
            "epoch 008/010, batch 001/002, step 0131/0496: loss=1.7175791263580322\n",
            "epoch 008/010, batch 001/002, step 0132/0496: loss=1.9749715328216553\n",
            "epoch 008/010, batch 001/002, step 0133/0496: loss=2.375230312347412\n",
            "epoch 008/010, batch 001/002, step 0134/0496: loss=1.698807954788208\n",
            "epoch 008/010, batch 001/002, step 0135/0496: loss=2.01139497756958\n",
            "epoch 008/010, batch 001/002, step 0136/0496: loss=1.94212007522583\n",
            "epoch 008/010, batch 001/002, step 0137/0496: loss=1.7336227893829346\n",
            "epoch 008/010, batch 001/002, step 0138/0496: loss=2.044024705886841\n",
            "epoch 008/010, batch 001/002, step 0139/0496: loss=4.479181289672852\n",
            "epoch 008/010, batch 001/002, step 0140/0496: loss=1.725650668144226\n",
            "epoch 008/010, batch 001/002, step 0141/0496: loss=2.095604419708252\n",
            "epoch 008/010, batch 001/002, step 0142/0496: loss=1.839349389076233\n",
            "epoch 008/010, batch 001/002, step 0143/0496: loss=2.5242066383361816\n",
            "epoch 008/010, batch 001/002, step 0144/0496: loss=2.3798508644104004\n",
            "epoch 008/010, batch 001/002, step 0145/0496: loss=2.2173948287963867\n",
            "epoch 008/010, batch 001/002, step 0146/0496: loss=1.3862996101379395\n",
            "epoch 008/010, batch 001/002, step 0147/0496: loss=2.4020462036132812\n",
            "epoch 008/010, batch 001/002, step 0148/0496: loss=1.879160761833191\n",
            "epoch 008/010, batch 001/002, step 0149/0496: loss=2.1217031478881836\n",
            "epoch 008/010, batch 001/002, step 0150/0496: loss=2.069563865661621\n",
            "epoch 008/010, batch 001/002, step 0151/0496: loss=2.0665340423583984\n",
            "epoch 008/010, batch 001/002, step 0152/0496: loss=1.635474443435669\n",
            "epoch 008/010, batch 001/002, step 0153/0496: loss=1.5435733795166016\n",
            "epoch 008/010, batch 001/002, step 0154/0496: loss=1.8885254859924316\n",
            "epoch 008/010, batch 001/002, step 0155/0496: loss=1.6318095922470093\n",
            "epoch 008/010, batch 001/002, step 0156/0496: loss=2.0786807537078857\n",
            "epoch 008/010, batch 001/002, step 0157/0496: loss=2.7952804565429688\n",
            "epoch 008/010, batch 001/002, step 0158/0496: loss=1.710174798965454\n",
            "epoch 008/010, batch 001/002, step 0159/0496: loss=1.6774042844772339\n",
            "epoch 008/010, batch 001/002, step 0160/0496: loss=2.9376370906829834\n",
            "epoch 008/010, batch 001/002, step 0161/0496: loss=1.6300913095474243\n",
            "epoch 008/010, batch 001/002, step 0162/0496: loss=1.7241415977478027\n",
            "epoch 008/010, batch 001/002, step 0163/0496: loss=1.8151417970657349\n",
            "epoch 008/010, batch 001/002, step 0164/0496: loss=2.2434468269348145\n",
            "epoch 008/010, batch 001/002, step 0165/0496: loss=1.849367618560791\n",
            "epoch 008/010, batch 001/002, step 0166/0496: loss=1.9575554132461548\n",
            "epoch 008/010, batch 001/002, step 0167/0496: loss=2.2262678146362305\n",
            "epoch 008/010, batch 001/002, step 0168/0496: loss=1.888207197189331\n",
            "epoch 008/010, batch 001/002, step 0169/0496: loss=1.5818363428115845\n",
            "epoch 008/010, batch 001/002, step 0170/0496: loss=1.3449654579162598\n",
            "epoch 008/010, batch 001/002, step 0171/0496: loss=1.8751863241195679\n",
            "epoch 008/010, batch 001/002, step 0172/0496: loss=1.5013166666030884\n",
            "epoch 008/010, batch 001/002, step 0173/0496: loss=1.8383184671401978\n",
            "epoch 008/010, batch 001/002, step 0174/0496: loss=2.193476676940918\n",
            "epoch 008/010, batch 001/002, step 0175/0496: loss=2.299149513244629\n",
            "epoch 008/010, batch 001/002, step 0176/0496: loss=1.2783344984054565\n",
            "epoch 008/010, batch 001/002, step 0177/0496: loss=2.0918526649475098\n",
            "epoch 008/010, batch 001/002, step 0178/0496: loss=1.9822505712509155\n",
            "epoch 008/010, batch 001/002, step 0179/0496: loss=2.283808708190918\n",
            "epoch 008/010, batch 001/002, step 0180/0496: loss=2.2308826446533203\n",
            "epoch 008/010, batch 001/002, step 0181/0496: loss=1.9483442306518555\n",
            "epoch 008/010, batch 001/002, step 0182/0496: loss=1.8524099588394165\n",
            "epoch 008/010, batch 001/002, step 0183/0496: loss=2.752195358276367\n",
            "epoch 008/010, batch 001/002, step 0184/0496: loss=1.8220292329788208\n",
            "epoch 008/010, batch 001/002, step 0185/0496: loss=1.9821717739105225\n",
            "epoch 008/010, batch 001/002, step 0186/0496: loss=1.848890781402588\n",
            "epoch 008/010, batch 001/002, step 0187/0496: loss=1.9040114879608154\n",
            "epoch 008/010, batch 001/002, step 0188/0496: loss=1.9122697114944458\n",
            "epoch 008/010, batch 001/002, step 0189/0496: loss=1.4908019304275513\n",
            "epoch 008/010, batch 001/002, step 0190/0496: loss=2.4355363845825195\n",
            "epoch 008/010, batch 001/002, step 0191/0496: loss=3.6706039905548096\n",
            "epoch 008/010, batch 001/002, step 0192/0496: loss=2.007786273956299\n",
            "epoch 008/010, batch 001/002, step 0193/0496: loss=1.3348658084869385\n",
            "epoch 008/010, batch 001/002, step 0194/0496: loss=1.8841354846954346\n",
            "epoch 008/010, batch 001/002, step 0195/0496: loss=1.792926549911499\n",
            "epoch 008/010, batch 001/002, step 0196/0496: loss=1.8984959125518799\n",
            "epoch 008/010, batch 001/002, step 0197/0496: loss=1.6912357807159424\n",
            "epoch 008/010, batch 001/002, step 0198/0496: loss=2.2132086753845215\n",
            "epoch 008/010, batch 001/002, step 0199/0496: loss=1.654327630996704\n",
            "epoch 008/010, batch 001/002, step 0200/0496: loss=1.794309139251709\n",
            "epoch 008/010, batch 001/002, step 0201/0496: loss=1.7156726121902466\n",
            "epoch 008/010, batch 001/002, step 0202/0496: loss=2.0380797386169434\n",
            "epoch 008/010, batch 001/002, step 0203/0496: loss=1.499591588973999\n",
            "epoch 008/010, batch 001/002, step 0204/0496: loss=2.010531187057495\n",
            "epoch 008/010, batch 001/002, step 0205/0496: loss=1.6078405380249023\n",
            "epoch 008/010, batch 001/002, step 0206/0496: loss=2.1748046875\n",
            "epoch 008/010, batch 001/002, step 0207/0496: loss=1.8292982578277588\n",
            "epoch 008/010, batch 001/002, step 0208/0496: loss=2.697631359100342\n",
            "epoch 008/010, batch 001/002, step 0209/0496: loss=2.1924655437469482\n",
            "epoch 008/010, batch 001/002, step 0210/0496: loss=1.7090849876403809\n",
            "epoch 008/010, batch 001/002, step 0211/0496: loss=2.03810453414917\n",
            "epoch 008/010, batch 001/002, step 0212/0496: loss=2.099820613861084\n",
            "epoch 008/010, batch 001/002, step 0213/0496: loss=1.8405225276947021\n",
            "epoch 008/010, batch 001/002, step 0214/0496: loss=2.118978261947632\n",
            "epoch 008/010, batch 001/002, step 0215/0496: loss=2.2090282440185547\n",
            "epoch 008/010, batch 001/002, step 0216/0496: loss=1.76832115650177\n",
            "epoch 008/010, batch 001/002, step 0217/0496: loss=2.219641923904419\n",
            "epoch 008/010, batch 001/002, step 0218/0496: loss=1.815474271774292\n",
            "epoch 008/010, batch 001/002, step 0219/0496: loss=3.138101100921631\n",
            "epoch 008/010, batch 001/002, step 0220/0496: loss=1.9001216888427734\n",
            "epoch 008/010, batch 001/002, step 0221/0496: loss=2.0184080600738525\n",
            "epoch 008/010, batch 001/002, step 0222/0496: loss=1.9025206565856934\n",
            "epoch 008/010, batch 001/002, step 0223/0496: loss=3.7690558433532715\n",
            "epoch 008/010, batch 001/002, step 0224/0496: loss=2.3008363246917725\n",
            "epoch 008/010, batch 001/002, step 0225/0496: loss=2.075789451599121\n",
            "epoch 008/010, batch 001/002, step 0226/0496: loss=2.000195264816284\n",
            "epoch 008/010, batch 001/002, step 0227/0496: loss=1.6453304290771484\n",
            "epoch 008/010, batch 001/002, step 0228/0496: loss=1.7879338264465332\n",
            "epoch 008/010, batch 001/002, step 0229/0496: loss=1.4801130294799805\n",
            "epoch 008/010, batch 001/002, step 0230/0496: loss=2.202305316925049\n",
            "epoch 008/010, batch 001/002, step 0231/0496: loss=2.0835976600646973\n",
            "epoch 008/010, batch 001/002, step 0232/0496: loss=1.5044658184051514\n",
            "epoch 008/010, batch 001/002, step 0233/0496: loss=1.8422274589538574\n",
            "epoch 008/010, batch 001/002, step 0234/0496: loss=1.7373583316802979\n",
            "epoch 008/010, batch 001/002, step 0235/0496: loss=1.7488991022109985\n",
            "epoch 008/010, batch 001/002, step 0236/0496: loss=2.0843658447265625\n",
            "epoch 008/010, batch 001/002, step 0237/0496: loss=2.395624876022339\n",
            "epoch 008/010, batch 001/002, step 0238/0496: loss=1.934919834136963\n",
            "epoch 008/010, batch 001/002, step 0239/0496: loss=1.6990410089492798\n",
            "epoch 008/010, batch 001/002, step 0240/0496: loss=1.8166117668151855\n",
            "epoch 008/010, batch 001/002, step 0241/0496: loss=1.7013392448425293\n",
            "epoch 008/010, batch 001/002, step 0242/0496: loss=1.412950038909912\n",
            "epoch 008/010, batch 001/002, step 0243/0496: loss=1.5743498802185059\n",
            "epoch 008/010, batch 001/002, step 0244/0496: loss=3.5092175006866455\n",
            "epoch 008/010, batch 001/002, step 0245/0496: loss=2.361772298812866\n",
            "epoch 008/010, batch 001/002, step 0246/0496: loss=1.5336288213729858\n",
            "epoch 008/010, batch 001/002, step 0247/0496: loss=1.9639089107513428\n",
            "epoch 008/010, batch 001/002, step 0248/0496: loss=1.8625845909118652\n",
            "epoch 008/010, batch 001/002, step 0249/0496: loss=2.466693639755249\n",
            "epoch 008/010, batch 001/002, step 0250/0496: loss=1.6881318092346191\n",
            "epoch 008/010, batch 001/002, step 0251/0496: loss=2.089118719100952\n",
            "epoch 008/010, batch 001/002, step 0252/0496: loss=1.6390376091003418\n",
            "epoch 008/010, batch 001/002, step 0253/0496: loss=2.518350839614868\n",
            "epoch 008/010, batch 001/002, step 0254/0496: loss=2.1131255626678467\n",
            "epoch 008/010, batch 001/002, step 0255/0496: loss=1.5784482955932617\n",
            "epoch 008/010, batch 001/002, step 0256/0496: loss=2.3091022968292236\n",
            "epoch 008/010, batch 001/002, step 0257/0496: loss=2.830172061920166\n",
            "epoch 008/010, batch 001/002, step 0258/0496: loss=2.5371456146240234\n",
            "epoch 008/010, batch 001/002, step 0259/0496: loss=2.8213038444519043\n",
            "epoch 008/010, batch 001/002, step 0260/0496: loss=1.937718152999878\n",
            "epoch 008/010, batch 001/002, step 0261/0496: loss=2.283700704574585\n",
            "epoch 008/010, batch 001/002, step 0262/0496: loss=1.8878698348999023\n",
            "epoch 008/010, batch 001/002, step 0263/0496: loss=1.980459213256836\n",
            "epoch 008/010, batch 001/002, step 0264/0496: loss=1.8323265314102173\n",
            "epoch 008/010, batch 001/002, step 0265/0496: loss=3.138577461242676\n",
            "epoch 008/010, batch 001/002, step 0266/0496: loss=2.00333309173584\n",
            "epoch 008/010, batch 001/002, step 0267/0496: loss=3.0552210807800293\n",
            "epoch 008/010, batch 001/002, step 0268/0496: loss=1.6546719074249268\n",
            "epoch 008/010, batch 001/002, step 0269/0496: loss=2.5759410858154297\n",
            "epoch 008/010, batch 001/002, step 0270/0496: loss=2.6335396766662598\n",
            "epoch 008/010, batch 001/002, step 0271/0496: loss=3.1258583068847656\n",
            "epoch 008/010, batch 001/002, step 0272/0496: loss=2.625532627105713\n",
            "epoch 008/010, batch 001/002, step 0273/0496: loss=2.529543399810791\n",
            "epoch 008/010, batch 001/002, step 0274/0496: loss=3.5114381313323975\n",
            "epoch 008/010, batch 001/002, step 0275/0496: loss=2.7990946769714355\n",
            "epoch 008/010, batch 001/002, step 0276/0496: loss=5.652592658996582\n",
            "epoch 008/010, batch 001/002, step 0277/0496: loss=2.303260087966919\n",
            "epoch 008/010, batch 001/002, step 0278/0496: loss=2.7058935165405273\n",
            "epoch 008/010, batch 001/002, step 0279/0496: loss=3.7259347438812256\n",
            "epoch 008/010, batch 001/002, step 0280/0496: loss=4.197855472564697\n",
            "epoch 008/010, batch 001/002, step 0281/0496: loss=3.219311237335205\n",
            "epoch 008/010, batch 001/002, step 0282/0496: loss=2.532862901687622\n",
            "epoch 008/010, batch 001/002, step 0283/0496: loss=3.6906116008758545\n",
            "epoch 008/010, batch 001/002, step 0284/0496: loss=2.151278495788574\n",
            "epoch 008/010, batch 001/002, step 0285/0496: loss=3.5573976039886475\n",
            "epoch 008/010, batch 001/002, step 0286/0496: loss=2.5139098167419434\n",
            "epoch 008/010, batch 001/002, step 0287/0496: loss=3.237642288208008\n",
            "epoch 008/010, batch 001/002, step 0288/0496: loss=2.333634853363037\n",
            "epoch 008/010, batch 001/002, step 0289/0496: loss=2.5584940910339355\n",
            "epoch 008/010, batch 001/002, step 0290/0496: loss=3.977202892303467\n",
            "epoch 008/010, batch 001/002, step 0291/0496: loss=1.7764272689819336\n",
            "epoch 008/010, batch 001/002, step 0292/0496: loss=2.398726224899292\n",
            "epoch 008/010, batch 001/002, step 0293/0496: loss=2.126972198486328\n",
            "epoch 008/010, batch 001/002, step 0294/0496: loss=3.2504353523254395\n",
            "epoch 008/010, batch 001/002, step 0295/0496: loss=2.247750759124756\n",
            "epoch 008/010, batch 001/002, step 0296/0496: loss=3.3193416595458984\n",
            "epoch 008/010, batch 001/002, step 0297/0496: loss=1.5422677993774414\n",
            "epoch 008/010, batch 001/002, step 0298/0496: loss=3.5317201614379883\n",
            "epoch 008/010, batch 001/002, step 0299/0496: loss=3.1398496627807617\n",
            "epoch 008/010, batch 001/002, step 0300/0496: loss=3.712660312652588\n",
            "epoch 008/010, batch 001/002, step 0301/0496: loss=2.6270253658294678\n",
            "epoch 008/010, batch 001/002, step 0302/0496: loss=3.3373026847839355\n",
            "epoch 008/010, batch 001/002, step 0303/0496: loss=2.857079029083252\n",
            "epoch 008/010, batch 001/002, step 0304/0496: loss=2.0905213356018066\n",
            "epoch 008/010, batch 001/002, step 0305/0496: loss=2.8688249588012695\n",
            "epoch 008/010, batch 001/002, step 0306/0496: loss=1.8332066535949707\n",
            "epoch 008/010, batch 001/002, step 0307/0496: loss=2.0686283111572266\n",
            "epoch 008/010, batch 001/002, step 0308/0496: loss=1.8553200960159302\n",
            "epoch 008/010, batch 001/002, step 0309/0496: loss=3.023641347885132\n",
            "epoch 008/010, batch 001/002, step 0310/0496: loss=2.1718809604644775\n",
            "epoch 008/010, batch 001/002, step 0311/0496: loss=2.571694850921631\n",
            "epoch 008/010, batch 001/002, step 0312/0496: loss=2.029557466506958\n",
            "epoch 008/010, batch 001/002, step 0313/0496: loss=1.7886404991149902\n",
            "epoch 008/010, batch 001/002, step 0314/0496: loss=1.9414676427841187\n",
            "epoch 008/010, batch 001/002, step 0315/0496: loss=1.9200539588928223\n",
            "epoch 008/010, batch 001/002, step 0316/0496: loss=2.865098237991333\n",
            "epoch 008/010, batch 001/002, step 0317/0496: loss=2.0539798736572266\n",
            "epoch 008/010, batch 001/002, step 0318/0496: loss=1.9436613321304321\n",
            "epoch 008/010, batch 001/002, step 0319/0496: loss=2.6191422939300537\n",
            "epoch 008/010, batch 001/002, step 0320/0496: loss=1.9586631059646606\n",
            "epoch 008/010, batch 001/002, step 0321/0496: loss=2.534712314605713\n",
            "epoch 008/010, batch 001/002, step 0322/0496: loss=1.775956153869629\n",
            "epoch 008/010, batch 001/002, step 0323/0496: loss=2.191316604614258\n",
            "epoch 008/010, batch 001/002, step 0324/0496: loss=1.4918845891952515\n",
            "epoch 008/010, batch 001/002, step 0325/0496: loss=2.0619654655456543\n",
            "epoch 008/010, batch 001/002, step 0326/0496: loss=2.363971471786499\n",
            "epoch 008/010, batch 001/002, step 0327/0496: loss=2.271111011505127\n",
            "epoch 008/010, batch 001/002, step 0328/0496: loss=2.5614962577819824\n",
            "epoch 008/010, batch 001/002, step 0329/0496: loss=1.9611607789993286\n",
            "epoch 008/010, batch 001/002, step 0330/0496: loss=1.9547123908996582\n",
            "epoch 008/010, batch 001/002, step 0331/0496: loss=1.6808160543441772\n",
            "epoch 008/010, batch 001/002, step 0332/0496: loss=2.366269826889038\n",
            "epoch 008/010, batch 001/002, step 0333/0496: loss=1.785390019416809\n",
            "epoch 008/010, batch 001/002, step 0334/0496: loss=2.4672441482543945\n",
            "epoch 008/010, batch 001/002, step 0335/0496: loss=1.6546587944030762\n",
            "epoch 008/010, batch 001/002, step 0336/0496: loss=2.354177236557007\n",
            "epoch 008/010, batch 001/002, step 0337/0496: loss=1.9579861164093018\n",
            "epoch 008/010, batch 001/002, step 0338/0496: loss=2.109438419342041\n",
            "epoch 008/010, batch 001/002, step 0339/0496: loss=2.1840600967407227\n",
            "epoch 008/010, batch 001/002, step 0340/0496: loss=2.039625644683838\n",
            "epoch 008/010, batch 001/002, step 0341/0496: loss=1.975982904434204\n",
            "epoch 008/010, batch 001/002, step 0342/0496: loss=2.0575497150421143\n",
            "epoch 008/010, batch 001/002, step 0343/0496: loss=1.808148980140686\n",
            "epoch 008/010, batch 001/002, step 0344/0496: loss=2.3004672527313232\n",
            "epoch 008/010, batch 001/002, step 0345/0496: loss=2.9384114742279053\n",
            "epoch 008/010, batch 001/002, step 0346/0496: loss=1.9017927646636963\n",
            "epoch 008/010, batch 001/002, step 0347/0496: loss=1.6791157722473145\n",
            "epoch 008/010, batch 001/002, step 0348/0496: loss=1.4110698699951172\n",
            "epoch 008/010, batch 001/002, step 0349/0496: loss=2.035733222961426\n",
            "epoch 008/010, batch 001/002, step 0350/0496: loss=2.103818655014038\n",
            "epoch 008/010, batch 001/002, step 0351/0496: loss=1.623313307762146\n",
            "epoch 008/010, batch 001/002, step 0352/0496: loss=2.2458438873291016\n",
            "epoch 008/010, batch 001/002, step 0353/0496: loss=2.048361301422119\n",
            "epoch 008/010, batch 001/002, step 0354/0496: loss=2.0097458362579346\n",
            "epoch 008/010, batch 001/002, step 0355/0496: loss=1.5632175207138062\n",
            "epoch 008/010, batch 001/002, step 0356/0496: loss=1.8654615879058838\n",
            "epoch 008/010, batch 001/002, step 0357/0496: loss=2.1076877117156982\n",
            "epoch 008/010, batch 001/002, step 0358/0496: loss=2.0378153324127197\n",
            "epoch 008/010, batch 001/002, step 0359/0496: loss=1.8575429916381836\n",
            "epoch 008/010, batch 001/002, step 0360/0496: loss=2.1465396881103516\n",
            "epoch 008/010, batch 001/002, step 0361/0496: loss=3.488530397415161\n",
            "epoch 008/010, batch 001/002, step 0362/0496: loss=1.6559038162231445\n",
            "epoch 008/010, batch 001/002, step 0363/0496: loss=1.8779243230819702\n",
            "epoch 008/010, batch 001/002, step 0364/0496: loss=1.60296630859375\n",
            "epoch 008/010, batch 001/002, step 0365/0496: loss=1.73945152759552\n",
            "epoch 008/010, batch 001/002, step 0366/0496: loss=1.6188905239105225\n",
            "epoch 008/010, batch 001/002, step 0367/0496: loss=2.0113754272460938\n",
            "epoch 008/010, batch 001/002, step 0368/0496: loss=2.5021023750305176\n",
            "epoch 008/010, batch 001/002, step 0369/0496: loss=1.9526423215866089\n",
            "epoch 008/010, batch 001/002, step 0370/0496: loss=2.0106637477874756\n",
            "epoch 008/010, batch 001/002, step 0371/0496: loss=2.1052582263946533\n",
            "epoch 008/010, batch 001/002, step 0372/0496: loss=2.1525912284851074\n",
            "epoch 008/010, batch 001/002, step 0373/0496: loss=2.1577277183532715\n",
            "epoch 008/010, batch 001/002, step 0374/0496: loss=1.677897334098816\n",
            "epoch 008/010, batch 001/002, step 0375/0496: loss=2.4599251747131348\n",
            "epoch 008/010, batch 001/002, step 0376/0496: loss=1.4108142852783203\n",
            "epoch 008/010, batch 001/002, step 0377/0496: loss=1.9912056922912598\n",
            "epoch 008/010, batch 001/002, step 0378/0496: loss=2.0142369270324707\n",
            "epoch 008/010, batch 001/002, step 0379/0496: loss=1.6036126613616943\n",
            "epoch 008/010, batch 001/002, step 0380/0496: loss=1.650026559829712\n",
            "epoch 008/010, batch 001/002, step 0381/0496: loss=1.8757200241088867\n",
            "epoch 008/010, batch 001/002, step 0382/0496: loss=2.5425496101379395\n",
            "epoch 008/010, batch 001/002, step 0383/0496: loss=1.6727700233459473\n",
            "epoch 008/010, batch 001/002, step 0384/0496: loss=1.4735445976257324\n",
            "epoch 008/010, batch 001/002, step 0385/0496: loss=1.8490569591522217\n",
            "epoch 008/010, batch 001/002, step 0386/0496: loss=1.803558349609375\n",
            "epoch 008/010, batch 001/002, step 0387/0496: loss=2.6841580867767334\n",
            "epoch 008/010, batch 001/002, step 0388/0496: loss=1.964623212814331\n",
            "epoch 008/010, batch 001/002, step 0389/0496: loss=1.78778874874115\n",
            "epoch 008/010, batch 001/002, step 0390/0496: loss=1.924421787261963\n",
            "epoch 008/010, batch 001/002, step 0391/0496: loss=1.8032798767089844\n",
            "epoch 008/010, batch 001/002, step 0392/0496: loss=1.8025333881378174\n",
            "epoch 008/010, batch 001/002, step 0393/0496: loss=2.612971305847168\n",
            "epoch 008/010, batch 001/002, step 0394/0496: loss=2.057922840118408\n",
            "epoch 008/010, batch 001/002, step 0395/0496: loss=2.268887996673584\n",
            "epoch 008/010, batch 001/002, step 0396/0496: loss=1.410672664642334\n",
            "epoch 008/010, batch 001/002, step 0397/0496: loss=1.774885892868042\n",
            "epoch 008/010, batch 001/002, step 0398/0496: loss=1.838800311088562\n",
            "epoch 008/010, batch 001/002, step 0399/0496: loss=1.609734058380127\n",
            "epoch 008/010, batch 001/002, step 0400/0496: loss=1.6600896120071411\n",
            "epoch 008/010, batch 001/002, step 0401/0496: loss=2.0288243293762207\n",
            "epoch 008/010, batch 001/002, step 0402/0496: loss=2.027496814727783\n",
            "epoch 008/010, batch 001/002, step 0403/0496: loss=2.7881715297698975\n",
            "epoch 008/010, batch 001/002, step 0404/0496: loss=1.3476166725158691\n",
            "epoch 008/010, batch 001/002, step 0405/0496: loss=2.2589924335479736\n",
            "epoch 008/010, batch 001/002, step 0406/0496: loss=1.8783996105194092\n",
            "epoch 008/010, batch 001/002, step 0407/0496: loss=1.7364146709442139\n",
            "epoch 008/010, batch 001/002, step 0408/0496: loss=1.6789613962173462\n",
            "epoch 008/010, batch 001/002, step 0409/0496: loss=3.186157703399658\n",
            "epoch 008/010, batch 001/002, step 0410/0496: loss=1.5427637100219727\n",
            "epoch 008/010, batch 001/002, step 0411/0496: loss=1.89211106300354\n",
            "epoch 008/010, batch 001/002, step 0412/0496: loss=1.45521080493927\n",
            "epoch 008/010, batch 001/002, step 0413/0496: loss=1.7120048999786377\n",
            "epoch 008/010, batch 001/002, step 0414/0496: loss=1.9035464525222778\n",
            "epoch 008/010, batch 001/002, step 0415/0496: loss=2.091616630554199\n",
            "epoch 008/010, batch 001/002, step 0416/0496: loss=1.8658467531204224\n",
            "epoch 008/010, batch 001/002, step 0417/0496: loss=1.7610937356948853\n",
            "epoch 008/010, batch 001/002, step 0418/0496: loss=2.1075937747955322\n",
            "epoch 008/010, batch 001/002, step 0419/0496: loss=2.165593147277832\n",
            "epoch 008/010, batch 001/002, step 0420/0496: loss=1.7089645862579346\n",
            "epoch 008/010, batch 001/002, step 0421/0496: loss=1.6704176664352417\n",
            "epoch 008/010, batch 001/002, step 0422/0496: loss=1.9841718673706055\n",
            "epoch 008/010, batch 001/002, step 0423/0496: loss=1.9792771339416504\n",
            "epoch 008/010, batch 001/002, step 0424/0496: loss=1.8435492515563965\n",
            "epoch 008/010, batch 001/002, step 0425/0496: loss=2.0526020526885986\n",
            "epoch 008/010, batch 001/002, step 0426/0496: loss=1.709374189376831\n",
            "epoch 008/010, batch 001/002, step 0427/0496: loss=1.724585771560669\n",
            "epoch 008/010, batch 001/002, step 0428/0496: loss=1.5798546075820923\n",
            "epoch 008/010, batch 001/002, step 0429/0496: loss=1.9609229564666748\n",
            "epoch 008/010, batch 001/002, step 0430/0496: loss=1.7029681205749512\n",
            "epoch 008/010, batch 001/002, step 0431/0496: loss=1.4271316528320312\n",
            "epoch 008/010, batch 001/002, step 0432/0496: loss=1.4088866710662842\n",
            "epoch 008/010, batch 001/002, step 0433/0496: loss=2.023761749267578\n",
            "epoch 008/010, batch 001/002, step 0434/0496: loss=1.6628631353378296\n",
            "epoch 008/010, batch 001/002, step 0435/0496: loss=2.1250813007354736\n",
            "epoch 008/010, batch 001/002, step 0436/0496: loss=2.062178134918213\n",
            "epoch 008/010, batch 001/002, step 0437/0496: loss=1.8624249696731567\n",
            "epoch 008/010, batch 001/002, step 0438/0496: loss=1.4419888257980347\n",
            "epoch 008/010, batch 001/002, step 0439/0496: loss=2.177795886993408\n",
            "epoch 008/010, batch 001/002, step 0440/0496: loss=1.8812060356140137\n",
            "epoch 008/010, batch 001/002, step 0441/0496: loss=1.6010385751724243\n",
            "epoch 008/010, batch 001/002, step 0442/0496: loss=2.0074093341827393\n",
            "epoch 008/010, batch 001/002, step 0443/0496: loss=2.04249906539917\n",
            "epoch 008/010, batch 001/002, step 0444/0496: loss=1.425429105758667\n",
            "epoch 008/010, batch 001/002, step 0445/0496: loss=1.7260732650756836\n",
            "epoch 008/010, batch 001/002, step 0446/0496: loss=2.106651782989502\n",
            "epoch 008/010, batch 001/002, step 0447/0496: loss=1.6320534944534302\n",
            "epoch 008/010, batch 001/002, step 0448/0496: loss=2.043491840362549\n",
            "epoch 008/010, batch 001/002, step 0449/0496: loss=2.1332414150238037\n",
            "epoch 008/010, batch 001/002, step 0450/0496: loss=2.080733299255371\n",
            "epoch 008/010, batch 001/002, step 0451/0496: loss=2.1821043491363525\n",
            "epoch 008/010, batch 001/002, step 0452/0496: loss=1.9873546361923218\n",
            "epoch 008/010, batch 001/002, step 0453/0496: loss=1.6014981269836426\n",
            "epoch 008/010, batch 001/002, step 0454/0496: loss=1.8182839155197144\n",
            "epoch 008/010, batch 001/002, step 0455/0496: loss=1.7655068635940552\n",
            "epoch 008/010, batch 001/002, step 0456/0496: loss=2.057847261428833\n",
            "epoch 008/010, batch 001/002, step 0457/0496: loss=2.1362123489379883\n",
            "epoch 008/010, batch 001/002, step 0458/0496: loss=1.5523993968963623\n",
            "epoch 008/010, batch 001/002, step 0459/0496: loss=2.036411762237549\n",
            "epoch 008/010, batch 001/002, step 0460/0496: loss=1.9163818359375\n",
            "epoch 008/010, batch 001/002, step 0461/0496: loss=1.5549967288970947\n",
            "epoch 008/010, batch 001/002, step 0462/0496: loss=1.9742021560668945\n",
            "epoch 008/010, batch 001/002, step 0463/0496: loss=2.1482126712799072\n",
            "epoch 008/010, batch 001/002, step 0464/0496: loss=2.2198486328125\n",
            "epoch 008/010, batch 001/002, step 0465/0496: loss=1.657428503036499\n",
            "epoch 008/010, batch 001/002, step 0466/0496: loss=1.4276132583618164\n",
            "epoch 008/010, batch 001/002, step 0467/0496: loss=1.6906309127807617\n",
            "epoch 008/010, batch 001/002, step 0468/0496: loss=1.6661763191223145\n",
            "epoch 008/010, batch 001/002, step 0469/0496: loss=1.9304373264312744\n",
            "epoch 008/010, batch 001/002, step 0470/0496: loss=2.1811728477478027\n",
            "epoch 008/010, batch 001/002, step 0471/0496: loss=1.8328821659088135\n",
            "epoch 008/010, batch 001/002, step 0472/0496: loss=1.952699899673462\n",
            "epoch 008/010, batch 001/002, step 0473/0496: loss=1.5442020893096924\n",
            "epoch 008/010, batch 001/002, step 0474/0496: loss=1.7788702249526978\n",
            "epoch 008/010, batch 001/002, step 0475/0496: loss=1.815228819847107\n",
            "epoch 008/010, batch 001/002, step 0476/0496: loss=1.9627583026885986\n",
            "epoch 008/010, batch 001/002, step 0477/0496: loss=3.5937328338623047\n",
            "epoch 008/010, batch 001/002, step 0478/0496: loss=1.5833560228347778\n",
            "epoch 008/010, batch 001/002, step 0479/0496: loss=1.8874521255493164\n",
            "epoch 008/010, batch 001/002, step 0480/0496: loss=1.61301589012146\n",
            "epoch 008/010, batch 001/002, step 0481/0496: loss=2.2510251998901367\n",
            "epoch 008/010, batch 001/002, step 0482/0496: loss=2.2340087890625\n",
            "epoch 008/010, batch 001/002, step 0483/0496: loss=1.9040402173995972\n",
            "epoch 008/010, batch 001/002, step 0484/0496: loss=1.9707733392715454\n",
            "epoch 008/010, batch 001/002, step 0485/0496: loss=1.8721134662628174\n",
            "epoch 008/010, batch 001/002, step 0486/0496: loss=1.3083094358444214\n",
            "epoch 008/010, batch 001/002, step 0487/0496: loss=3.4742021560668945\n",
            "epoch 008/010, batch 001/002, step 0488/0496: loss=1.4477262496948242\n",
            "epoch 008/010, batch 001/002, step 0489/0496: loss=2.6618220806121826\n",
            "epoch 008/010, batch 001/002, step 0490/0496: loss=2.1688499450683594\n",
            "epoch 008/010, batch 001/002, step 0491/0496: loss=1.5014034509658813\n",
            "epoch 008/010, batch 001/002, step 0492/0496: loss=1.8355836868286133\n",
            "epoch 008/010, batch 001/002, step 0493/0496: loss=1.4689514636993408\n",
            "epoch 008/010, batch 001/002, step 0494/0496: loss=1.70431649684906\n",
            "epoch 008/010, batch 001/002, step 0495/0496: loss=1.5555630922317505\n",
            "epoch 008/010, batch 001/002, step 0496/0496: loss=2.4005517959594727\n",
            "epoch 008/010, batch 002/002, step 0001/0496: loss=1.4927409887313843\n",
            "epoch 008/010, batch 002/002, step 0002/0496: loss=3.292637825012207\n",
            "epoch 008/010, batch 002/002, step 0003/0496: loss=2.099811315536499\n",
            "epoch 008/010, batch 002/002, step 0004/0496: loss=1.5072762966156006\n",
            "epoch 008/010, batch 002/002, step 0005/0496: loss=1.7288804054260254\n",
            "epoch 008/010, batch 002/002, step 0006/0496: loss=2.2669644355773926\n",
            "epoch 008/010, batch 002/002, step 0007/0496: loss=1.9820492267608643\n",
            "epoch 008/010, batch 002/002, step 0008/0496: loss=1.521493911743164\n",
            "epoch 008/010, batch 002/002, step 0009/0496: loss=2.399628162384033\n",
            "epoch 008/010, batch 002/002, step 0010/0496: loss=2.0004701614379883\n",
            "epoch 008/010, batch 002/002, step 0011/0496: loss=1.8367458581924438\n",
            "epoch 008/010, batch 002/002, step 0012/0496: loss=1.6552232503890991\n",
            "epoch 008/010, batch 002/002, step 0013/0496: loss=1.6629914045333862\n",
            "epoch 008/010, batch 002/002, step 0014/0496: loss=1.8831140995025635\n",
            "epoch 008/010, batch 002/002, step 0015/0496: loss=1.8868027925491333\n",
            "epoch 008/010, batch 002/002, step 0016/0496: loss=1.6975691318511963\n",
            "epoch 008/010, batch 002/002, step 0017/0496: loss=1.8414908647537231\n",
            "epoch 008/010, batch 002/002, step 0018/0496: loss=1.7957332134246826\n",
            "epoch 008/010, batch 002/002, step 0019/0496: loss=1.9378318786621094\n",
            "epoch 008/010, batch 002/002, step 0020/0496: loss=1.5496091842651367\n",
            "epoch 008/010, batch 002/002, step 0021/0496: loss=2.0364503860473633\n",
            "epoch 008/010, batch 002/002, step 0022/0496: loss=2.0037755966186523\n",
            "epoch 008/010, batch 002/002, step 0023/0496: loss=1.6659919023513794\n",
            "epoch 008/010, batch 002/002, step 0024/0496: loss=1.708284616470337\n",
            "epoch 008/010, batch 002/002, step 0025/0496: loss=3.6555776596069336\n",
            "epoch 008/010, batch 002/002, step 0026/0496: loss=1.5721650123596191\n",
            "epoch 008/010, batch 002/002, step 0027/0496: loss=2.184922695159912\n",
            "epoch 008/010, batch 002/002, step 0028/0496: loss=2.459807872772217\n",
            "epoch 008/010, batch 002/002, step 0029/0496: loss=2.020691394805908\n",
            "epoch 008/010, batch 002/002, step 0030/0496: loss=3.1342220306396484\n",
            "epoch 008/010, batch 002/002, step 0031/0496: loss=1.5089759826660156\n",
            "epoch 008/010, batch 002/002, step 0032/0496: loss=1.8753409385681152\n",
            "epoch 008/010, batch 002/002, step 0033/0496: loss=1.7400920391082764\n",
            "epoch 008/010, batch 002/002, step 0034/0496: loss=2.1773509979248047\n",
            "epoch 008/010, batch 002/002, step 0035/0496: loss=2.2556934356689453\n",
            "epoch 008/010, batch 002/002, step 0036/0496: loss=2.246762275695801\n",
            "epoch 008/010, batch 002/002, step 0037/0496: loss=1.7369771003723145\n",
            "epoch 008/010, batch 002/002, step 0038/0496: loss=2.144430160522461\n",
            "epoch 008/010, batch 002/002, step 0039/0496: loss=1.958694577217102\n",
            "epoch 008/010, batch 002/002, step 0040/0496: loss=1.6794779300689697\n",
            "epoch 008/010, batch 002/002, step 0041/0496: loss=2.9827985763549805\n",
            "epoch 008/010, batch 002/002, step 0042/0496: loss=1.9632450342178345\n",
            "epoch 008/010, batch 002/002, step 0043/0496: loss=1.520516037940979\n",
            "epoch 008/010, batch 002/002, step 0044/0496: loss=2.3259027004241943\n",
            "epoch 008/010, batch 002/002, step 0045/0496: loss=1.6774868965148926\n",
            "epoch 008/010, batch 002/002, step 0046/0496: loss=1.5841515064239502\n",
            "epoch 008/010, batch 002/002, step 0047/0496: loss=1.516465187072754\n",
            "epoch 008/010, batch 002/002, step 0048/0496: loss=2.0805537700653076\n",
            "epoch 008/010, batch 002/002, step 0049/0496: loss=1.9887723922729492\n",
            "epoch 008/010, batch 002/002, step 0050/0496: loss=2.5348565578460693\n",
            "epoch 008/010, batch 002/002, step 0051/0496: loss=1.7465424537658691\n",
            "epoch 008/010, batch 002/002, step 0052/0496: loss=1.6667529344558716\n",
            "epoch 008/010, batch 002/002, step 0053/0496: loss=1.7202661037445068\n",
            "epoch 008/010, batch 002/002, step 0054/0496: loss=1.5995330810546875\n",
            "epoch 008/010, batch 002/002, step 0055/0496: loss=2.189741611480713\n",
            "epoch 008/010, batch 002/002, step 0056/0496: loss=1.288978934288025\n",
            "epoch 008/010, batch 002/002, step 0057/0496: loss=1.4601268768310547\n",
            "epoch 008/010, batch 002/002, step 0058/0496: loss=1.9451265335083008\n",
            "epoch 008/010, batch 002/002, step 0059/0496: loss=1.7833247184753418\n",
            "epoch 008/010, batch 002/002, step 0060/0496: loss=1.7029856443405151\n",
            "epoch 008/010, batch 002/002, step 0061/0496: loss=1.6970100402832031\n",
            "epoch 008/010, batch 002/002, step 0062/0496: loss=1.4611669778823853\n",
            "epoch 008/010, batch 002/002, step 0063/0496: loss=2.394200563430786\n",
            "epoch 008/010, batch 002/002, step 0064/0496: loss=1.6385257244110107\n",
            "epoch 008/010, batch 002/002, step 0065/0496: loss=1.7915464639663696\n",
            "epoch 008/010, batch 002/002, step 0066/0496: loss=1.903869867324829\n",
            "epoch 008/010, batch 002/002, step 0067/0496: loss=1.5877277851104736\n",
            "epoch 008/010, batch 002/002, step 0068/0496: loss=1.671954870223999\n",
            "epoch 008/010, batch 002/002, step 0069/0496: loss=1.631532907485962\n",
            "epoch 008/010, batch 002/002, step 0070/0496: loss=1.6112442016601562\n",
            "epoch 008/010, batch 002/002, step 0071/0496: loss=1.9112849235534668\n",
            "epoch 008/010, batch 002/002, step 0072/0496: loss=1.9066846370697021\n",
            "epoch 008/010, batch 002/002, step 0073/0496: loss=1.9012150764465332\n",
            "epoch 008/010, batch 002/002, step 0074/0496: loss=1.4055213928222656\n",
            "epoch 008/010, batch 002/002, step 0075/0496: loss=1.8979246616363525\n",
            "epoch 008/010, batch 002/002, step 0076/0496: loss=1.2363958358764648\n",
            "epoch 008/010, batch 002/002, step 0077/0496: loss=2.2197210788726807\n",
            "epoch 008/010, batch 002/002, step 0078/0496: loss=3.3129677772521973\n",
            "epoch 008/010, batch 002/002, step 0079/0496: loss=1.725555658340454\n",
            "epoch 008/010, batch 002/002, step 0080/0496: loss=1.5371849536895752\n",
            "epoch 008/010, batch 002/002, step 0081/0496: loss=1.6961612701416016\n",
            "epoch 008/010, batch 002/002, step 0082/0496: loss=2.190107583999634\n",
            "epoch 008/010, batch 002/002, step 0083/0496: loss=1.5964112281799316\n",
            "epoch 008/010, batch 002/002, step 0084/0496: loss=3.120896816253662\n",
            "epoch 008/010, batch 002/002, step 0085/0496: loss=3.271456241607666\n",
            "epoch 008/010, batch 002/002, step 0086/0496: loss=1.794969916343689\n",
            "epoch 008/010, batch 002/002, step 0087/0496: loss=1.8841123580932617\n",
            "epoch 008/010, batch 002/002, step 0088/0496: loss=1.9992108345031738\n",
            "epoch 008/010, batch 002/002, step 0089/0496: loss=3.486985206604004\n",
            "epoch 008/010, batch 002/002, step 0090/0496: loss=1.7623865604400635\n",
            "epoch 008/010, batch 002/002, step 0091/0496: loss=1.4377317428588867\n",
            "epoch 008/010, batch 002/002, step 0092/0496: loss=2.1123132705688477\n",
            "epoch 008/010, batch 002/002, step 0093/0496: loss=1.63721764087677\n",
            "epoch 008/010, batch 002/002, step 0094/0496: loss=1.7079417705535889\n",
            "epoch 008/010, batch 002/002, step 0095/0496: loss=2.1074295043945312\n",
            "epoch 008/010, batch 002/002, step 0096/0496: loss=1.639859914779663\n",
            "epoch 008/010, batch 002/002, step 0097/0496: loss=1.629015564918518\n",
            "epoch 008/010, batch 002/002, step 0098/0496: loss=1.9656049013137817\n",
            "epoch 008/010, batch 002/002, step 0099/0496: loss=1.5731910467147827\n",
            "epoch 008/010, batch 002/002, step 0100/0496: loss=1.9089035987854004\n",
            "epoch 008/010, batch 002/002, step 0101/0496: loss=2.228816032409668\n",
            "epoch 008/010, batch 002/002, step 0102/0496: loss=1.8688223361968994\n",
            "epoch 008/010, batch 002/002, step 0103/0496: loss=1.3627450466156006\n",
            "epoch 008/010, batch 002/002, step 0104/0496: loss=1.5733731985092163\n",
            "epoch 008/010, batch 002/002, step 0105/0496: loss=1.7437827587127686\n",
            "epoch 008/010, batch 002/002, step 0106/0496: loss=1.7731940746307373\n",
            "epoch 008/010, batch 002/002, step 0107/0496: loss=1.6577869653701782\n",
            "epoch 008/010, batch 002/002, step 0108/0496: loss=2.113238573074341\n",
            "epoch 008/010, batch 002/002, step 0109/0496: loss=2.1579649448394775\n",
            "epoch 008/010, batch 002/002, step 0110/0496: loss=1.7088961601257324\n",
            "epoch 008/010, batch 002/002, step 0111/0496: loss=2.1652565002441406\n",
            "epoch 008/010, batch 002/002, step 0112/0496: loss=2.535900831222534\n",
            "epoch 008/010, batch 002/002, step 0113/0496: loss=1.6570930480957031\n",
            "epoch 008/010, batch 002/002, step 0114/0496: loss=1.192911148071289\n",
            "epoch 008/010, batch 002/002, step 0115/0496: loss=1.914387583732605\n",
            "epoch 008/010, batch 002/002, step 0116/0496: loss=1.5063669681549072\n",
            "epoch 008/010, batch 002/002, step 0117/0496: loss=1.9120579957962036\n",
            "epoch 008/010, batch 002/002, step 0118/0496: loss=2.2489936351776123\n",
            "epoch 008/010, batch 002/002, step 0119/0496: loss=2.1026482582092285\n",
            "epoch 008/010, batch 002/002, step 0120/0496: loss=1.842146873474121\n",
            "epoch 008/010, batch 002/002, step 0121/0496: loss=1.5613901615142822\n",
            "epoch 008/010, batch 002/002, step 0122/0496: loss=1.6313893795013428\n",
            "epoch 008/010, batch 002/002, step 0123/0496: loss=1.6529765129089355\n",
            "epoch 008/010, batch 002/002, step 0124/0496: loss=1.5858678817749023\n",
            "epoch 008/010, batch 002/002, step 0125/0496: loss=1.6754599809646606\n",
            "epoch 008/010, batch 002/002, step 0126/0496: loss=1.8756790161132812\n",
            "epoch 008/010, batch 002/002, step 0127/0496: loss=3.2205731868743896\n",
            "epoch 008/010, batch 002/002, step 0128/0496: loss=1.6147209405899048\n",
            "epoch 008/010, batch 002/002, step 0129/0496: loss=2.180220127105713\n",
            "epoch 008/010, batch 002/002, step 0130/0496: loss=1.8163946866989136\n",
            "epoch 008/010, batch 002/002, step 0131/0496: loss=2.2535948753356934\n",
            "epoch 008/010, batch 002/002, step 0132/0496: loss=2.208209276199341\n",
            "epoch 008/010, batch 002/002, step 0133/0496: loss=2.032505989074707\n",
            "epoch 008/010, batch 002/002, step 0134/0496: loss=1.386570930480957\n",
            "epoch 008/010, batch 002/002, step 0135/0496: loss=1.7637269496917725\n",
            "epoch 008/010, batch 002/002, step 0136/0496: loss=3.195751667022705\n",
            "epoch 008/010, batch 002/002, step 0137/0496: loss=1.5649064779281616\n",
            "epoch 008/010, batch 002/002, step 0138/0496: loss=1.6365997791290283\n",
            "epoch 008/010, batch 002/002, step 0139/0496: loss=2.0415823459625244\n",
            "epoch 008/010, batch 002/002, step 0140/0496: loss=2.1830310821533203\n",
            "epoch 008/010, batch 002/002, step 0141/0496: loss=1.786590576171875\n",
            "epoch 008/010, batch 002/002, step 0142/0496: loss=1.8407504558563232\n",
            "epoch 008/010, batch 002/002, step 0143/0496: loss=1.5737543106079102\n",
            "epoch 008/010, batch 002/002, step 0144/0496: loss=3.270399570465088\n",
            "epoch 008/010, batch 002/002, step 0145/0496: loss=1.915661096572876\n",
            "epoch 008/010, batch 002/002, step 0146/0496: loss=2.419417381286621\n",
            "epoch 008/010, batch 002/002, step 0147/0496: loss=1.3227832317352295\n",
            "epoch 008/010, batch 002/002, step 0148/0496: loss=2.5694797039031982\n",
            "epoch 008/010, batch 002/002, step 0149/0496: loss=1.9805397987365723\n",
            "epoch 008/010, batch 002/002, step 0150/0496: loss=1.816477656364441\n",
            "epoch 008/010, batch 002/002, step 0151/0496: loss=1.7849717140197754\n",
            "epoch 008/010, batch 002/002, step 0152/0496: loss=2.290160655975342\n",
            "epoch 008/010, batch 002/002, step 0153/0496: loss=1.7955721616744995\n",
            "epoch 008/010, batch 002/002, step 0154/0496: loss=1.9167566299438477\n",
            "epoch 008/010, batch 002/002, step 0155/0496: loss=3.3713483810424805\n",
            "epoch 008/010, batch 002/002, step 0156/0496: loss=1.645928144454956\n",
            "epoch 008/010, batch 002/002, step 0157/0496: loss=1.8151142597198486\n",
            "epoch 008/010, batch 002/002, step 0158/0496: loss=1.5626695156097412\n",
            "epoch 008/010, batch 002/002, step 0159/0496: loss=2.2544562816619873\n",
            "epoch 008/010, batch 002/002, step 0160/0496: loss=1.5537781715393066\n",
            "epoch 008/010, batch 002/002, step 0161/0496: loss=1.5190727710723877\n",
            "epoch 008/010, batch 002/002, step 0162/0496: loss=1.5872502326965332\n",
            "epoch 008/010, batch 002/002, step 0163/0496: loss=1.5833091735839844\n",
            "epoch 008/010, batch 002/002, step 0164/0496: loss=2.807454824447632\n",
            "epoch 008/010, batch 002/002, step 0165/0496: loss=1.8022818565368652\n",
            "epoch 008/010, batch 002/002, step 0166/0496: loss=1.4086203575134277\n",
            "epoch 008/010, batch 002/002, step 0167/0496: loss=2.046001434326172\n",
            "epoch 008/010, batch 002/002, step 0168/0496: loss=1.5481345653533936\n",
            "epoch 008/010, batch 002/002, step 0169/0496: loss=1.710755467414856\n",
            "epoch 008/010, batch 002/002, step 0170/0496: loss=1.6016607284545898\n",
            "epoch 008/010, batch 002/002, step 0171/0496: loss=2.2556183338165283\n",
            "epoch 008/010, batch 002/002, step 0172/0496: loss=1.9696671962738037\n",
            "epoch 008/010, batch 002/002, step 0173/0496: loss=1.8160769939422607\n",
            "epoch 008/010, batch 002/002, step 0174/0496: loss=1.892077088356018\n",
            "epoch 008/010, batch 002/002, step 0175/0496: loss=2.140554904937744\n",
            "epoch 008/010, batch 002/002, step 0176/0496: loss=2.111191749572754\n",
            "epoch 008/010, batch 002/002, step 0177/0496: loss=1.749948501586914\n",
            "epoch 008/010, batch 002/002, step 0178/0496: loss=2.085793972015381\n",
            "epoch 008/010, batch 002/002, step 0179/0496: loss=1.878812551498413\n",
            "epoch 008/010, batch 002/002, step 0180/0496: loss=1.5622938871383667\n",
            "epoch 008/010, batch 002/002, step 0181/0496: loss=1.7615199089050293\n",
            "epoch 008/010, batch 002/002, step 0182/0496: loss=1.8391213417053223\n",
            "epoch 008/010, batch 002/002, step 0183/0496: loss=1.6209213733673096\n",
            "epoch 008/010, batch 002/002, step 0184/0496: loss=1.8261692523956299\n",
            "epoch 008/010, batch 002/002, step 0185/0496: loss=1.6939189434051514\n",
            "epoch 008/010, batch 002/002, step 0186/0496: loss=1.7960327863693237\n",
            "epoch 008/010, batch 002/002, step 0187/0496: loss=2.8450140953063965\n",
            "epoch 008/010, batch 002/002, step 0188/0496: loss=1.8634096384048462\n",
            "epoch 008/010, batch 002/002, step 0189/0496: loss=2.344294548034668\n",
            "epoch 008/010, batch 002/002, step 0190/0496: loss=1.655807375907898\n",
            "epoch 008/010, batch 002/002, step 0191/0496: loss=1.9642661809921265\n",
            "epoch 008/010, batch 002/002, step 0192/0496: loss=1.8300695419311523\n",
            "epoch 008/010, batch 002/002, step 0193/0496: loss=2.051091194152832\n",
            "epoch 008/010, batch 002/002, step 0194/0496: loss=3.410564422607422\n",
            "epoch 008/010, batch 002/002, step 0195/0496: loss=1.9287500381469727\n",
            "epoch 008/010, batch 002/002, step 0196/0496: loss=1.5638694763183594\n",
            "epoch 008/010, batch 002/002, step 0197/0496: loss=1.8159780502319336\n",
            "epoch 008/010, batch 002/002, step 0198/0496: loss=1.5550874471664429\n",
            "epoch 008/010, batch 002/002, step 0199/0496: loss=1.9697823524475098\n",
            "epoch 008/010, batch 002/002, step 0200/0496: loss=2.95011830329895\n",
            "epoch 008/010, batch 002/002, step 0201/0496: loss=1.5262688398361206\n",
            "epoch 008/010, batch 002/002, step 0202/0496: loss=1.7750877141952515\n",
            "epoch 008/010, batch 002/002, step 0203/0496: loss=1.7269370555877686\n",
            "epoch 008/010, batch 002/002, step 0204/0496: loss=1.9228163957595825\n",
            "epoch 008/010, batch 002/002, step 0205/0496: loss=1.71994948387146\n",
            "epoch 008/010, batch 002/002, step 0206/0496: loss=1.6707422733306885\n",
            "epoch 008/010, batch 002/002, step 0207/0496: loss=2.4300031661987305\n",
            "epoch 008/010, batch 002/002, step 0208/0496: loss=2.3708887100219727\n",
            "epoch 008/010, batch 002/002, step 0209/0496: loss=1.9020241498947144\n",
            "epoch 008/010, batch 002/002, step 0210/0496: loss=1.527026891708374\n",
            "epoch 008/010, batch 002/002, step 0211/0496: loss=1.7845478057861328\n",
            "epoch 008/010, batch 002/002, step 0212/0496: loss=1.7954245805740356\n",
            "epoch 008/010, batch 002/002, step 0213/0496: loss=2.252819538116455\n",
            "epoch 008/010, batch 002/002, step 0214/0496: loss=1.6565008163452148\n",
            "epoch 008/010, batch 002/002, step 0215/0496: loss=1.6268960237503052\n",
            "epoch 008/010, batch 002/002, step 0216/0496: loss=1.654961347579956\n",
            "epoch 008/010, batch 002/002, step 0217/0496: loss=2.589677572250366\n",
            "epoch 008/010, batch 002/002, step 0218/0496: loss=2.274604320526123\n",
            "epoch 008/010, batch 002/002, step 0219/0496: loss=1.449636697769165\n",
            "epoch 008/010, batch 002/002, step 0220/0496: loss=1.8446190357208252\n",
            "epoch 008/010, batch 002/002, step 0221/0496: loss=1.6680320501327515\n",
            "epoch 008/010, batch 002/002, step 0222/0496: loss=2.6017446517944336\n",
            "epoch 008/010, batch 002/002, step 0223/0496: loss=1.9309887886047363\n",
            "epoch 008/010, batch 002/002, step 0224/0496: loss=1.900781512260437\n",
            "epoch 008/010, batch 002/002, step 0225/0496: loss=1.876751184463501\n",
            "epoch 008/010, batch 002/002, step 0226/0496: loss=1.9819254875183105\n",
            "epoch 008/010, batch 002/002, step 0227/0496: loss=1.6021296977996826\n",
            "epoch 008/010, batch 002/002, step 0228/0496: loss=2.1307740211486816\n",
            "epoch 008/010, batch 002/002, step 0229/0496: loss=1.944203495979309\n",
            "epoch 008/010, batch 002/002, step 0230/0496: loss=3.2864787578582764\n",
            "epoch 008/010, batch 002/002, step 0231/0496: loss=2.101248025894165\n",
            "epoch 008/010, batch 002/002, step 0232/0496: loss=2.1545586585998535\n",
            "epoch 008/010, batch 002/002, step 0233/0496: loss=3.2514517307281494\n",
            "epoch 008/010, batch 002/002, step 0234/0496: loss=2.1274595260620117\n",
            "epoch 008/010, batch 002/002, step 0235/0496: loss=1.5236083269119263\n",
            "epoch 008/010, batch 002/002, step 0236/0496: loss=1.6864322423934937\n",
            "epoch 008/010, batch 002/002, step 0237/0496: loss=1.7431458234786987\n",
            "epoch 008/010, batch 002/002, step 0238/0496: loss=2.7692360877990723\n",
            "epoch 008/010, batch 002/002, step 0239/0496: loss=1.7960522174835205\n",
            "epoch 008/010, batch 002/002, step 0240/0496: loss=1.9281141757965088\n",
            "epoch 008/010, batch 002/002, step 0241/0496: loss=2.033677816390991\n",
            "epoch 008/010, batch 002/002, step 0242/0496: loss=2.3039655685424805\n",
            "epoch 008/010, batch 002/002, step 0243/0496: loss=1.785488486289978\n",
            "epoch 008/010, batch 002/002, step 0244/0496: loss=2.042910099029541\n",
            "epoch 008/010, batch 002/002, step 0245/0496: loss=2.1521596908569336\n",
            "epoch 008/010, batch 002/002, step 0246/0496: loss=2.066822052001953\n",
            "epoch 008/010, batch 002/002, step 0247/0496: loss=1.9280216693878174\n",
            "epoch 008/010, batch 002/002, step 0248/0496: loss=1.867170810699463\n",
            "epoch 008/010, batch 002/002, step 0249/0496: loss=1.8052287101745605\n",
            "epoch 008/010, batch 002/002, step 0250/0496: loss=4.06345796585083\n",
            "epoch 008/010, batch 002/002, step 0251/0496: loss=1.788830280303955\n",
            "epoch 008/010, batch 002/002, step 0252/0496: loss=2.2183749675750732\n",
            "epoch 008/010, batch 002/002, step 0253/0496: loss=2.4671130180358887\n",
            "epoch 008/010, batch 002/002, step 0254/0496: loss=1.8451534509658813\n",
            "epoch 008/010, batch 002/002, step 0255/0496: loss=2.084120512008667\n",
            "epoch 008/010, batch 002/002, step 0256/0496: loss=1.5888216495513916\n",
            "epoch 008/010, batch 002/002, step 0257/0496: loss=2.276134490966797\n",
            "epoch 008/010, batch 002/002, step 0258/0496: loss=1.908638834953308\n",
            "epoch 008/010, batch 002/002, step 0259/0496: loss=2.104789972305298\n",
            "epoch 008/010, batch 002/002, step 0260/0496: loss=1.5631229877471924\n",
            "epoch 008/010, batch 002/002, step 0261/0496: loss=2.137327194213867\n",
            "epoch 008/010, batch 002/002, step 0262/0496: loss=2.2154903411865234\n",
            "epoch 008/010, batch 002/002, step 0263/0496: loss=3.016517162322998\n",
            "epoch 008/010, batch 002/002, step 0264/0496: loss=1.78485906124115\n",
            "epoch 008/010, batch 002/002, step 0265/0496: loss=2.809986114501953\n",
            "epoch 008/010, batch 002/002, step 0266/0496: loss=1.9452918767929077\n",
            "epoch 008/010, batch 002/002, step 0267/0496: loss=2.669482946395874\n",
            "epoch 008/010, batch 002/002, step 0268/0496: loss=1.7855843305587769\n",
            "epoch 008/010, batch 002/002, step 0269/0496: loss=2.046031951904297\n",
            "epoch 008/010, batch 002/002, step 0270/0496: loss=1.4556512832641602\n",
            "epoch 008/010, batch 002/002, step 0271/0496: loss=1.7816030979156494\n",
            "epoch 008/010, batch 002/002, step 0272/0496: loss=1.8435618877410889\n",
            "epoch 008/010, batch 002/002, step 0273/0496: loss=1.5656566619873047\n",
            "epoch 008/010, batch 002/002, step 0274/0496: loss=2.1372480392456055\n",
            "epoch 008/010, batch 002/002, step 0275/0496: loss=1.623396873474121\n",
            "epoch 008/010, batch 002/002, step 0276/0496: loss=1.3445684909820557\n",
            "epoch 008/010, batch 002/002, step 0277/0496: loss=1.615440011024475\n",
            "epoch 008/010, batch 002/002, step 0278/0496: loss=1.7146353721618652\n",
            "epoch 008/010, batch 002/002, step 0279/0496: loss=1.9592385292053223\n",
            "epoch 008/010, batch 002/002, step 0280/0496: loss=1.8609428405761719\n",
            "epoch 008/010, batch 002/002, step 0281/0496: loss=1.359252691268921\n",
            "epoch 008/010, batch 002/002, step 0282/0496: loss=1.9599586725234985\n",
            "epoch 008/010, batch 002/002, step 0283/0496: loss=1.8458868265151978\n",
            "epoch 008/010, batch 002/002, step 0284/0496: loss=2.4454641342163086\n",
            "epoch 008/010, batch 002/002, step 0285/0496: loss=1.5363293886184692\n",
            "epoch 008/010, batch 002/002, step 0286/0496: loss=1.3670384883880615\n",
            "epoch 008/010, batch 002/002, step 0287/0496: loss=2.8510282039642334\n",
            "epoch 008/010, batch 002/002, step 0288/0496: loss=1.4163576364517212\n",
            "epoch 008/010, batch 002/002, step 0289/0496: loss=1.6803902387619019\n",
            "epoch 008/010, batch 002/002, step 0290/0496: loss=1.8033145666122437\n",
            "epoch 008/010, batch 002/002, step 0291/0496: loss=1.5314960479736328\n",
            "epoch 008/010, batch 002/002, step 0292/0496: loss=1.8608351945877075\n",
            "epoch 008/010, batch 002/002, step 0293/0496: loss=2.70723819732666\n",
            "epoch 008/010, batch 002/002, step 0294/0496: loss=1.9930835962295532\n",
            "epoch 008/010, batch 002/002, step 0295/0496: loss=1.8075859546661377\n",
            "epoch 008/010, batch 002/002, step 0296/0496: loss=2.024580955505371\n",
            "epoch 008/010, batch 002/002, step 0297/0496: loss=1.7939069271087646\n",
            "epoch 008/010, batch 002/002, step 0298/0496: loss=1.4764553308486938\n",
            "epoch 008/010, batch 002/002, step 0299/0496: loss=1.6640539169311523\n",
            "epoch 008/010, batch 002/002, step 0300/0496: loss=2.2355921268463135\n",
            "epoch 008/010, batch 002/002, step 0301/0496: loss=1.7246689796447754\n",
            "epoch 008/010, batch 002/002, step 0302/0496: loss=2.8793606758117676\n",
            "epoch 008/010, batch 002/002, step 0303/0496: loss=1.7041606903076172\n",
            "epoch 008/010, batch 002/002, step 0304/0496: loss=1.854373812675476\n",
            "epoch 008/010, batch 002/002, step 0305/0496: loss=2.2365145683288574\n",
            "epoch 008/010, batch 002/002, step 0306/0496: loss=1.9708963632583618\n",
            "epoch 008/010, batch 002/002, step 0307/0496: loss=1.8120884895324707\n",
            "epoch 008/010, batch 002/002, step 0308/0496: loss=1.8162338733673096\n",
            "epoch 008/010, batch 002/002, step 0309/0496: loss=2.0027918815612793\n",
            "epoch 008/010, batch 002/002, step 0310/0496: loss=1.604088544845581\n",
            "epoch 008/010, batch 002/002, step 0311/0496: loss=1.6889817714691162\n",
            "epoch 008/010, batch 002/002, step 0312/0496: loss=1.6132457256317139\n",
            "epoch 008/010, batch 002/002, step 0313/0496: loss=1.2185732126235962\n",
            "epoch 008/010, batch 002/002, step 0314/0496: loss=1.3358778953552246\n",
            "epoch 008/010, batch 002/002, step 0315/0496: loss=2.0600972175598145\n",
            "epoch 008/010, batch 002/002, step 0316/0496: loss=1.5457899570465088\n",
            "epoch 008/010, batch 002/002, step 0317/0496: loss=2.4841606616973877\n",
            "epoch 008/010, batch 002/002, step 0318/0496: loss=1.6481001377105713\n",
            "epoch 008/010, batch 002/002, step 0319/0496: loss=1.93056321144104\n",
            "epoch 008/010, batch 002/002, step 0320/0496: loss=1.821760654449463\n",
            "epoch 008/010, batch 002/002, step 0321/0496: loss=1.2417645454406738\n",
            "epoch 008/010, batch 002/002, step 0322/0496: loss=1.6925008296966553\n",
            "epoch 008/010, batch 002/002, step 0323/0496: loss=1.5249385833740234\n",
            "epoch 008/010, batch 002/002, step 0324/0496: loss=1.4516685009002686\n",
            "epoch 008/010, batch 002/002, step 0325/0496: loss=1.9144855737686157\n",
            "epoch 008/010, batch 002/002, step 0326/0496: loss=3.197981834411621\n",
            "epoch 008/010, batch 002/002, step 0327/0496: loss=1.5223830938339233\n",
            "epoch 008/010, batch 002/002, step 0328/0496: loss=1.5620366334915161\n",
            "epoch 008/010, batch 002/002, step 0329/0496: loss=1.3120603561401367\n",
            "epoch 008/010, batch 002/002, step 0330/0496: loss=1.6787974834442139\n",
            "epoch 008/010, batch 002/002, step 0331/0496: loss=1.3946049213409424\n",
            "epoch 008/010, batch 002/002, step 0332/0496: loss=1.431857943534851\n",
            "epoch 008/010, batch 002/002, step 0333/0496: loss=1.7703874111175537\n",
            "epoch 008/010, batch 002/002, step 0334/0496: loss=1.4925847053527832\n",
            "epoch 008/010, batch 002/002, step 0335/0496: loss=1.9563404321670532\n",
            "epoch 008/010, batch 002/002, step 0336/0496: loss=1.4773582220077515\n",
            "epoch 008/010, batch 002/002, step 0337/0496: loss=2.009549140930176\n",
            "epoch 008/010, batch 002/002, step 0338/0496: loss=3.045079231262207\n",
            "epoch 008/010, batch 002/002, step 0339/0496: loss=1.3640350103378296\n",
            "epoch 008/010, batch 002/002, step 0340/0496: loss=1.5368746519088745\n",
            "epoch 008/010, batch 002/002, step 0341/0496: loss=1.9476888179779053\n",
            "epoch 008/010, batch 002/002, step 0342/0496: loss=1.2702605724334717\n",
            "epoch 008/010, batch 002/002, step 0343/0496: loss=2.4000539779663086\n",
            "epoch 008/010, batch 002/002, step 0344/0496: loss=2.531775951385498\n",
            "epoch 008/010, batch 002/002, step 0345/0496: loss=1.600805640220642\n",
            "epoch 008/010, batch 002/002, step 0346/0496: loss=1.4752380847930908\n",
            "epoch 008/010, batch 002/002, step 0347/0496: loss=1.5562033653259277\n",
            "epoch 008/010, batch 002/002, step 0348/0496: loss=1.7545247077941895\n",
            "epoch 008/010, batch 002/002, step 0349/0496: loss=1.1983742713928223\n",
            "epoch 008/010, batch 002/002, step 0350/0496: loss=1.6799551248550415\n",
            "epoch 008/010, batch 002/002, step 0351/0496: loss=1.4975025653839111\n",
            "epoch 008/010, batch 002/002, step 0352/0496: loss=1.807680368423462\n",
            "epoch 008/010, batch 002/002, step 0353/0496: loss=1.9111181497573853\n",
            "epoch 008/010, batch 002/002, step 0354/0496: loss=1.466675043106079\n",
            "epoch 008/010, batch 002/002, step 0355/0496: loss=1.9364746809005737\n",
            "epoch 008/010, batch 002/002, step 0356/0496: loss=2.7726633548736572\n",
            "epoch 008/010, batch 002/002, step 0357/0496: loss=1.9238578081130981\n",
            "epoch 008/010, batch 002/002, step 0358/0496: loss=1.5324171781539917\n",
            "epoch 008/010, batch 002/002, step 0359/0496: loss=1.7090160846710205\n",
            "epoch 008/010, batch 002/002, step 0360/0496: loss=2.0369293689727783\n",
            "epoch 008/010, batch 002/002, step 0361/0496: loss=1.9408187866210938\n",
            "epoch 008/010, batch 002/002, step 0362/0496: loss=1.736189365386963\n",
            "epoch 008/010, batch 002/002, step 0363/0496: loss=1.9783804416656494\n",
            "epoch 008/010, batch 002/002, step 0364/0496: loss=1.8069114685058594\n",
            "epoch 008/010, batch 002/002, step 0365/0496: loss=1.245558738708496\n",
            "epoch 008/010, batch 002/002, step 0366/0496: loss=2.0973620414733887\n",
            "epoch 008/010, batch 002/002, step 0367/0496: loss=1.9810303449630737\n",
            "epoch 008/010, batch 002/002, step 0368/0496: loss=1.6225738525390625\n",
            "epoch 008/010, batch 002/002, step 0369/0496: loss=1.887272834777832\n",
            "epoch 008/010, batch 002/002, step 0370/0496: loss=2.155522346496582\n",
            "epoch 008/010, batch 002/002, step 0371/0496: loss=1.8037688732147217\n",
            "epoch 008/010, batch 002/002, step 0372/0496: loss=1.887510895729065\n",
            "epoch 008/010, batch 002/002, step 0373/0496: loss=1.8619179725646973\n",
            "epoch 008/010, batch 002/002, step 0374/0496: loss=1.5964362621307373\n",
            "epoch 008/010, batch 002/002, step 0375/0496: loss=3.609675407409668\n",
            "epoch 008/010, batch 002/002, step 0376/0496: loss=1.5769799947738647\n",
            "epoch 008/010, batch 002/002, step 0377/0496: loss=1.9640036821365356\n",
            "epoch 008/010, batch 002/002, step 0378/0496: loss=2.7715725898742676\n",
            "epoch 008/010, batch 002/002, step 0379/0496: loss=1.958362102508545\n",
            "epoch 008/010, batch 002/002, step 0380/0496: loss=1.720543384552002\n",
            "epoch 008/010, batch 002/002, step 0381/0496: loss=2.3589794635772705\n",
            "epoch 008/010, batch 002/002, step 0382/0496: loss=1.6488761901855469\n",
            "epoch 008/010, batch 002/002, step 0383/0496: loss=1.5836005210876465\n",
            "epoch 008/010, batch 002/002, step 0384/0496: loss=1.9201468229293823\n",
            "epoch 008/010, batch 002/002, step 0385/0496: loss=1.8399659395217896\n",
            "epoch 008/010, batch 002/002, step 0386/0496: loss=1.875388503074646\n",
            "epoch 008/010, batch 002/002, step 0387/0496: loss=1.5330287218093872\n",
            "epoch 008/010, batch 002/002, step 0388/0496: loss=1.8491497039794922\n",
            "epoch 008/010, batch 002/002, step 0389/0496: loss=2.1569042205810547\n",
            "epoch 008/010, batch 002/002, step 0390/0496: loss=2.3961057662963867\n",
            "epoch 008/010, batch 002/002, step 0391/0496: loss=1.9123759269714355\n",
            "epoch 008/010, batch 002/002, step 0392/0496: loss=1.4587986469268799\n",
            "epoch 008/010, batch 002/002, step 0393/0496: loss=1.5635607242584229\n",
            "epoch 008/010, batch 002/002, step 0394/0496: loss=1.5229675769805908\n",
            "epoch 008/010, batch 002/002, step 0395/0496: loss=1.7037230730056763\n",
            "epoch 008/010, batch 002/002, step 0396/0496: loss=1.366470217704773\n",
            "epoch 008/010, batch 002/002, step 0397/0496: loss=1.8647626638412476\n",
            "epoch 008/010, batch 002/002, step 0398/0496: loss=1.5864996910095215\n",
            "epoch 008/010, batch 002/002, step 0399/0496: loss=2.5386314392089844\n",
            "epoch 008/010, batch 002/002, step 0400/0496: loss=1.8598315715789795\n",
            "epoch 008/010, batch 002/002, step 0401/0496: loss=1.8284385204315186\n",
            "epoch 008/010, batch 002/002, step 0402/0496: loss=1.8759363889694214\n",
            "epoch 008/010, batch 002/002, step 0403/0496: loss=1.9732567071914673\n",
            "epoch 008/010, batch 002/002, step 0404/0496: loss=1.4013761281967163\n",
            "epoch 008/010, batch 002/002, step 0405/0496: loss=1.6379055976867676\n",
            "epoch 008/010, batch 002/002, step 0406/0496: loss=2.047931671142578\n",
            "epoch 008/010, batch 002/002, step 0407/0496: loss=1.3838424682617188\n",
            "epoch 008/010, batch 002/002, step 0408/0496: loss=1.6158132553100586\n",
            "epoch 008/010, batch 002/002, step 0409/0496: loss=1.4752732515335083\n",
            "epoch 008/010, batch 002/002, step 0410/0496: loss=1.6717242002487183\n",
            "epoch 008/010, batch 002/002, step 0411/0496: loss=1.5269546508789062\n",
            "epoch 008/010, batch 002/002, step 0412/0496: loss=1.7031257152557373\n",
            "epoch 008/010, batch 002/002, step 0413/0496: loss=1.7660717964172363\n",
            "epoch 008/010, batch 002/002, step 0414/0496: loss=1.75688636302948\n",
            "epoch 008/010, batch 002/002, step 0415/0496: loss=2.687887191772461\n",
            "epoch 008/010, batch 002/002, step 0416/0496: loss=1.241102695465088\n",
            "epoch 008/010, batch 002/002, step 0417/0496: loss=1.330727458000183\n",
            "epoch 008/010, batch 002/002, step 0418/0496: loss=1.380828619003296\n",
            "epoch 008/010, batch 002/002, step 0419/0496: loss=1.6613218784332275\n",
            "epoch 008/010, batch 002/002, step 0420/0496: loss=1.6893210411071777\n",
            "epoch 008/010, batch 002/002, step 0421/0496: loss=1.6189826726913452\n",
            "epoch 008/010, batch 002/002, step 0422/0496: loss=1.8592381477355957\n",
            "epoch 008/010, batch 002/002, step 0423/0496: loss=1.9179913997650146\n",
            "epoch 008/010, batch 002/002, step 0424/0496: loss=1.5888091325759888\n",
            "epoch 008/010, batch 002/002, step 0425/0496: loss=1.549988031387329\n",
            "epoch 008/010, batch 002/002, step 0426/0496: loss=1.5466413497924805\n",
            "epoch 008/010, batch 002/002, step 0427/0496: loss=1.562608003616333\n",
            "epoch 008/010, batch 002/002, step 0428/0496: loss=1.8813039064407349\n",
            "epoch 008/010, batch 002/002, step 0429/0496: loss=1.5466395616531372\n",
            "epoch 008/010, batch 002/002, step 0430/0496: loss=1.7292143106460571\n",
            "epoch 008/010, batch 002/002, step 0431/0496: loss=1.8363984823226929\n",
            "epoch 008/010, batch 002/002, step 0432/0496: loss=1.9392365217208862\n",
            "epoch 008/010, batch 002/002, step 0433/0496: loss=1.9101226329803467\n",
            "epoch 008/010, batch 002/002, step 0434/0496: loss=2.0668458938598633\n",
            "epoch 008/010, batch 002/002, step 0435/0496: loss=1.3436611890792847\n",
            "epoch 008/010, batch 002/002, step 0436/0496: loss=1.568695068359375\n",
            "epoch 008/010, batch 002/002, step 0437/0496: loss=1.47601318359375\n",
            "epoch 008/010, batch 002/002, step 0438/0496: loss=1.504940152168274\n",
            "epoch 008/010, batch 002/002, step 0439/0496: loss=1.8209891319274902\n",
            "epoch 008/010, batch 002/002, step 0440/0496: loss=1.4677197933197021\n",
            "epoch 008/010, batch 002/002, step 0441/0496: loss=1.671722173690796\n",
            "epoch 008/010, batch 002/002, step 0442/0496: loss=1.4340636730194092\n",
            "epoch 008/010, batch 002/002, step 0443/0496: loss=3.6076080799102783\n",
            "epoch 008/010, batch 002/002, step 0444/0496: loss=1.558903694152832\n",
            "epoch 008/010, batch 002/002, step 0445/0496: loss=2.075713634490967\n",
            "epoch 008/010, batch 002/002, step 0446/0496: loss=1.695204257965088\n",
            "epoch 008/010, batch 002/002, step 0447/0496: loss=1.8299028873443604\n",
            "epoch 008/010, batch 002/002, step 0448/0496: loss=3.3835256099700928\n",
            "epoch 008/010, batch 002/002, step 0449/0496: loss=3.2031922340393066\n",
            "epoch 008/010, batch 002/002, step 0450/0496: loss=2.527552366256714\n",
            "epoch 008/010, batch 002/002, step 0451/0496: loss=1.4572663307189941\n",
            "epoch 008/010, batch 002/002, step 0452/0496: loss=1.9671982526779175\n",
            "epoch 008/010, batch 002/002, step 0453/0496: loss=1.398410439491272\n",
            "epoch 008/010, batch 002/002, step 0454/0496: loss=1.980115294456482\n",
            "epoch 008/010, batch 002/002, step 0455/0496: loss=1.5196303129196167\n",
            "epoch 008/010, batch 002/002, step 0456/0496: loss=1.6995658874511719\n",
            "epoch 008/010, batch 002/002, step 0457/0496: loss=1.7634663581848145\n",
            "epoch 008/010, batch 002/002, step 0458/0496: loss=1.7733131647109985\n",
            "epoch 008/010, batch 002/002, step 0459/0496: loss=3.0073928833007812\n",
            "epoch 008/010, batch 002/002, step 0460/0496: loss=1.7917357683181763\n",
            "epoch 008/010, batch 002/002, step 0461/0496: loss=2.038210868835449\n",
            "epoch 008/010, batch 002/002, step 0462/0496: loss=1.7424888610839844\n",
            "epoch 008/010, batch 002/002, step 0463/0496: loss=1.9314463138580322\n",
            "epoch 008/010, batch 002/002, step 0464/0496: loss=1.7249937057495117\n",
            "epoch 008/010, batch 002/002, step 0465/0496: loss=1.6722325086593628\n",
            "epoch 008/010, batch 002/002, step 0466/0496: loss=3.0258493423461914\n",
            "epoch 008/010, batch 002/002, step 0467/0496: loss=1.447251796722412\n",
            "epoch 008/010, batch 002/002, step 0468/0496: loss=2.112592935562134\n",
            "epoch 008/010, batch 002/002, step 0469/0496: loss=1.439894676208496\n",
            "epoch 008/010, batch 002/002, step 0470/0496: loss=2.31770396232605\n",
            "epoch 008/010, batch 002/002, step 0471/0496: loss=1.558011531829834\n",
            "epoch 008/010, batch 002/002, step 0472/0496: loss=1.733033537864685\n",
            "epoch 008/010, batch 002/002, step 0473/0496: loss=1.4886926412582397\n",
            "epoch 008/010, batch 002/002, step 0474/0496: loss=1.898669958114624\n",
            "epoch 008/010, batch 002/002, step 0475/0496: loss=2.0859503746032715\n",
            "epoch 008/010, batch 002/002, step 0476/0496: loss=2.0897433757781982\n",
            "epoch 008/010, batch 002/002, step 0477/0496: loss=1.7676093578338623\n",
            "epoch 008/010, batch 002/002, step 0478/0496: loss=1.3683955669403076\n",
            "epoch 008/010, batch 002/002, step 0479/0496: loss=2.0685365200042725\n",
            "epoch 008/010, batch 002/002, step 0480/0496: loss=1.5834853649139404\n",
            "epoch 008/010, batch 002/002, step 0481/0496: loss=1.8549748659133911\n",
            "epoch 008/010, batch 002/002, step 0482/0496: loss=1.6844079494476318\n",
            "epoch 008/010, batch 002/002, step 0483/0496: loss=1.6280009746551514\n",
            "epoch 008/010, batch 002/002, step 0484/0496: loss=1.8363239765167236\n",
            "epoch 008/010, batch 002/002, step 0485/0496: loss=1.635409951210022\n",
            "epoch 008/010, batch 002/002, step 0486/0496: loss=1.7019160985946655\n",
            "epoch 008/010, batch 002/002, step 0487/0496: loss=1.4642064571380615\n",
            "epoch 008/010, batch 002/002, step 0488/0496: loss=1.9892475605010986\n",
            "epoch 008/010, batch 002/002, step 0489/0496: loss=2.0136609077453613\n",
            "epoch 008/010, batch 002/002, step 0490/0496: loss=1.1937296390533447\n",
            "epoch 008/010, batch 002/002, step 0491/0496: loss=1.3761388063430786\n",
            "epoch 008/010, batch 002/002, step 0492/0496: loss=1.7249644994735718\n",
            "epoch 008/010, batch 002/002, step 0493/0496: loss=1.7335723638534546\n",
            "epoch 008/010, batch 002/002, step 0494/0496: loss=1.6076500415802002\n",
            "epoch 008/010, batch 002/002, step 0495/0496: loss=2.0060811042785645\n",
            "epoch 008/010, batch 002/002, step 0496/0496: loss=1.6293723583221436\n",
            "epoch 009/010, batch 001/002, step 0001/0496: loss=1.6227530241012573\n",
            "epoch 009/010, batch 001/002, step 0002/0496: loss=1.7587864398956299\n",
            "epoch 009/010, batch 001/002, step 0003/0496: loss=2.254132032394409\n",
            "epoch 009/010, batch 001/002, step 0004/0496: loss=1.899600625038147\n",
            "epoch 009/010, batch 001/002, step 0005/0496: loss=1.777729868888855\n",
            "epoch 009/010, batch 001/002, step 0006/0496: loss=1.8713282346725464\n",
            "epoch 009/010, batch 001/002, step 0007/0496: loss=1.816305160522461\n",
            "epoch 009/010, batch 001/002, step 0008/0496: loss=1.8558111190795898\n",
            "epoch 009/010, batch 001/002, step 0009/0496: loss=2.18172287940979\n",
            "epoch 009/010, batch 001/002, step 0010/0496: loss=2.2372865676879883\n",
            "epoch 009/010, batch 001/002, step 0011/0496: loss=1.8976526260375977\n",
            "epoch 009/010, batch 001/002, step 0012/0496: loss=1.8567627668380737\n",
            "epoch 009/010, batch 001/002, step 0013/0496: loss=2.2647953033447266\n",
            "epoch 009/010, batch 001/002, step 0014/0496: loss=2.946279287338257\n",
            "epoch 009/010, batch 001/002, step 0015/0496: loss=2.607226610183716\n",
            "epoch 009/010, batch 001/002, step 0016/0496: loss=2.4882583618164062\n",
            "epoch 009/010, batch 001/002, step 0017/0496: loss=1.6328550577163696\n",
            "epoch 009/010, batch 001/002, step 0018/0496: loss=1.56241774559021\n",
            "epoch 009/010, batch 001/002, step 0019/0496: loss=1.478981614112854\n",
            "epoch 009/010, batch 001/002, step 0020/0496: loss=2.1200504302978516\n",
            "epoch 009/010, batch 001/002, step 0021/0496: loss=2.0689029693603516\n",
            "epoch 009/010, batch 001/002, step 0022/0496: loss=1.8142712116241455\n",
            "epoch 009/010, batch 001/002, step 0023/0496: loss=2.1302762031555176\n",
            "epoch 009/010, batch 001/002, step 0024/0496: loss=2.1567177772521973\n",
            "epoch 009/010, batch 001/002, step 0025/0496: loss=1.5185686349868774\n",
            "epoch 009/010, batch 001/002, step 0026/0496: loss=1.95774507522583\n",
            "epoch 009/010, batch 001/002, step 0027/0496: loss=1.8611116409301758\n",
            "epoch 009/010, batch 001/002, step 0028/0496: loss=1.9716485738754272\n",
            "epoch 009/010, batch 001/002, step 0029/0496: loss=2.2427549362182617\n",
            "epoch 009/010, batch 001/002, step 0030/0496: loss=1.6743113994598389\n",
            "epoch 009/010, batch 001/002, step 0031/0496: loss=1.9524245262145996\n",
            "epoch 009/010, batch 001/002, step 0032/0496: loss=2.0213494300842285\n",
            "epoch 009/010, batch 001/002, step 0033/0496: loss=1.4500696659088135\n",
            "epoch 009/010, batch 001/002, step 0034/0496: loss=1.7707985639572144\n",
            "epoch 009/010, batch 001/002, step 0035/0496: loss=2.8623456954956055\n",
            "epoch 009/010, batch 001/002, step 0036/0496: loss=1.7026386260986328\n",
            "epoch 009/010, batch 001/002, step 0037/0496: loss=1.7476353645324707\n",
            "epoch 009/010, batch 001/002, step 0038/0496: loss=1.8732247352600098\n",
            "epoch 009/010, batch 001/002, step 0039/0496: loss=2.189383029937744\n",
            "epoch 009/010, batch 001/002, step 0040/0496: loss=1.7429099082946777\n",
            "epoch 009/010, batch 001/002, step 0041/0496: loss=1.5509922504425049\n",
            "epoch 009/010, batch 001/002, step 0042/0496: loss=1.632069706916809\n",
            "epoch 009/010, batch 001/002, step 0043/0496: loss=1.5109825134277344\n",
            "epoch 009/010, batch 001/002, step 0044/0496: loss=1.2884845733642578\n",
            "epoch 009/010, batch 001/002, step 0045/0496: loss=1.684207797050476\n",
            "epoch 009/010, batch 001/002, step 0046/0496: loss=1.5780799388885498\n",
            "epoch 009/010, batch 001/002, step 0047/0496: loss=2.052938461303711\n",
            "epoch 009/010, batch 001/002, step 0048/0496: loss=1.542090892791748\n",
            "epoch 009/010, batch 001/002, step 0049/0496: loss=1.601147174835205\n",
            "epoch 009/010, batch 001/002, step 0050/0496: loss=1.4054203033447266\n",
            "epoch 009/010, batch 001/002, step 0051/0496: loss=1.8697283267974854\n",
            "epoch 009/010, batch 001/002, step 0052/0496: loss=1.6907784938812256\n",
            "epoch 009/010, batch 001/002, step 0053/0496: loss=1.7408630847930908\n",
            "epoch 009/010, batch 001/002, step 0054/0496: loss=2.676807403564453\n",
            "epoch 009/010, batch 001/002, step 0055/0496: loss=1.8527333736419678\n",
            "epoch 009/010, batch 001/002, step 0056/0496: loss=1.8763676881790161\n",
            "epoch 009/010, batch 001/002, step 0057/0496: loss=1.5880767107009888\n",
            "epoch 009/010, batch 001/002, step 0058/0496: loss=1.6732133626937866\n",
            "epoch 009/010, batch 001/002, step 0059/0496: loss=1.6078511476516724\n",
            "epoch 009/010, batch 001/002, step 0060/0496: loss=1.4549040794372559\n",
            "epoch 009/010, batch 001/002, step 0061/0496: loss=1.7542839050292969\n",
            "epoch 009/010, batch 001/002, step 0062/0496: loss=2.9451704025268555\n",
            "epoch 009/010, batch 001/002, step 0063/0496: loss=2.3371834754943848\n",
            "epoch 009/010, batch 001/002, step 0064/0496: loss=2.771500587463379\n",
            "epoch 009/010, batch 001/002, step 0065/0496: loss=1.9962491989135742\n",
            "epoch 009/010, batch 001/002, step 0066/0496: loss=1.5571635961532593\n",
            "epoch 009/010, batch 001/002, step 0067/0496: loss=1.6512565612792969\n",
            "epoch 009/010, batch 001/002, step 0068/0496: loss=2.2161874771118164\n",
            "epoch 009/010, batch 001/002, step 0069/0496: loss=1.8941271305084229\n",
            "epoch 009/010, batch 001/002, step 0070/0496: loss=1.8067467212677002\n",
            "epoch 009/010, batch 001/002, step 0071/0496: loss=1.881282925605774\n",
            "epoch 009/010, batch 001/002, step 0072/0496: loss=2.031888484954834\n",
            "epoch 009/010, batch 001/002, step 0073/0496: loss=1.8756849765777588\n",
            "epoch 009/010, batch 001/002, step 0074/0496: loss=2.331289768218994\n",
            "epoch 009/010, batch 001/002, step 0075/0496: loss=2.107178211212158\n",
            "epoch 009/010, batch 001/002, step 0076/0496: loss=2.3880257606506348\n",
            "epoch 009/010, batch 001/002, step 0077/0496: loss=1.812363624572754\n",
            "epoch 009/010, batch 001/002, step 0078/0496: loss=1.8989720344543457\n",
            "epoch 009/010, batch 001/002, step 0079/0496: loss=2.6410417556762695\n",
            "epoch 009/010, batch 001/002, step 0080/0496: loss=1.7051148414611816\n",
            "epoch 009/010, batch 001/002, step 0081/0496: loss=2.36179780960083\n",
            "epoch 009/010, batch 001/002, step 0082/0496: loss=1.6707594394683838\n",
            "epoch 009/010, batch 001/002, step 0083/0496: loss=1.6719212532043457\n",
            "epoch 009/010, batch 001/002, step 0084/0496: loss=3.3144969940185547\n",
            "epoch 009/010, batch 001/002, step 0085/0496: loss=2.253868579864502\n",
            "epoch 009/010, batch 001/002, step 0086/0496: loss=1.3688740730285645\n",
            "epoch 009/010, batch 001/002, step 0087/0496: loss=2.0254416465759277\n",
            "epoch 009/010, batch 001/002, step 0088/0496: loss=2.946040630340576\n",
            "epoch 009/010, batch 001/002, step 0089/0496: loss=3.105856418609619\n",
            "epoch 009/010, batch 001/002, step 0090/0496: loss=1.9099245071411133\n",
            "epoch 009/010, batch 001/002, step 0091/0496: loss=2.986114501953125\n",
            "epoch 009/010, batch 001/002, step 0092/0496: loss=2.478508472442627\n",
            "epoch 009/010, batch 001/002, step 0093/0496: loss=2.1833996772766113\n",
            "epoch 009/010, batch 001/002, step 0094/0496: loss=2.218750476837158\n",
            "epoch 009/010, batch 001/002, step 0095/0496: loss=2.1096127033233643\n",
            "epoch 009/010, batch 001/002, step 0096/0496: loss=2.339329481124878\n",
            "epoch 009/010, batch 001/002, step 0097/0496: loss=1.6645448207855225\n",
            "epoch 009/010, batch 001/002, step 0098/0496: loss=2.297823667526245\n",
            "epoch 009/010, batch 001/002, step 0099/0496: loss=1.5316154956817627\n",
            "epoch 009/010, batch 001/002, step 0100/0496: loss=2.540971279144287\n",
            "epoch 009/010, batch 001/002, step 0101/0496: loss=1.7942352294921875\n",
            "epoch 009/010, batch 001/002, step 0102/0496: loss=1.7759220600128174\n",
            "epoch 009/010, batch 001/002, step 0103/0496: loss=1.931910753250122\n",
            "epoch 009/010, batch 001/002, step 0104/0496: loss=1.592158317565918\n",
            "epoch 009/010, batch 001/002, step 0105/0496: loss=2.026110887527466\n",
            "epoch 009/010, batch 001/002, step 0106/0496: loss=1.481395959854126\n",
            "epoch 009/010, batch 001/002, step 0107/0496: loss=2.312513589859009\n",
            "epoch 009/010, batch 001/002, step 0108/0496: loss=1.6745707988739014\n",
            "epoch 009/010, batch 001/002, step 0109/0496: loss=1.6938997507095337\n",
            "epoch 009/010, batch 001/002, step 0110/0496: loss=2.0436477661132812\n",
            "epoch 009/010, batch 001/002, step 0111/0496: loss=1.6027779579162598\n",
            "epoch 009/010, batch 001/002, step 0112/0496: loss=1.8536919355392456\n",
            "epoch 009/010, batch 001/002, step 0113/0496: loss=1.7042487859725952\n",
            "epoch 009/010, batch 001/002, step 0114/0496: loss=1.4632046222686768\n",
            "epoch 009/010, batch 001/002, step 0115/0496: loss=1.9955646991729736\n",
            "epoch 009/010, batch 001/002, step 0116/0496: loss=1.2856628894805908\n",
            "epoch 009/010, batch 001/002, step 0117/0496: loss=1.5295116901397705\n",
            "epoch 009/010, batch 001/002, step 0118/0496: loss=1.5822374820709229\n",
            "epoch 009/010, batch 001/002, step 0119/0496: loss=1.5761252641677856\n",
            "epoch 009/010, batch 001/002, step 0120/0496: loss=1.2618510723114014\n",
            "epoch 009/010, batch 001/002, step 0121/0496: loss=1.9581849575042725\n",
            "epoch 009/010, batch 001/002, step 0122/0496: loss=1.5681982040405273\n",
            "epoch 009/010, batch 001/002, step 0123/0496: loss=1.6247156858444214\n",
            "epoch 009/010, batch 001/002, step 0124/0496: loss=2.286090850830078\n",
            "epoch 009/010, batch 001/002, step 0125/0496: loss=1.8137004375457764\n",
            "epoch 009/010, batch 001/002, step 0126/0496: loss=3.4356606006622314\n",
            "epoch 009/010, batch 001/002, step 0127/0496: loss=2.8507819175720215\n",
            "epoch 009/010, batch 001/002, step 0128/0496: loss=1.5179563760757446\n",
            "epoch 009/010, batch 001/002, step 0129/0496: loss=1.961057424545288\n",
            "epoch 009/010, batch 001/002, step 0130/0496: loss=3.023550510406494\n",
            "epoch 009/010, batch 001/002, step 0131/0496: loss=1.6789014339447021\n",
            "epoch 009/010, batch 001/002, step 0132/0496: loss=2.173168659210205\n",
            "epoch 009/010, batch 001/002, step 0133/0496: loss=2.605431079864502\n",
            "epoch 009/010, batch 001/002, step 0134/0496: loss=1.1924139261245728\n",
            "epoch 009/010, batch 001/002, step 0135/0496: loss=1.3191773891448975\n",
            "epoch 009/010, batch 001/002, step 0136/0496: loss=1.659363865852356\n",
            "epoch 009/010, batch 001/002, step 0137/0496: loss=2.0625574588775635\n",
            "epoch 009/010, batch 001/002, step 0138/0496: loss=1.58421790599823\n",
            "epoch 009/010, batch 001/002, step 0139/0496: loss=1.6013822555541992\n",
            "epoch 009/010, batch 001/002, step 0140/0496: loss=1.7776246070861816\n",
            "epoch 009/010, batch 001/002, step 0141/0496: loss=1.8599369525909424\n",
            "epoch 009/010, batch 001/002, step 0142/0496: loss=1.550356149673462\n",
            "epoch 009/010, batch 001/002, step 0143/0496: loss=1.6218311786651611\n",
            "epoch 009/010, batch 001/002, step 0144/0496: loss=1.2489675283432007\n",
            "epoch 009/010, batch 001/002, step 0145/0496: loss=1.5887742042541504\n",
            "epoch 009/010, batch 001/002, step 0146/0496: loss=1.9167988300323486\n",
            "epoch 009/010, batch 001/002, step 0147/0496: loss=1.733351469039917\n",
            "epoch 009/010, batch 001/002, step 0148/0496: loss=1.8558576107025146\n",
            "epoch 009/010, batch 001/002, step 0149/0496: loss=1.9496904611587524\n",
            "epoch 009/010, batch 001/002, step 0150/0496: loss=1.9984618425369263\n",
            "epoch 009/010, batch 001/002, step 0151/0496: loss=2.1486308574676514\n",
            "epoch 009/010, batch 001/002, step 0152/0496: loss=1.8939404487609863\n",
            "epoch 009/010, batch 001/002, step 0153/0496: loss=3.6937344074249268\n",
            "epoch 009/010, batch 001/002, step 0154/0496: loss=1.7130173444747925\n",
            "epoch 009/010, batch 001/002, step 0155/0496: loss=1.665873408317566\n",
            "epoch 009/010, batch 001/002, step 0156/0496: loss=2.044064998626709\n",
            "epoch 009/010, batch 001/002, step 0157/0496: loss=1.562349557876587\n",
            "epoch 009/010, batch 001/002, step 0158/0496: loss=1.803774356842041\n",
            "epoch 009/010, batch 001/002, step 0159/0496: loss=1.381299376487732\n",
            "epoch 009/010, batch 001/002, step 0160/0496: loss=1.782618522644043\n",
            "epoch 009/010, batch 001/002, step 0161/0496: loss=1.5481524467468262\n",
            "epoch 009/010, batch 001/002, step 0162/0496: loss=1.4005823135375977\n",
            "epoch 009/010, batch 001/002, step 0163/0496: loss=1.7019343376159668\n",
            "epoch 009/010, batch 001/002, step 0164/0496: loss=1.6593351364135742\n",
            "epoch 009/010, batch 001/002, step 0165/0496: loss=1.8442203998565674\n",
            "epoch 009/010, batch 001/002, step 0166/0496: loss=1.210981011390686\n",
            "epoch 009/010, batch 001/002, step 0167/0496: loss=1.6883065700531006\n",
            "epoch 009/010, batch 001/002, step 0168/0496: loss=1.6266989707946777\n",
            "epoch 009/010, batch 001/002, step 0169/0496: loss=1.413730263710022\n",
            "epoch 009/010, batch 001/002, step 0170/0496: loss=1.6591254472732544\n",
            "epoch 009/010, batch 001/002, step 0171/0496: loss=1.789647102355957\n",
            "epoch 009/010, batch 001/002, step 0172/0496: loss=1.9596493244171143\n",
            "epoch 009/010, batch 001/002, step 0173/0496: loss=1.328502893447876\n",
            "epoch 009/010, batch 001/002, step 0174/0496: loss=1.577836036682129\n",
            "epoch 009/010, batch 001/002, step 0175/0496: loss=1.84575355052948\n",
            "epoch 009/010, batch 001/002, step 0176/0496: loss=2.0838828086853027\n",
            "epoch 009/010, batch 001/002, step 0177/0496: loss=1.9258253574371338\n",
            "epoch 009/010, batch 001/002, step 0178/0496: loss=1.4172285795211792\n",
            "epoch 009/010, batch 001/002, step 0179/0496: loss=2.2499451637268066\n",
            "epoch 009/010, batch 001/002, step 0180/0496: loss=2.220520257949829\n",
            "epoch 009/010, batch 001/002, step 0181/0496: loss=2.0255367755889893\n",
            "epoch 009/010, batch 001/002, step 0182/0496: loss=1.7569527626037598\n",
            "epoch 009/010, batch 001/002, step 0183/0496: loss=2.3897833824157715\n",
            "epoch 009/010, batch 001/002, step 0184/0496: loss=1.5528569221496582\n",
            "epoch 009/010, batch 001/002, step 0185/0496: loss=1.974672555923462\n",
            "epoch 009/010, batch 001/002, step 0186/0496: loss=2.2246615886688232\n",
            "epoch 009/010, batch 001/002, step 0187/0496: loss=3.357623815536499\n",
            "epoch 009/010, batch 001/002, step 0188/0496: loss=1.7831547260284424\n",
            "epoch 009/010, batch 001/002, step 0189/0496: loss=3.326409101486206\n",
            "epoch 009/010, batch 001/002, step 0190/0496: loss=1.4423171281814575\n",
            "epoch 009/010, batch 001/002, step 0191/0496: loss=3.6907598972320557\n",
            "epoch 009/010, batch 001/002, step 0192/0496: loss=2.2770864963531494\n",
            "epoch 009/010, batch 001/002, step 0193/0496: loss=2.7819957733154297\n",
            "epoch 009/010, batch 001/002, step 0194/0496: loss=2.0231127738952637\n",
            "epoch 009/010, batch 001/002, step 0195/0496: loss=2.2841873168945312\n",
            "epoch 009/010, batch 001/002, step 0196/0496: loss=3.188549041748047\n",
            "epoch 009/010, batch 001/002, step 0197/0496: loss=2.5851616859436035\n",
            "epoch 009/010, batch 001/002, step 0198/0496: loss=2.673823118209839\n",
            "epoch 009/010, batch 001/002, step 0199/0496: loss=1.669872522354126\n",
            "epoch 009/010, batch 001/002, step 0200/0496: loss=2.1734633445739746\n",
            "epoch 009/010, batch 001/002, step 0201/0496: loss=1.771172046661377\n",
            "epoch 009/010, batch 001/002, step 0202/0496: loss=2.3779385089874268\n",
            "epoch 009/010, batch 001/002, step 0203/0496: loss=1.837492823600769\n",
            "epoch 009/010, batch 001/002, step 0204/0496: loss=1.9753005504608154\n",
            "epoch 009/010, batch 001/002, step 0205/0496: loss=1.272006869316101\n",
            "epoch 009/010, batch 001/002, step 0206/0496: loss=1.962662935256958\n",
            "epoch 009/010, batch 001/002, step 0207/0496: loss=1.9088596105575562\n",
            "epoch 009/010, batch 001/002, step 0208/0496: loss=1.9418865442276\n",
            "epoch 009/010, batch 001/002, step 0209/0496: loss=1.8027920722961426\n",
            "epoch 009/010, batch 001/002, step 0210/0496: loss=1.7981945276260376\n",
            "epoch 009/010, batch 001/002, step 0211/0496: loss=1.3341625928878784\n",
            "epoch 009/010, batch 001/002, step 0212/0496: loss=1.7583553791046143\n",
            "epoch 009/010, batch 001/002, step 0213/0496: loss=1.9041597843170166\n",
            "epoch 009/010, batch 001/002, step 0214/0496: loss=2.116405487060547\n",
            "epoch 009/010, batch 001/002, step 0215/0496: loss=1.9605852365493774\n",
            "epoch 009/010, batch 001/002, step 0216/0496: loss=1.455979347229004\n",
            "epoch 009/010, batch 001/002, step 0217/0496: loss=1.3390519618988037\n",
            "epoch 009/010, batch 001/002, step 0218/0496: loss=1.8663866519927979\n",
            "epoch 009/010, batch 001/002, step 0219/0496: loss=1.2088234424591064\n",
            "epoch 009/010, batch 001/002, step 0220/0496: loss=1.4557644128799438\n",
            "epoch 009/010, batch 001/002, step 0221/0496: loss=1.102529525756836\n",
            "epoch 009/010, batch 001/002, step 0222/0496: loss=1.88279128074646\n",
            "epoch 009/010, batch 001/002, step 0223/0496: loss=1.736616611480713\n",
            "epoch 009/010, batch 001/002, step 0224/0496: loss=1.651352882385254\n",
            "epoch 009/010, batch 001/002, step 0225/0496: loss=2.1930534839630127\n",
            "epoch 009/010, batch 001/002, step 0226/0496: loss=3.576024293899536\n",
            "epoch 009/010, batch 001/002, step 0227/0496: loss=2.8852665424346924\n",
            "epoch 009/010, batch 001/002, step 0228/0496: loss=2.3159873485565186\n",
            "epoch 009/010, batch 001/002, step 0229/0496: loss=2.4394562244415283\n",
            "epoch 009/010, batch 001/002, step 0230/0496: loss=1.7653368711471558\n",
            "epoch 009/010, batch 001/002, step 0231/0496: loss=1.6779735088348389\n",
            "epoch 009/010, batch 001/002, step 0232/0496: loss=2.8057432174682617\n",
            "epoch 009/010, batch 001/002, step 0233/0496: loss=1.657662272453308\n",
            "epoch 009/010, batch 001/002, step 0234/0496: loss=2.314666271209717\n",
            "epoch 009/010, batch 001/002, step 0235/0496: loss=1.89602530002594\n",
            "epoch 009/010, batch 001/002, step 0236/0496: loss=2.764634132385254\n",
            "epoch 009/010, batch 001/002, step 0237/0496: loss=1.9155068397521973\n",
            "epoch 009/010, batch 001/002, step 0238/0496: loss=2.216439723968506\n",
            "epoch 009/010, batch 001/002, step 0239/0496: loss=1.2099865674972534\n",
            "epoch 009/010, batch 001/002, step 0240/0496: loss=2.2745747566223145\n",
            "epoch 009/010, batch 001/002, step 0241/0496: loss=1.470314383506775\n",
            "epoch 009/010, batch 001/002, step 0242/0496: loss=1.7150230407714844\n",
            "epoch 009/010, batch 001/002, step 0243/0496: loss=1.327539324760437\n",
            "epoch 009/010, batch 001/002, step 0244/0496: loss=2.10675311088562\n",
            "epoch 009/010, batch 001/002, step 0245/0496: loss=1.7673717737197876\n",
            "epoch 009/010, batch 001/002, step 0246/0496: loss=2.5098671913146973\n",
            "epoch 009/010, batch 001/002, step 0247/0496: loss=1.739100456237793\n",
            "epoch 009/010, batch 001/002, step 0248/0496: loss=1.7071157693862915\n",
            "epoch 009/010, batch 001/002, step 0249/0496: loss=1.927054762840271\n",
            "epoch 009/010, batch 001/002, step 0250/0496: loss=2.180264472961426\n",
            "epoch 009/010, batch 001/002, step 0251/0496: loss=1.7043437957763672\n",
            "epoch 009/010, batch 001/002, step 0252/0496: loss=2.11810040473938\n",
            "epoch 009/010, batch 001/002, step 0253/0496: loss=1.7604575157165527\n",
            "epoch 009/010, batch 001/002, step 0254/0496: loss=1.9295024871826172\n",
            "epoch 009/010, batch 001/002, step 0255/0496: loss=1.1404234170913696\n",
            "epoch 009/010, batch 001/002, step 0256/0496: loss=1.5256116390228271\n",
            "epoch 009/010, batch 001/002, step 0257/0496: loss=1.4976637363433838\n",
            "epoch 009/010, batch 001/002, step 0258/0496: loss=1.9973701238632202\n",
            "epoch 009/010, batch 001/002, step 0259/0496: loss=1.8117146492004395\n",
            "epoch 009/010, batch 001/002, step 0260/0496: loss=1.8369390964508057\n",
            "epoch 009/010, batch 001/002, step 0261/0496: loss=1.5252692699432373\n",
            "epoch 009/010, batch 001/002, step 0262/0496: loss=1.420744776725769\n",
            "epoch 009/010, batch 001/002, step 0263/0496: loss=1.4486757516860962\n",
            "epoch 009/010, batch 001/002, step 0264/0496: loss=1.4701844453811646\n",
            "epoch 009/010, batch 001/002, step 0265/0496: loss=1.8776066303253174\n",
            "epoch 009/010, batch 001/002, step 0266/0496: loss=2.2723464965820312\n",
            "epoch 009/010, batch 001/002, step 0267/0496: loss=2.3458917140960693\n",
            "epoch 009/010, batch 001/002, step 0268/0496: loss=1.588628888130188\n",
            "epoch 009/010, batch 001/002, step 0269/0496: loss=1.8226420879364014\n",
            "epoch 009/010, batch 001/002, step 0270/0496: loss=1.8855042457580566\n",
            "epoch 009/010, batch 001/002, step 0271/0496: loss=1.7104734182357788\n",
            "epoch 009/010, batch 001/002, step 0272/0496: loss=1.7163443565368652\n",
            "epoch 009/010, batch 001/002, step 0273/0496: loss=1.4459439516067505\n",
            "epoch 009/010, batch 001/002, step 0274/0496: loss=1.419908046722412\n",
            "epoch 009/010, batch 001/002, step 0275/0496: loss=1.8986973762512207\n",
            "epoch 009/010, batch 001/002, step 0276/0496: loss=1.790884017944336\n",
            "epoch 009/010, batch 001/002, step 0277/0496: loss=1.9456453323364258\n",
            "epoch 009/010, batch 001/002, step 0278/0496: loss=1.9331817626953125\n",
            "epoch 009/010, batch 001/002, step 0279/0496: loss=1.4267897605895996\n",
            "epoch 009/010, batch 001/002, step 0280/0496: loss=1.644951581954956\n",
            "epoch 009/010, batch 001/002, step 0281/0496: loss=1.6491971015930176\n",
            "epoch 009/010, batch 001/002, step 0282/0496: loss=1.7335832118988037\n",
            "epoch 009/010, batch 001/002, step 0283/0496: loss=1.2004730701446533\n",
            "epoch 009/010, batch 001/002, step 0284/0496: loss=2.000016212463379\n",
            "epoch 009/010, batch 001/002, step 0285/0496: loss=1.7493363618850708\n",
            "epoch 009/010, batch 001/002, step 0286/0496: loss=1.348474383354187\n",
            "epoch 009/010, batch 001/002, step 0287/0496: loss=2.0330467224121094\n",
            "epoch 009/010, batch 001/002, step 0288/0496: loss=1.4650261402130127\n",
            "epoch 009/010, batch 001/002, step 0289/0496: loss=1.7517691850662231\n",
            "epoch 009/010, batch 001/002, step 0290/0496: loss=1.2241542339324951\n",
            "epoch 009/010, batch 001/002, step 0291/0496: loss=1.7454229593276978\n",
            "epoch 009/010, batch 001/002, step 0292/0496: loss=1.628486156463623\n",
            "epoch 009/010, batch 001/002, step 0293/0496: loss=1.655431866645813\n",
            "epoch 009/010, batch 001/002, step 0294/0496: loss=1.9001837968826294\n",
            "epoch 009/010, batch 001/002, step 0295/0496: loss=1.2071983814239502\n",
            "epoch 009/010, batch 001/002, step 0296/0496: loss=1.9722237586975098\n",
            "epoch 009/010, batch 001/002, step 0297/0496: loss=1.9681715965270996\n",
            "epoch 009/010, batch 001/002, step 0298/0496: loss=1.2363018989562988\n",
            "epoch 009/010, batch 001/002, step 0299/0496: loss=1.5592619180679321\n",
            "epoch 009/010, batch 001/002, step 0300/0496: loss=1.703463077545166\n",
            "epoch 009/010, batch 001/002, step 0301/0496: loss=1.9405653476715088\n",
            "epoch 009/010, batch 001/002, step 0302/0496: loss=1.876732349395752\n",
            "epoch 009/010, batch 001/002, step 0303/0496: loss=1.993987798690796\n",
            "epoch 009/010, batch 001/002, step 0304/0496: loss=1.9289592504501343\n",
            "epoch 009/010, batch 001/002, step 0305/0496: loss=1.8327319622039795\n",
            "epoch 009/010, batch 001/002, step 0306/0496: loss=1.8184404373168945\n",
            "epoch 009/010, batch 001/002, step 0307/0496: loss=1.3515928983688354\n",
            "epoch 009/010, batch 001/002, step 0308/0496: loss=1.3770989179611206\n",
            "epoch 009/010, batch 001/002, step 0309/0496: loss=1.9340449571609497\n",
            "epoch 009/010, batch 001/002, step 0310/0496: loss=1.8260328769683838\n",
            "epoch 009/010, batch 001/002, step 0311/0496: loss=2.0071072578430176\n",
            "epoch 009/010, batch 001/002, step 0312/0496: loss=2.0752956867218018\n",
            "epoch 009/010, batch 001/002, step 0313/0496: loss=1.3686370849609375\n",
            "epoch 009/010, batch 001/002, step 0314/0496: loss=1.6007080078125\n",
            "epoch 009/010, batch 001/002, step 0315/0496: loss=1.815279245376587\n",
            "epoch 009/010, batch 001/002, step 0316/0496: loss=1.921295166015625\n",
            "epoch 009/010, batch 001/002, step 0317/0496: loss=1.5168410539627075\n",
            "epoch 009/010, batch 001/002, step 0318/0496: loss=1.5444507598876953\n",
            "epoch 009/010, batch 001/002, step 0319/0496: loss=1.6752339601516724\n",
            "epoch 009/010, batch 001/002, step 0320/0496: loss=3.1535701751708984\n",
            "epoch 009/010, batch 001/002, step 0321/0496: loss=1.6427001953125\n",
            "epoch 009/010, batch 001/002, step 0322/0496: loss=2.230031728744507\n",
            "epoch 009/010, batch 001/002, step 0323/0496: loss=1.6278244256973267\n",
            "epoch 009/010, batch 001/002, step 0324/0496: loss=2.863582134246826\n",
            "epoch 009/010, batch 001/002, step 0325/0496: loss=3.460970401763916\n",
            "epoch 009/010, batch 001/002, step 0326/0496: loss=1.5372395515441895\n",
            "epoch 009/010, batch 001/002, step 0327/0496: loss=3.0234713554382324\n",
            "epoch 009/010, batch 001/002, step 0328/0496: loss=1.7872761487960815\n",
            "epoch 009/010, batch 001/002, step 0329/0496: loss=3.1746139526367188\n",
            "epoch 009/010, batch 001/002, step 0330/0496: loss=1.7012746334075928\n",
            "epoch 009/010, batch 001/002, step 0331/0496: loss=3.639021158218384\n",
            "epoch 009/010, batch 001/002, step 0332/0496: loss=1.7300114631652832\n",
            "epoch 009/010, batch 001/002, step 0333/0496: loss=2.2658865451812744\n",
            "epoch 009/010, batch 001/002, step 0334/0496: loss=1.7115272283554077\n",
            "epoch 009/010, batch 001/002, step 0335/0496: loss=2.5115811824798584\n",
            "epoch 009/010, batch 001/002, step 0336/0496: loss=2.357940673828125\n",
            "epoch 009/010, batch 001/002, step 0337/0496: loss=2.6127662658691406\n",
            "epoch 009/010, batch 001/002, step 0338/0496: loss=4.171994209289551\n",
            "epoch 009/010, batch 001/002, step 0339/0496: loss=1.560739278793335\n",
            "epoch 009/010, batch 001/002, step 0340/0496: loss=7.611618995666504\n",
            "epoch 009/010, batch 001/002, step 0341/0496: loss=7.513604164123535\n",
            "epoch 009/010, batch 001/002, step 0342/0496: loss=3.271756172180176\n",
            "epoch 009/010, batch 001/002, step 0343/0496: loss=7.229236602783203\n",
            "epoch 009/010, batch 001/002, step 0344/0496: loss=2.5262653827667236\n",
            "epoch 009/010, batch 001/002, step 0345/0496: loss=4.511016845703125\n",
            "epoch 009/010, batch 001/002, step 0346/0496: loss=2.9084525108337402\n",
            "epoch 009/010, batch 001/002, step 0347/0496: loss=2.6891250610351562\n",
            "epoch 009/010, batch 001/002, step 0348/0496: loss=6.784099578857422\n",
            "epoch 009/010, batch 001/002, step 0349/0496: loss=4.769591808319092\n",
            "epoch 009/010, batch 001/002, step 0350/0496: loss=3.5407309532165527\n",
            "epoch 009/010, batch 001/002, step 0351/0496: loss=2.1771903038024902\n",
            "epoch 009/010, batch 001/002, step 0352/0496: loss=2.6713132858276367\n",
            "epoch 009/010, batch 001/002, step 0353/0496: loss=4.39134407043457\n",
            "epoch 009/010, batch 001/002, step 0354/0496: loss=3.6018638610839844\n",
            "epoch 009/010, batch 001/002, step 0355/0496: loss=4.276671409606934\n",
            "epoch 009/010, batch 001/002, step 0356/0496: loss=4.309928894042969\n",
            "epoch 009/010, batch 001/002, step 0357/0496: loss=2.805879592895508\n",
            "epoch 009/010, batch 001/002, step 0358/0496: loss=2.436006546020508\n",
            "epoch 009/010, batch 001/002, step 0359/0496: loss=2.9808011054992676\n",
            "epoch 009/010, batch 001/002, step 0360/0496: loss=3.466547966003418\n",
            "epoch 009/010, batch 001/002, step 0361/0496: loss=3.2549734115600586\n",
            "epoch 009/010, batch 001/002, step 0362/0496: loss=2.7734856605529785\n",
            "epoch 009/010, batch 001/002, step 0363/0496: loss=7.363452911376953\n",
            "epoch 009/010, batch 001/002, step 0364/0496: loss=3.9532060623168945\n",
            "epoch 009/010, batch 001/002, step 0365/0496: loss=5.242176532745361\n",
            "epoch 009/010, batch 001/002, step 0366/0496: loss=3.4838552474975586\n",
            "epoch 009/010, batch 001/002, step 0367/0496: loss=5.137413024902344\n",
            "epoch 009/010, batch 001/002, step 0368/0496: loss=4.211112976074219\n",
            "epoch 009/010, batch 001/002, step 0369/0496: loss=3.0423460006713867\n",
            "epoch 009/010, batch 001/002, step 0370/0496: loss=2.6005043983459473\n",
            "epoch 009/010, batch 001/002, step 0371/0496: loss=2.954289436340332\n",
            "epoch 009/010, batch 001/002, step 0372/0496: loss=2.6852874755859375\n",
            "epoch 009/010, batch 001/002, step 0373/0496: loss=4.373355865478516\n",
            "epoch 009/010, batch 001/002, step 0374/0496: loss=2.0488576889038086\n",
            "epoch 009/010, batch 001/002, step 0375/0496: loss=1.8667078018188477\n",
            "epoch 009/010, batch 001/002, step 0376/0496: loss=2.172398805618286\n",
            "epoch 009/010, batch 001/002, step 0377/0496: loss=2.3316802978515625\n",
            "epoch 009/010, batch 001/002, step 0378/0496: loss=2.266957998275757\n",
            "epoch 009/010, batch 001/002, step 0379/0496: loss=1.6481791734695435\n",
            "epoch 009/010, batch 001/002, step 0380/0496: loss=1.632054090499878\n",
            "epoch 009/010, batch 001/002, step 0381/0496: loss=1.5958857536315918\n",
            "epoch 009/010, batch 001/002, step 0382/0496: loss=1.8013465404510498\n",
            "epoch 009/010, batch 001/002, step 0383/0496: loss=1.7798913717269897\n",
            "epoch 009/010, batch 001/002, step 0384/0496: loss=1.8972009420394897\n",
            "epoch 009/010, batch 001/002, step 0385/0496: loss=2.692302703857422\n",
            "epoch 009/010, batch 001/002, step 0386/0496: loss=2.1560254096984863\n",
            "epoch 009/010, batch 001/002, step 0387/0496: loss=1.8003543615341187\n",
            "epoch 009/010, batch 001/002, step 0388/0496: loss=2.3004310131073\n",
            "epoch 009/010, batch 001/002, step 0389/0496: loss=2.292957305908203\n",
            "epoch 009/010, batch 001/002, step 0390/0496: loss=3.522852659225464\n",
            "epoch 009/010, batch 001/002, step 0391/0496: loss=1.7617228031158447\n",
            "epoch 009/010, batch 001/002, step 0392/0496: loss=3.010298728942871\n",
            "epoch 009/010, batch 001/002, step 0393/0496: loss=1.6257905960083008\n",
            "epoch 009/010, batch 001/002, step 0394/0496: loss=2.6393537521362305\n",
            "epoch 009/010, batch 001/002, step 0395/0496: loss=2.357161521911621\n",
            "epoch 009/010, batch 001/002, step 0396/0496: loss=1.7912565469741821\n",
            "epoch 009/010, batch 001/002, step 0397/0496: loss=2.2207820415496826\n",
            "epoch 009/010, batch 001/002, step 0398/0496: loss=3.0984561443328857\n",
            "epoch 009/010, batch 001/002, step 0399/0496: loss=2.5359044075012207\n",
            "epoch 009/010, batch 001/002, step 0400/0496: loss=1.7586162090301514\n",
            "epoch 009/010, batch 001/002, step 0401/0496: loss=2.1565756797790527\n",
            "epoch 009/010, batch 001/002, step 0402/0496: loss=1.8173913955688477\n",
            "epoch 009/010, batch 001/002, step 0403/0496: loss=1.968078851699829\n",
            "epoch 009/010, batch 001/002, step 0404/0496: loss=2.051668405532837\n",
            "epoch 009/010, batch 001/002, step 0405/0496: loss=2.239781618118286\n",
            "epoch 009/010, batch 001/002, step 0406/0496: loss=1.434046745300293\n",
            "epoch 009/010, batch 001/002, step 0407/0496: loss=2.086453914642334\n",
            "epoch 009/010, batch 001/002, step 0408/0496: loss=1.403265357017517\n",
            "epoch 009/010, batch 001/002, step 0409/0496: loss=1.9380136728286743\n",
            "epoch 009/010, batch 001/002, step 0410/0496: loss=1.8984785079956055\n",
            "epoch 009/010, batch 001/002, step 0411/0496: loss=1.6346025466918945\n",
            "epoch 009/010, batch 001/002, step 0412/0496: loss=1.959522008895874\n",
            "epoch 009/010, batch 001/002, step 0413/0496: loss=1.584952473640442\n",
            "epoch 009/010, batch 001/002, step 0414/0496: loss=1.666248083114624\n",
            "epoch 009/010, batch 001/002, step 0415/0496: loss=1.2739052772521973\n",
            "epoch 009/010, batch 001/002, step 0416/0496: loss=1.4152607917785645\n",
            "epoch 009/010, batch 001/002, step 0417/0496: loss=1.6056416034698486\n",
            "epoch 009/010, batch 001/002, step 0418/0496: loss=2.0564520359039307\n",
            "epoch 009/010, batch 001/002, step 0419/0496: loss=1.4147039651870728\n",
            "epoch 009/010, batch 001/002, step 0420/0496: loss=1.7361173629760742\n",
            "epoch 009/010, batch 001/002, step 0421/0496: loss=1.2063922882080078\n",
            "epoch 009/010, batch 001/002, step 0422/0496: loss=1.2116186618804932\n",
            "epoch 009/010, batch 001/002, step 0423/0496: loss=1.680320143699646\n",
            "epoch 009/010, batch 001/002, step 0424/0496: loss=1.4926867485046387\n",
            "epoch 009/010, batch 001/002, step 0425/0496: loss=1.3491783142089844\n",
            "epoch 009/010, batch 001/002, step 0426/0496: loss=1.2867530584335327\n",
            "epoch 009/010, batch 001/002, step 0427/0496: loss=1.7034518718719482\n",
            "epoch 009/010, batch 001/002, step 0428/0496: loss=1.4483445882797241\n",
            "epoch 009/010, batch 001/002, step 0429/0496: loss=1.6868829727172852\n",
            "epoch 009/010, batch 001/002, step 0430/0496: loss=1.311154842376709\n",
            "epoch 009/010, batch 001/002, step 0431/0496: loss=1.4975128173828125\n",
            "epoch 009/010, batch 001/002, step 0432/0496: loss=1.13614022731781\n",
            "epoch 009/010, batch 001/002, step 0433/0496: loss=2.2179579734802246\n",
            "epoch 009/010, batch 001/002, step 0434/0496: loss=1.7312407493591309\n",
            "epoch 009/010, batch 001/002, step 0435/0496: loss=1.5535097122192383\n",
            "epoch 009/010, batch 001/002, step 0436/0496: loss=1.5228899717330933\n",
            "epoch 009/010, batch 001/002, step 0437/0496: loss=1.5753583908081055\n",
            "epoch 009/010, batch 001/002, step 0438/0496: loss=1.5642454624176025\n",
            "epoch 009/010, batch 001/002, step 0439/0496: loss=1.520129919052124\n",
            "epoch 009/010, batch 001/002, step 0440/0496: loss=1.2886300086975098\n",
            "epoch 009/010, batch 001/002, step 0441/0496: loss=1.7212655544281006\n",
            "epoch 009/010, batch 001/002, step 0442/0496: loss=1.6870574951171875\n",
            "epoch 009/010, batch 001/002, step 0443/0496: loss=1.8189274072647095\n",
            "epoch 009/010, batch 001/002, step 0444/0496: loss=1.980649709701538\n",
            "epoch 009/010, batch 001/002, step 0445/0496: loss=1.8371102809906006\n",
            "epoch 009/010, batch 001/002, step 0446/0496: loss=1.5069162845611572\n",
            "epoch 009/010, batch 001/002, step 0447/0496: loss=1.3427199125289917\n",
            "epoch 009/010, batch 001/002, step 0448/0496: loss=1.5026733875274658\n",
            "epoch 009/010, batch 001/002, step 0449/0496: loss=1.6146116256713867\n",
            "epoch 009/010, batch 001/002, step 0450/0496: loss=2.9934189319610596\n",
            "epoch 009/010, batch 001/002, step 0451/0496: loss=1.7106213569641113\n",
            "epoch 009/010, batch 001/002, step 0452/0496: loss=1.629523515701294\n",
            "epoch 009/010, batch 001/002, step 0453/0496: loss=1.7896461486816406\n",
            "epoch 009/010, batch 001/002, step 0454/0496: loss=1.0432298183441162\n",
            "epoch 009/010, batch 001/002, step 0455/0496: loss=1.5975432395935059\n",
            "epoch 009/010, batch 001/002, step 0456/0496: loss=1.4916224479675293\n",
            "epoch 009/010, batch 001/002, step 0457/0496: loss=1.278849482536316\n",
            "epoch 009/010, batch 001/002, step 0458/0496: loss=1.797589898109436\n",
            "epoch 009/010, batch 001/002, step 0459/0496: loss=1.8595997095108032\n",
            "epoch 009/010, batch 001/002, step 0460/0496: loss=1.4152133464813232\n",
            "epoch 009/010, batch 001/002, step 0461/0496: loss=1.2103943824768066\n",
            "epoch 009/010, batch 001/002, step 0462/0496: loss=1.2044717073440552\n",
            "epoch 009/010, batch 001/002, step 0463/0496: loss=1.239469289779663\n",
            "epoch 009/010, batch 001/002, step 0464/0496: loss=1.84331476688385\n",
            "epoch 009/010, batch 001/002, step 0465/0496: loss=1.6542924642562866\n",
            "epoch 009/010, batch 001/002, step 0466/0496: loss=1.976304292678833\n",
            "epoch 009/010, batch 001/002, step 0467/0496: loss=1.8971762657165527\n",
            "epoch 009/010, batch 001/002, step 0468/0496: loss=1.4403669834136963\n",
            "epoch 009/010, batch 001/002, step 0469/0496: loss=1.5150572061538696\n",
            "epoch 009/010, batch 001/002, step 0470/0496: loss=4.680688858032227\n",
            "epoch 009/010, batch 001/002, step 0471/0496: loss=2.9260849952697754\n",
            "epoch 009/010, batch 001/002, step 0472/0496: loss=1.3726673126220703\n",
            "epoch 009/010, batch 001/002, step 0473/0496: loss=1.375725269317627\n",
            "epoch 009/010, batch 001/002, step 0474/0496: loss=1.1818602085113525\n",
            "epoch 009/010, batch 001/002, step 0475/0496: loss=1.7981417179107666\n",
            "epoch 009/010, batch 001/002, step 0476/0496: loss=1.3812494277954102\n",
            "epoch 009/010, batch 001/002, step 0477/0496: loss=1.578167200088501\n",
            "epoch 009/010, batch 001/002, step 0478/0496: loss=1.6928694248199463\n",
            "epoch 009/010, batch 001/002, step 0479/0496: loss=1.5853948593139648\n",
            "epoch 009/010, batch 001/002, step 0480/0496: loss=1.4210717678070068\n",
            "epoch 009/010, batch 001/002, step 0481/0496: loss=1.6345974206924438\n",
            "epoch 009/010, batch 001/002, step 0482/0496: loss=1.8351798057556152\n",
            "epoch 009/010, batch 001/002, step 0483/0496: loss=1.6883056163787842\n",
            "epoch 009/010, batch 001/002, step 0484/0496: loss=1.3071987628936768\n",
            "epoch 009/010, batch 001/002, step 0485/0496: loss=1.4078309535980225\n",
            "epoch 009/010, batch 001/002, step 0486/0496: loss=1.7955920696258545\n",
            "epoch 009/010, batch 001/002, step 0487/0496: loss=1.5152390003204346\n",
            "epoch 009/010, batch 001/002, step 0488/0496: loss=1.669803261756897\n",
            "epoch 009/010, batch 001/002, step 0489/0496: loss=1.3363068103790283\n",
            "epoch 009/010, batch 001/002, step 0490/0496: loss=1.6596921682357788\n",
            "epoch 009/010, batch 001/002, step 0491/0496: loss=1.4533002376556396\n",
            "epoch 009/010, batch 001/002, step 0492/0496: loss=1.7602607011795044\n",
            "epoch 009/010, batch 001/002, step 0493/0496: loss=1.5255104303359985\n",
            "epoch 009/010, batch 001/002, step 0494/0496: loss=1.8380134105682373\n",
            "epoch 009/010, batch 001/002, step 0495/0496: loss=1.475402593612671\n",
            "epoch 009/010, batch 001/002, step 0496/0496: loss=1.2248291969299316\n",
            "epoch 009/010, batch 002/002, step 0001/0496: loss=1.7098838090896606\n",
            "epoch 009/010, batch 002/002, step 0002/0496: loss=1.9980082511901855\n",
            "epoch 009/010, batch 002/002, step 0003/0496: loss=1.7492165565490723\n",
            "epoch 009/010, batch 002/002, step 0004/0496: loss=1.3646742105484009\n",
            "epoch 009/010, batch 002/002, step 0005/0496: loss=1.8130321502685547\n",
            "epoch 009/010, batch 002/002, step 0006/0496: loss=1.8456991910934448\n",
            "epoch 009/010, batch 002/002, step 0007/0496: loss=2.044201135635376\n",
            "epoch 009/010, batch 002/002, step 0008/0496: loss=1.9912141561508179\n",
            "epoch 009/010, batch 002/002, step 0009/0496: loss=1.7370572090148926\n",
            "epoch 009/010, batch 002/002, step 0010/0496: loss=2.5602941513061523\n",
            "epoch 009/010, batch 002/002, step 0011/0496: loss=1.5875601768493652\n",
            "epoch 009/010, batch 002/002, step 0012/0496: loss=1.3008804321289062\n",
            "epoch 009/010, batch 002/002, step 0013/0496: loss=1.6155509948730469\n",
            "epoch 009/010, batch 002/002, step 0014/0496: loss=1.615964412689209\n",
            "epoch 009/010, batch 002/002, step 0015/0496: loss=1.506667137145996\n",
            "epoch 009/010, batch 002/002, step 0016/0496: loss=1.9804763793945312\n",
            "epoch 009/010, batch 002/002, step 0017/0496: loss=1.848539113998413\n",
            "epoch 009/010, batch 002/002, step 0018/0496: loss=1.4048113822937012\n",
            "epoch 009/010, batch 002/002, step 0019/0496: loss=1.681389331817627\n",
            "epoch 009/010, batch 002/002, step 0020/0496: loss=1.6292983293533325\n",
            "epoch 009/010, batch 002/002, step 0021/0496: loss=1.804358959197998\n",
            "epoch 009/010, batch 002/002, step 0022/0496: loss=1.4293031692504883\n",
            "epoch 009/010, batch 002/002, step 0023/0496: loss=1.6408438682556152\n",
            "epoch 009/010, batch 002/002, step 0024/0496: loss=1.5898592472076416\n",
            "epoch 009/010, batch 002/002, step 0025/0496: loss=1.5175122022628784\n",
            "epoch 009/010, batch 002/002, step 0026/0496: loss=1.2486262321472168\n",
            "epoch 009/010, batch 002/002, step 0027/0496: loss=1.848789930343628\n",
            "epoch 009/010, batch 002/002, step 0028/0496: loss=2.171996831893921\n",
            "epoch 009/010, batch 002/002, step 0029/0496: loss=1.5427656173706055\n",
            "epoch 009/010, batch 002/002, step 0030/0496: loss=1.717981219291687\n",
            "epoch 009/010, batch 002/002, step 0031/0496: loss=1.5734448432922363\n",
            "epoch 009/010, batch 002/002, step 0032/0496: loss=1.5889320373535156\n",
            "epoch 009/010, batch 002/002, step 0033/0496: loss=2.0185649394989014\n",
            "epoch 009/010, batch 002/002, step 0034/0496: loss=1.6300641298294067\n",
            "epoch 009/010, batch 002/002, step 0035/0496: loss=2.0707669258117676\n",
            "epoch 009/010, batch 002/002, step 0036/0496: loss=1.1803165674209595\n",
            "epoch 009/010, batch 002/002, step 0037/0496: loss=1.6219215393066406\n",
            "epoch 009/010, batch 002/002, step 0038/0496: loss=1.4864330291748047\n",
            "epoch 009/010, batch 002/002, step 0039/0496: loss=1.8528282642364502\n",
            "epoch 009/010, batch 002/002, step 0040/0496: loss=1.6757229566574097\n",
            "epoch 009/010, batch 002/002, step 0041/0496: loss=1.9225757122039795\n",
            "epoch 009/010, batch 002/002, step 0042/0496: loss=2.4826250076293945\n",
            "epoch 009/010, batch 002/002, step 0043/0496: loss=1.5532482862472534\n",
            "epoch 009/010, batch 002/002, step 0044/0496: loss=1.6691734790802002\n",
            "epoch 009/010, batch 002/002, step 0045/0496: loss=1.7377028465270996\n",
            "epoch 009/010, batch 002/002, step 0046/0496: loss=2.793858528137207\n",
            "epoch 009/010, batch 002/002, step 0047/0496: loss=1.8307998180389404\n",
            "epoch 009/010, batch 002/002, step 0048/0496: loss=1.5375561714172363\n",
            "epoch 009/010, batch 002/002, step 0049/0496: loss=1.670113205909729\n",
            "epoch 009/010, batch 002/002, step 0050/0496: loss=1.54049813747406\n",
            "epoch 009/010, batch 002/002, step 0051/0496: loss=1.7547868490219116\n",
            "epoch 009/010, batch 002/002, step 0052/0496: loss=1.6942384243011475\n",
            "epoch 009/010, batch 002/002, step 0053/0496: loss=1.6603379249572754\n",
            "epoch 009/010, batch 002/002, step 0054/0496: loss=1.3666002750396729\n",
            "epoch 009/010, batch 002/002, step 0055/0496: loss=1.970890998840332\n",
            "epoch 009/010, batch 002/002, step 0056/0496: loss=1.450676441192627\n",
            "epoch 009/010, batch 002/002, step 0057/0496: loss=2.028210401535034\n",
            "epoch 009/010, batch 002/002, step 0058/0496: loss=2.117465019226074\n",
            "epoch 009/010, batch 002/002, step 0059/0496: loss=1.6677229404449463\n",
            "epoch 009/010, batch 002/002, step 0060/0496: loss=1.699928641319275\n",
            "epoch 009/010, batch 002/002, step 0061/0496: loss=1.518491268157959\n",
            "epoch 009/010, batch 002/002, step 0062/0496: loss=1.5591609477996826\n",
            "epoch 009/010, batch 002/002, step 0063/0496: loss=1.961742639541626\n",
            "epoch 009/010, batch 002/002, step 0064/0496: loss=1.9489517211914062\n",
            "epoch 009/010, batch 002/002, step 0065/0496: loss=3.3361334800720215\n",
            "epoch 009/010, batch 002/002, step 0066/0496: loss=1.4989714622497559\n",
            "epoch 009/010, batch 002/002, step 0067/0496: loss=1.8484411239624023\n",
            "epoch 009/010, batch 002/002, step 0068/0496: loss=1.867923378944397\n",
            "epoch 009/010, batch 002/002, step 0069/0496: loss=1.6665585041046143\n",
            "epoch 009/010, batch 002/002, step 0070/0496: loss=1.8007385730743408\n",
            "epoch 009/010, batch 002/002, step 0071/0496: loss=1.5869511365890503\n",
            "epoch 009/010, batch 002/002, step 0072/0496: loss=1.7170007228851318\n",
            "epoch 009/010, batch 002/002, step 0073/0496: loss=1.6973519325256348\n",
            "epoch 009/010, batch 002/002, step 0074/0496: loss=1.5365160703659058\n",
            "epoch 009/010, batch 002/002, step 0075/0496: loss=1.5545556545257568\n",
            "epoch 009/010, batch 002/002, step 0076/0496: loss=1.7847166061401367\n",
            "epoch 009/010, batch 002/002, step 0077/0496: loss=2.162430763244629\n",
            "epoch 009/010, batch 002/002, step 0078/0496: loss=3.129535675048828\n",
            "epoch 009/010, batch 002/002, step 0079/0496: loss=2.4772446155548096\n",
            "epoch 009/010, batch 002/002, step 0080/0496: loss=1.3874762058258057\n",
            "epoch 009/010, batch 002/002, step 0081/0496: loss=1.5729560852050781\n",
            "epoch 009/010, batch 002/002, step 0082/0496: loss=2.2511487007141113\n",
            "epoch 009/010, batch 002/002, step 0083/0496: loss=1.3134148120880127\n",
            "epoch 009/010, batch 002/002, step 0084/0496: loss=1.3699307441711426\n",
            "epoch 009/010, batch 002/002, step 0085/0496: loss=1.892918586730957\n",
            "epoch 009/010, batch 002/002, step 0086/0496: loss=1.6904264688491821\n",
            "epoch 009/010, batch 002/002, step 0087/0496: loss=1.6027871370315552\n",
            "epoch 009/010, batch 002/002, step 0088/0496: loss=1.4990912675857544\n",
            "epoch 009/010, batch 002/002, step 0089/0496: loss=1.7049890756607056\n",
            "epoch 009/010, batch 002/002, step 0090/0496: loss=1.2342389822006226\n",
            "epoch 009/010, batch 002/002, step 0091/0496: loss=1.4121452569961548\n",
            "epoch 009/010, batch 002/002, step 0092/0496: loss=1.6965312957763672\n",
            "epoch 009/010, batch 002/002, step 0093/0496: loss=1.4525949954986572\n",
            "epoch 009/010, batch 002/002, step 0094/0496: loss=1.4944813251495361\n",
            "epoch 009/010, batch 002/002, step 0095/0496: loss=2.8889636993408203\n",
            "epoch 009/010, batch 002/002, step 0096/0496: loss=1.503514289855957\n",
            "epoch 009/010, batch 002/002, step 0097/0496: loss=1.9033063650131226\n",
            "epoch 009/010, batch 002/002, step 0098/0496: loss=1.7512069940567017\n",
            "epoch 009/010, batch 002/002, step 0099/0496: loss=1.6307246685028076\n",
            "epoch 009/010, batch 002/002, step 0100/0496: loss=1.3862314224243164\n",
            "epoch 009/010, batch 002/002, step 0101/0496: loss=1.5273380279541016\n",
            "epoch 009/010, batch 002/002, step 0102/0496: loss=1.4938976764678955\n",
            "epoch 009/010, batch 002/002, step 0103/0496: loss=1.778590202331543\n",
            "epoch 009/010, batch 002/002, step 0104/0496: loss=1.2686793804168701\n",
            "epoch 009/010, batch 002/002, step 0105/0496: loss=1.5886839628219604\n",
            "epoch 009/010, batch 002/002, step 0106/0496: loss=1.8031113147735596\n",
            "epoch 009/010, batch 002/002, step 0107/0496: loss=1.5876939296722412\n",
            "epoch 009/010, batch 002/002, step 0108/0496: loss=1.6348156929016113\n",
            "epoch 009/010, batch 002/002, step 0109/0496: loss=1.5012242794036865\n",
            "epoch 009/010, batch 002/002, step 0110/0496: loss=1.5620231628417969\n",
            "epoch 009/010, batch 002/002, step 0111/0496: loss=1.2567133903503418\n",
            "epoch 009/010, batch 002/002, step 0112/0496: loss=1.69949471950531\n",
            "epoch 009/010, batch 002/002, step 0113/0496: loss=1.645167350769043\n",
            "epoch 009/010, batch 002/002, step 0114/0496: loss=1.7952576875686646\n",
            "epoch 009/010, batch 002/002, step 0115/0496: loss=1.8536784648895264\n",
            "epoch 009/010, batch 002/002, step 0116/0496: loss=1.5848087072372437\n",
            "epoch 009/010, batch 002/002, step 0117/0496: loss=1.1816463470458984\n",
            "epoch 009/010, batch 002/002, step 0118/0496: loss=2.9032444953918457\n",
            "epoch 009/010, batch 002/002, step 0119/0496: loss=1.6379389762878418\n",
            "epoch 009/010, batch 002/002, step 0120/0496: loss=1.6174671649932861\n",
            "epoch 009/010, batch 002/002, step 0121/0496: loss=1.8939905166625977\n",
            "epoch 009/010, batch 002/002, step 0122/0496: loss=1.7842984199523926\n",
            "epoch 009/010, batch 002/002, step 0123/0496: loss=1.3529714345932007\n",
            "epoch 009/010, batch 002/002, step 0124/0496: loss=1.6790754795074463\n",
            "epoch 009/010, batch 002/002, step 0125/0496: loss=2.431516170501709\n",
            "epoch 009/010, batch 002/002, step 0126/0496: loss=1.4174610376358032\n",
            "epoch 009/010, batch 002/002, step 0127/0496: loss=1.1335629224777222\n",
            "epoch 009/010, batch 002/002, step 0128/0496: loss=1.3325098752975464\n",
            "epoch 009/010, batch 002/002, step 0129/0496: loss=1.7457876205444336\n",
            "epoch 009/010, batch 002/002, step 0130/0496: loss=1.7222118377685547\n",
            "epoch 009/010, batch 002/002, step 0131/0496: loss=1.6294974088668823\n",
            "epoch 009/010, batch 002/002, step 0132/0496: loss=1.863471269607544\n",
            "epoch 009/010, batch 002/002, step 0133/0496: loss=2.4129953384399414\n",
            "epoch 009/010, batch 002/002, step 0134/0496: loss=2.250166416168213\n",
            "epoch 009/010, batch 002/002, step 0135/0496: loss=1.6308696269989014\n",
            "epoch 009/010, batch 002/002, step 0136/0496: loss=1.4352164268493652\n",
            "epoch 009/010, batch 002/002, step 0137/0496: loss=1.9969412088394165\n",
            "epoch 009/010, batch 002/002, step 0138/0496: loss=1.8155128955841064\n",
            "epoch 009/010, batch 002/002, step 0139/0496: loss=1.8735525608062744\n",
            "epoch 009/010, batch 002/002, step 0140/0496: loss=1.6928465366363525\n",
            "epoch 009/010, batch 002/002, step 0141/0496: loss=1.7720136642456055\n",
            "epoch 009/010, batch 002/002, step 0142/0496: loss=1.6764781475067139\n",
            "epoch 009/010, batch 002/002, step 0143/0496: loss=1.7393674850463867\n",
            "epoch 009/010, batch 002/002, step 0144/0496: loss=1.9063068628311157\n",
            "epoch 009/010, batch 002/002, step 0145/0496: loss=1.951244592666626\n",
            "epoch 009/010, batch 002/002, step 0146/0496: loss=1.3670696020126343\n",
            "epoch 009/010, batch 002/002, step 0147/0496: loss=1.8914854526519775\n",
            "epoch 009/010, batch 002/002, step 0148/0496: loss=1.4966082572937012\n",
            "epoch 009/010, batch 002/002, step 0149/0496: loss=1.2581846714019775\n",
            "epoch 009/010, batch 002/002, step 0150/0496: loss=1.9648464918136597\n",
            "epoch 009/010, batch 002/002, step 0151/0496: loss=1.5318975448608398\n",
            "epoch 009/010, batch 002/002, step 0152/0496: loss=1.3744606971740723\n",
            "epoch 009/010, batch 002/002, step 0153/0496: loss=2.5275793075561523\n",
            "epoch 009/010, batch 002/002, step 0154/0496: loss=2.2640271186828613\n",
            "epoch 009/010, batch 002/002, step 0155/0496: loss=1.5835222005844116\n",
            "epoch 009/010, batch 002/002, step 0156/0496: loss=2.362215995788574\n",
            "epoch 009/010, batch 002/002, step 0157/0496: loss=1.7552460432052612\n",
            "epoch 009/010, batch 002/002, step 0158/0496: loss=2.059523582458496\n",
            "epoch 009/010, batch 002/002, step 0159/0496: loss=2.6058900356292725\n",
            "epoch 009/010, batch 002/002, step 0160/0496: loss=1.880884051322937\n",
            "epoch 009/010, batch 002/002, step 0161/0496: loss=1.7500534057617188\n",
            "epoch 009/010, batch 002/002, step 0162/0496: loss=1.252528190612793\n",
            "epoch 009/010, batch 002/002, step 0163/0496: loss=1.6188956499099731\n",
            "epoch 009/010, batch 002/002, step 0164/0496: loss=2.875976800918579\n",
            "epoch 009/010, batch 002/002, step 0165/0496: loss=3.2281670570373535\n",
            "epoch 009/010, batch 002/002, step 0166/0496: loss=1.6744059324264526\n",
            "epoch 009/010, batch 002/002, step 0167/0496: loss=2.922933340072632\n",
            "epoch 009/010, batch 002/002, step 0168/0496: loss=1.5199503898620605\n",
            "epoch 009/010, batch 002/002, step 0169/0496: loss=1.9616566896438599\n",
            "epoch 009/010, batch 002/002, step 0170/0496: loss=1.4001917839050293\n",
            "epoch 009/010, batch 002/002, step 0171/0496: loss=1.940824031829834\n",
            "epoch 009/010, batch 002/002, step 0172/0496: loss=1.5173556804656982\n",
            "epoch 009/010, batch 002/002, step 0173/0496: loss=1.685154914855957\n",
            "epoch 009/010, batch 002/002, step 0174/0496: loss=1.4616063833236694\n",
            "epoch 009/010, batch 002/002, step 0175/0496: loss=1.9153046607971191\n",
            "epoch 009/010, batch 002/002, step 0176/0496: loss=2.637657403945923\n",
            "epoch 009/010, batch 002/002, step 0177/0496: loss=1.4671913385391235\n",
            "epoch 009/010, batch 002/002, step 0178/0496: loss=1.8986914157867432\n",
            "epoch 009/010, batch 002/002, step 0179/0496: loss=2.4134976863861084\n",
            "epoch 009/010, batch 002/002, step 0180/0496: loss=2.6521196365356445\n",
            "epoch 009/010, batch 002/002, step 0181/0496: loss=2.8149256706237793\n",
            "epoch 009/010, batch 002/002, step 0182/0496: loss=2.096057415008545\n",
            "epoch 009/010, batch 002/002, step 0183/0496: loss=3.306394338607788\n",
            "epoch 009/010, batch 002/002, step 0184/0496: loss=2.319098949432373\n",
            "epoch 009/010, batch 002/002, step 0185/0496: loss=5.239257335662842\n",
            "epoch 009/010, batch 002/002, step 0186/0496: loss=2.5977845191955566\n",
            "epoch 009/010, batch 002/002, step 0187/0496: loss=4.630207061767578\n",
            "epoch 009/010, batch 002/002, step 0188/0496: loss=2.386467933654785\n",
            "epoch 009/010, batch 002/002, step 0189/0496: loss=2.9164695739746094\n",
            "epoch 009/010, batch 002/002, step 0190/0496: loss=2.0897562503814697\n",
            "epoch 009/010, batch 002/002, step 0191/0496: loss=2.290961265563965\n",
            "epoch 009/010, batch 002/002, step 0192/0496: loss=2.862612724304199\n",
            "epoch 009/010, batch 002/002, step 0193/0496: loss=2.562676429748535\n",
            "epoch 009/010, batch 002/002, step 0194/0496: loss=1.9894835948944092\n",
            "epoch 009/010, batch 002/002, step 0195/0496: loss=2.0572333335876465\n",
            "epoch 009/010, batch 002/002, step 0196/0496: loss=2.1743178367614746\n",
            "epoch 009/010, batch 002/002, step 0197/0496: loss=2.7400035858154297\n",
            "epoch 009/010, batch 002/002, step 0198/0496: loss=2.1286754608154297\n",
            "epoch 009/010, batch 002/002, step 0199/0496: loss=2.0222508907318115\n",
            "epoch 009/010, batch 002/002, step 0200/0496: loss=2.6418569087982178\n",
            "epoch 009/010, batch 002/002, step 0201/0496: loss=1.691859245300293\n",
            "epoch 009/010, batch 002/002, step 0202/0496: loss=1.8754483461380005\n",
            "epoch 009/010, batch 002/002, step 0203/0496: loss=1.3601274490356445\n",
            "epoch 009/010, batch 002/002, step 0204/0496: loss=2.2319259643554688\n",
            "epoch 009/010, batch 002/002, step 0205/0496: loss=1.696427345275879\n",
            "epoch 009/010, batch 002/002, step 0206/0496: loss=2.0149412155151367\n",
            "epoch 009/010, batch 002/002, step 0207/0496: loss=2.3976540565490723\n",
            "epoch 009/010, batch 002/002, step 0208/0496: loss=1.5740821361541748\n",
            "epoch 009/010, batch 002/002, step 0209/0496: loss=1.5953985452651978\n",
            "epoch 009/010, batch 002/002, step 0210/0496: loss=1.5039101839065552\n",
            "epoch 009/010, batch 002/002, step 0211/0496: loss=2.0800704956054688\n",
            "epoch 009/010, batch 002/002, step 0212/0496: loss=2.313649892807007\n",
            "epoch 009/010, batch 002/002, step 0213/0496: loss=1.541551113128662\n",
            "epoch 009/010, batch 002/002, step 0214/0496: loss=1.5277878046035767\n",
            "epoch 009/010, batch 002/002, step 0215/0496: loss=1.879224181175232\n",
            "epoch 009/010, batch 002/002, step 0216/0496: loss=1.4292449951171875\n",
            "epoch 009/010, batch 002/002, step 0217/0496: loss=1.477316975593567\n",
            "epoch 009/010, batch 002/002, step 0218/0496: loss=1.8233726024627686\n",
            "epoch 009/010, batch 002/002, step 0219/0496: loss=1.5466943979263306\n",
            "epoch 009/010, batch 002/002, step 0220/0496: loss=1.918184518814087\n",
            "epoch 009/010, batch 002/002, step 0221/0496: loss=1.562699556350708\n",
            "epoch 009/010, batch 002/002, step 0222/0496: loss=1.9376938343048096\n",
            "epoch 009/010, batch 002/002, step 0223/0496: loss=1.6541181802749634\n",
            "epoch 009/010, batch 002/002, step 0224/0496: loss=1.2436864376068115\n",
            "epoch 009/010, batch 002/002, step 0225/0496: loss=1.2873477935791016\n",
            "epoch 009/010, batch 002/002, step 0226/0496: loss=1.5386377573013306\n",
            "epoch 009/010, batch 002/002, step 0227/0496: loss=1.6874874830245972\n",
            "epoch 009/010, batch 002/002, step 0228/0496: loss=1.2843122482299805\n",
            "epoch 009/010, batch 002/002, step 0229/0496: loss=2.2439708709716797\n",
            "epoch 009/010, batch 002/002, step 0230/0496: loss=1.5147550106048584\n",
            "epoch 009/010, batch 002/002, step 0231/0496: loss=1.927816390991211\n",
            "epoch 009/010, batch 002/002, step 0232/0496: loss=1.5461959838867188\n",
            "epoch 009/010, batch 002/002, step 0233/0496: loss=1.612428903579712\n",
            "epoch 009/010, batch 002/002, step 0234/0496: loss=1.0899465084075928\n",
            "epoch 009/010, batch 002/002, step 0235/0496: loss=1.9126226902008057\n",
            "epoch 009/010, batch 002/002, step 0236/0496: loss=2.4759066104888916\n",
            "epoch 009/010, batch 002/002, step 0237/0496: loss=1.7570769786834717\n",
            "epoch 009/010, batch 002/002, step 0238/0496: loss=2.5268378257751465\n",
            "epoch 009/010, batch 002/002, step 0239/0496: loss=1.908748984336853\n",
            "epoch 009/010, batch 002/002, step 0240/0496: loss=2.165940761566162\n",
            "epoch 009/010, batch 002/002, step 0241/0496: loss=1.4758994579315186\n",
            "epoch 009/010, batch 002/002, step 0242/0496: loss=1.8893183469772339\n",
            "epoch 009/010, batch 002/002, step 0243/0496: loss=1.673083782196045\n",
            "epoch 009/010, batch 002/002, step 0244/0496: loss=3.4660418033599854\n",
            "epoch 009/010, batch 002/002, step 0245/0496: loss=1.7351051568984985\n",
            "epoch 009/010, batch 002/002, step 0246/0496: loss=2.1658127307891846\n",
            "epoch 009/010, batch 002/002, step 0247/0496: loss=1.5116033554077148\n",
            "epoch 009/010, batch 002/002, step 0248/0496: loss=1.8703324794769287\n",
            "epoch 009/010, batch 002/002, step 0249/0496: loss=1.5300683975219727\n",
            "epoch 009/010, batch 002/002, step 0250/0496: loss=1.7907572984695435\n",
            "epoch 009/010, batch 002/002, step 0251/0496: loss=1.8343099355697632\n",
            "epoch 009/010, batch 002/002, step 0252/0496: loss=1.8319114446640015\n",
            "epoch 009/010, batch 002/002, step 0253/0496: loss=1.788770318031311\n",
            "epoch 009/010, batch 002/002, step 0254/0496: loss=1.7469533681869507\n",
            "epoch 009/010, batch 002/002, step 0255/0496: loss=1.5506778955459595\n",
            "epoch 009/010, batch 002/002, step 0256/0496: loss=1.495654582977295\n",
            "epoch 009/010, batch 002/002, step 0257/0496: loss=1.8286758661270142\n",
            "epoch 009/010, batch 002/002, step 0258/0496: loss=1.7212731838226318\n",
            "epoch 009/010, batch 002/002, step 0259/0496: loss=1.7434852123260498\n",
            "epoch 009/010, batch 002/002, step 0260/0496: loss=1.4826774597167969\n",
            "epoch 009/010, batch 002/002, step 0261/0496: loss=1.87270188331604\n",
            "epoch 009/010, batch 002/002, step 0262/0496: loss=1.397929072380066\n",
            "epoch 009/010, batch 002/002, step 0263/0496: loss=1.708357810974121\n",
            "epoch 009/010, batch 002/002, step 0264/0496: loss=1.4403588771820068\n",
            "epoch 009/010, batch 002/002, step 0265/0496: loss=2.1588168144226074\n",
            "epoch 009/010, batch 002/002, step 0266/0496: loss=1.9471642971038818\n",
            "epoch 009/010, batch 002/002, step 0267/0496: loss=2.159287214279175\n",
            "epoch 009/010, batch 002/002, step 0268/0496: loss=1.8574106693267822\n",
            "epoch 009/010, batch 002/002, step 0269/0496: loss=2.0675156116485596\n",
            "epoch 009/010, batch 002/002, step 0270/0496: loss=2.0258865356445312\n",
            "epoch 009/010, batch 002/002, step 0271/0496: loss=3.8689610958099365\n",
            "epoch 009/010, batch 002/002, step 0272/0496: loss=1.385576844215393\n",
            "epoch 009/010, batch 002/002, step 0273/0496: loss=1.7214868068695068\n",
            "epoch 009/010, batch 002/002, step 0274/0496: loss=1.551961064338684\n",
            "epoch 009/010, batch 002/002, step 0275/0496: loss=1.7304526567459106\n",
            "epoch 009/010, batch 002/002, step 0276/0496: loss=2.038539409637451\n",
            "epoch 009/010, batch 002/002, step 0277/0496: loss=1.5700139999389648\n",
            "epoch 009/010, batch 002/002, step 0278/0496: loss=1.8054684400558472\n",
            "epoch 009/010, batch 002/002, step 0279/0496: loss=1.2489053010940552\n",
            "epoch 009/010, batch 002/002, step 0280/0496: loss=1.3026149272918701\n",
            "epoch 009/010, batch 002/002, step 0281/0496: loss=1.768030047416687\n",
            "epoch 009/010, batch 002/002, step 0282/0496: loss=1.5928845405578613\n",
            "epoch 009/010, batch 002/002, step 0283/0496: loss=1.3672339916229248\n",
            "epoch 009/010, batch 002/002, step 0284/0496: loss=1.4651631116867065\n",
            "epoch 009/010, batch 002/002, step 0285/0496: loss=1.4041318893432617\n",
            "epoch 009/010, batch 002/002, step 0286/0496: loss=1.728308916091919\n",
            "epoch 009/010, batch 002/002, step 0287/0496: loss=1.1704696416854858\n",
            "epoch 009/010, batch 002/002, step 0288/0496: loss=1.655173659324646\n",
            "epoch 009/010, batch 002/002, step 0289/0496: loss=1.177212119102478\n",
            "epoch 009/010, batch 002/002, step 0290/0496: loss=1.5153740644454956\n",
            "epoch 009/010, batch 002/002, step 0291/0496: loss=1.438582181930542\n",
            "epoch 009/010, batch 002/002, step 0292/0496: loss=2.120152473449707\n",
            "epoch 009/010, batch 002/002, step 0293/0496: loss=1.4920575618743896\n",
            "epoch 009/010, batch 002/002, step 0294/0496: loss=1.5434690713882446\n",
            "epoch 009/010, batch 002/002, step 0295/0496: loss=1.4001092910766602\n",
            "epoch 009/010, batch 002/002, step 0296/0496: loss=2.389072895050049\n",
            "epoch 009/010, batch 002/002, step 0297/0496: loss=1.3896799087524414\n",
            "epoch 009/010, batch 002/002, step 0298/0496: loss=1.4372596740722656\n",
            "epoch 009/010, batch 002/002, step 0299/0496: loss=1.5333738327026367\n",
            "epoch 009/010, batch 002/002, step 0300/0496: loss=1.7006756067276\n",
            "epoch 009/010, batch 002/002, step 0301/0496: loss=1.786912441253662\n",
            "epoch 009/010, batch 002/002, step 0302/0496: loss=1.3436863422393799\n",
            "epoch 009/010, batch 002/002, step 0303/0496: loss=1.1689642667770386\n",
            "epoch 009/010, batch 002/002, step 0304/0496: loss=2.2105579376220703\n",
            "epoch 009/010, batch 002/002, step 0305/0496: loss=1.220515489578247\n",
            "epoch 009/010, batch 002/002, step 0306/0496: loss=1.5558485984802246\n",
            "epoch 009/010, batch 002/002, step 0307/0496: loss=1.128470540046692\n",
            "epoch 009/010, batch 002/002, step 0308/0496: loss=1.5395328998565674\n",
            "epoch 009/010, batch 002/002, step 0309/0496: loss=1.3965507745742798\n",
            "epoch 009/010, batch 002/002, step 0310/0496: loss=1.779719591140747\n",
            "epoch 009/010, batch 002/002, step 0311/0496: loss=1.5682202577590942\n",
            "epoch 009/010, batch 002/002, step 0312/0496: loss=1.509089469909668\n",
            "epoch 009/010, batch 002/002, step 0313/0496: loss=1.970594882965088\n",
            "epoch 009/010, batch 002/002, step 0314/0496: loss=1.3312664031982422\n",
            "epoch 009/010, batch 002/002, step 0315/0496: loss=1.4327290058135986\n",
            "epoch 009/010, batch 002/002, step 0316/0496: loss=1.1818432807922363\n",
            "epoch 009/010, batch 002/002, step 0317/0496: loss=0.990550696849823\n",
            "epoch 009/010, batch 002/002, step 0318/0496: loss=1.8505632877349854\n",
            "epoch 009/010, batch 002/002, step 0319/0496: loss=1.213672161102295\n",
            "epoch 009/010, batch 002/002, step 0320/0496: loss=1.6264618635177612\n",
            "epoch 009/010, batch 002/002, step 0321/0496: loss=1.7441762685775757\n",
            "epoch 009/010, batch 002/002, step 0322/0496: loss=2.733537435531616\n",
            "epoch 009/010, batch 002/002, step 0323/0496: loss=1.6056489944458008\n",
            "epoch 009/010, batch 002/002, step 0324/0496: loss=1.463629126548767\n",
            "epoch 009/010, batch 002/002, step 0325/0496: loss=1.1916297674179077\n",
            "epoch 009/010, batch 002/002, step 0326/0496: loss=1.231299638748169\n",
            "epoch 009/010, batch 002/002, step 0327/0496: loss=1.6467869281768799\n",
            "epoch 009/010, batch 002/002, step 0328/0496: loss=1.4831264019012451\n",
            "epoch 009/010, batch 002/002, step 0329/0496: loss=1.097029447555542\n",
            "epoch 009/010, batch 002/002, step 0330/0496: loss=1.4466090202331543\n",
            "epoch 009/010, batch 002/002, step 0331/0496: loss=1.339243769645691\n",
            "epoch 009/010, batch 002/002, step 0332/0496: loss=1.4221341609954834\n",
            "epoch 009/010, batch 002/002, step 0333/0496: loss=1.3596882820129395\n",
            "epoch 009/010, batch 002/002, step 0334/0496: loss=1.6310112476348877\n",
            "epoch 009/010, batch 002/002, step 0335/0496: loss=1.42844557762146\n",
            "epoch 009/010, batch 002/002, step 0336/0496: loss=1.3884291648864746\n",
            "epoch 009/010, batch 002/002, step 0337/0496: loss=1.1863677501678467\n",
            "epoch 009/010, batch 002/002, step 0338/0496: loss=1.6688214540481567\n",
            "epoch 009/010, batch 002/002, step 0339/0496: loss=1.9130789041519165\n",
            "epoch 009/010, batch 002/002, step 0340/0496: loss=1.9816882610321045\n",
            "epoch 009/010, batch 002/002, step 0341/0496: loss=1.4395709037780762\n",
            "epoch 009/010, batch 002/002, step 0342/0496: loss=1.9557417631149292\n",
            "epoch 009/010, batch 002/002, step 0343/0496: loss=1.4760851860046387\n",
            "epoch 009/010, batch 002/002, step 0344/0496: loss=1.4989731311798096\n",
            "epoch 009/010, batch 002/002, step 0345/0496: loss=1.6560949087142944\n",
            "epoch 009/010, batch 002/002, step 0346/0496: loss=1.6590380668640137\n",
            "epoch 009/010, batch 002/002, step 0347/0496: loss=1.7840795516967773\n",
            "epoch 009/010, batch 002/002, step 0348/0496: loss=1.7396832704544067\n",
            "epoch 009/010, batch 002/002, step 0349/0496: loss=2.8851611614227295\n",
            "epoch 009/010, batch 002/002, step 0350/0496: loss=1.9021328687667847\n",
            "epoch 009/010, batch 002/002, step 0351/0496: loss=1.5615508556365967\n",
            "epoch 009/010, batch 002/002, step 0352/0496: loss=2.1544814109802246\n",
            "epoch 009/010, batch 002/002, step 0353/0496: loss=1.4692280292510986\n",
            "epoch 009/010, batch 002/002, step 0354/0496: loss=1.4305816888809204\n",
            "epoch 009/010, batch 002/002, step 0355/0496: loss=1.239365577697754\n",
            "epoch 009/010, batch 002/002, step 0356/0496: loss=2.07564115524292\n",
            "epoch 009/010, batch 002/002, step 0357/0496: loss=1.6075859069824219\n",
            "epoch 009/010, batch 002/002, step 0358/0496: loss=1.7312268018722534\n",
            "epoch 009/010, batch 002/002, step 0359/0496: loss=2.210799217224121\n",
            "epoch 009/010, batch 002/002, step 0360/0496: loss=3.359875202178955\n",
            "epoch 009/010, batch 002/002, step 0361/0496: loss=1.4238929748535156\n",
            "epoch 009/010, batch 002/002, step 0362/0496: loss=1.5256152153015137\n",
            "epoch 009/010, batch 002/002, step 0363/0496: loss=1.2941863536834717\n",
            "epoch 009/010, batch 002/002, step 0364/0496: loss=1.616929292678833\n",
            "epoch 009/010, batch 002/002, step 0365/0496: loss=1.7573091983795166\n",
            "epoch 009/010, batch 002/002, step 0366/0496: loss=2.8516745567321777\n",
            "epoch 009/010, batch 002/002, step 0367/0496: loss=1.9793822765350342\n",
            "epoch 009/010, batch 002/002, step 0368/0496: loss=1.853878378868103\n",
            "epoch 009/010, batch 002/002, step 0369/0496: loss=1.0849689245224\n",
            "epoch 009/010, batch 002/002, step 0370/0496: loss=2.113429546356201\n",
            "epoch 009/010, batch 002/002, step 0371/0496: loss=2.269602060317993\n",
            "epoch 009/010, batch 002/002, step 0372/0496: loss=1.3379547595977783\n",
            "epoch 009/010, batch 002/002, step 0373/0496: loss=1.6143217086791992\n",
            "epoch 009/010, batch 002/002, step 0374/0496: loss=1.322463035583496\n",
            "epoch 009/010, batch 002/002, step 0375/0496: loss=1.6833829879760742\n",
            "epoch 009/010, batch 002/002, step 0376/0496: loss=1.253129243850708\n",
            "epoch 009/010, batch 002/002, step 0377/0496: loss=2.0139615535736084\n",
            "epoch 009/010, batch 002/002, step 0378/0496: loss=1.8662189245224\n",
            "epoch 009/010, batch 002/002, step 0379/0496: loss=3.607863426208496\n",
            "epoch 009/010, batch 002/002, step 0380/0496: loss=1.5039784908294678\n",
            "epoch 009/010, batch 002/002, step 0381/0496: loss=1.423901081085205\n",
            "epoch 009/010, batch 002/002, step 0382/0496: loss=1.4745278358459473\n",
            "epoch 009/010, batch 002/002, step 0383/0496: loss=1.596407175064087\n",
            "epoch 009/010, batch 002/002, step 0384/0496: loss=1.275572657585144\n",
            "epoch 009/010, batch 002/002, step 0385/0496: loss=1.539872646331787\n",
            "epoch 009/010, batch 002/002, step 0386/0496: loss=1.4043056964874268\n",
            "epoch 009/010, batch 002/002, step 0387/0496: loss=1.8344924449920654\n",
            "epoch 009/010, batch 002/002, step 0388/0496: loss=1.5286345481872559\n",
            "epoch 009/010, batch 002/002, step 0389/0496: loss=1.8012468814849854\n",
            "epoch 009/010, batch 002/002, step 0390/0496: loss=1.3959062099456787\n",
            "epoch 009/010, batch 002/002, step 0391/0496: loss=1.6828162670135498\n",
            "epoch 009/010, batch 002/002, step 0392/0496: loss=1.3163772821426392\n",
            "epoch 009/010, batch 002/002, step 0393/0496: loss=1.1218593120574951\n",
            "epoch 009/010, batch 002/002, step 0394/0496: loss=1.823155164718628\n",
            "epoch 009/010, batch 002/002, step 0395/0496: loss=1.3577181100845337\n",
            "epoch 009/010, batch 002/002, step 0396/0496: loss=1.6580092906951904\n",
            "epoch 009/010, batch 002/002, step 0397/0496: loss=1.4640417098999023\n",
            "epoch 009/010, batch 002/002, step 0398/0496: loss=1.2681955099105835\n",
            "epoch 009/010, batch 002/002, step 0399/0496: loss=1.2923003435134888\n",
            "epoch 009/010, batch 002/002, step 0400/0496: loss=1.8812233209609985\n",
            "epoch 009/010, batch 002/002, step 0401/0496: loss=1.7972135543823242\n",
            "epoch 009/010, batch 002/002, step 0402/0496: loss=1.1022059917449951\n",
            "epoch 009/010, batch 002/002, step 0403/0496: loss=1.529465675354004\n",
            "epoch 009/010, batch 002/002, step 0404/0496: loss=1.426344633102417\n",
            "epoch 009/010, batch 002/002, step 0405/0496: loss=1.2454683780670166\n",
            "epoch 009/010, batch 002/002, step 0406/0496: loss=1.4758543968200684\n",
            "epoch 009/010, batch 002/002, step 0407/0496: loss=1.6456029415130615\n",
            "epoch 009/010, batch 002/002, step 0408/0496: loss=1.2619810104370117\n",
            "epoch 009/010, batch 002/002, step 0409/0496: loss=1.5342512130737305\n",
            "epoch 009/010, batch 002/002, step 0410/0496: loss=1.469879150390625\n",
            "epoch 009/010, batch 002/002, step 0411/0496: loss=1.769437313079834\n",
            "epoch 009/010, batch 002/002, step 0412/0496: loss=1.5878005027770996\n",
            "epoch 009/010, batch 002/002, step 0413/0496: loss=1.586572289466858\n",
            "epoch 009/010, batch 002/002, step 0414/0496: loss=1.901147484779358\n",
            "epoch 009/010, batch 002/002, step 0415/0496: loss=1.3355789184570312\n",
            "epoch 009/010, batch 002/002, step 0416/0496: loss=1.398432731628418\n",
            "epoch 009/010, batch 002/002, step 0417/0496: loss=1.5217863321304321\n",
            "epoch 009/010, batch 002/002, step 0418/0496: loss=1.4863405227661133\n",
            "epoch 009/010, batch 002/002, step 0419/0496: loss=1.8096669912338257\n",
            "epoch 009/010, batch 002/002, step 0420/0496: loss=1.5144174098968506\n",
            "epoch 009/010, batch 002/002, step 0421/0496: loss=1.4685289859771729\n",
            "epoch 009/010, batch 002/002, step 0422/0496: loss=1.3757209777832031\n",
            "epoch 009/010, batch 002/002, step 0423/0496: loss=1.469049334526062\n",
            "epoch 009/010, batch 002/002, step 0424/0496: loss=1.2937726974487305\n",
            "epoch 009/010, batch 002/002, step 0425/0496: loss=2.7574076652526855\n",
            "epoch 009/010, batch 002/002, step 0426/0496: loss=1.419884443283081\n",
            "epoch 009/010, batch 002/002, step 0427/0496: loss=2.4099206924438477\n",
            "epoch 009/010, batch 002/002, step 0428/0496: loss=3.963073968887329\n",
            "epoch 009/010, batch 002/002, step 0429/0496: loss=1.0966441631317139\n",
            "epoch 009/010, batch 002/002, step 0430/0496: loss=1.99619722366333\n",
            "epoch 009/010, batch 002/002, step 0431/0496: loss=1.5291612148284912\n",
            "epoch 009/010, batch 002/002, step 0432/0496: loss=1.6771976947784424\n",
            "epoch 009/010, batch 002/002, step 0433/0496: loss=1.774963140487671\n",
            "epoch 009/010, batch 002/002, step 0434/0496: loss=1.4612045288085938\n",
            "epoch 009/010, batch 002/002, step 0435/0496: loss=1.216233730316162\n",
            "epoch 009/010, batch 002/002, step 0436/0496: loss=2.5613789558410645\n",
            "epoch 009/010, batch 002/002, step 0437/0496: loss=1.3539326190948486\n",
            "epoch 009/010, batch 002/002, step 0438/0496: loss=1.7719775438308716\n",
            "epoch 009/010, batch 002/002, step 0439/0496: loss=1.4493165016174316\n",
            "epoch 009/010, batch 002/002, step 0440/0496: loss=1.8320693969726562\n",
            "epoch 009/010, batch 002/002, step 0441/0496: loss=1.488016963005066\n",
            "epoch 009/010, batch 002/002, step 0442/0496: loss=2.0435712337493896\n",
            "epoch 009/010, batch 002/002, step 0443/0496: loss=1.9016815423965454\n",
            "epoch 009/010, batch 002/002, step 0444/0496: loss=1.6174591779708862\n",
            "epoch 009/010, batch 002/002, step 0445/0496: loss=1.7078187465667725\n",
            "epoch 009/010, batch 002/002, step 0446/0496: loss=1.4982244968414307\n",
            "epoch 009/010, batch 002/002, step 0447/0496: loss=2.0642459392547607\n",
            "epoch 009/010, batch 002/002, step 0448/0496: loss=1.4263406991958618\n",
            "epoch 009/010, batch 002/002, step 0449/0496: loss=1.5771243572235107\n",
            "epoch 009/010, batch 002/002, step 0450/0496: loss=1.4773609638214111\n",
            "epoch 009/010, batch 002/002, step 0451/0496: loss=1.3238129615783691\n",
            "epoch 009/010, batch 002/002, step 0452/0496: loss=1.3588539361953735\n",
            "epoch 009/010, batch 002/002, step 0453/0496: loss=2.7555885314941406\n",
            "epoch 009/010, batch 002/002, step 0454/0496: loss=1.5157723426818848\n",
            "epoch 009/010, batch 002/002, step 0455/0496: loss=1.1287686824798584\n",
            "epoch 009/010, batch 002/002, step 0456/0496: loss=1.775813102722168\n",
            "epoch 009/010, batch 002/002, step 0457/0496: loss=1.5727317333221436\n",
            "epoch 009/010, batch 002/002, step 0458/0496: loss=2.091322422027588\n",
            "epoch 009/010, batch 002/002, step 0459/0496: loss=1.390795111656189\n",
            "epoch 009/010, batch 002/002, step 0460/0496: loss=1.1298131942749023\n",
            "epoch 009/010, batch 002/002, step 0461/0496: loss=1.4467906951904297\n",
            "epoch 009/010, batch 002/002, step 0462/0496: loss=1.5326789617538452\n",
            "epoch 009/010, batch 002/002, step 0463/0496: loss=1.162552833557129\n",
            "epoch 009/010, batch 002/002, step 0464/0496: loss=1.579279899597168\n",
            "epoch 009/010, batch 002/002, step 0465/0496: loss=1.8152048587799072\n",
            "epoch 009/010, batch 002/002, step 0466/0496: loss=1.7126799821853638\n",
            "epoch 009/010, batch 002/002, step 0467/0496: loss=1.238358736038208\n",
            "epoch 009/010, batch 002/002, step 0468/0496: loss=2.5410385131835938\n",
            "epoch 009/010, batch 002/002, step 0469/0496: loss=1.8835515975952148\n",
            "epoch 009/010, batch 002/002, step 0470/0496: loss=2.722334384918213\n",
            "epoch 009/010, batch 002/002, step 0471/0496: loss=1.6524498462677002\n",
            "epoch 009/010, batch 002/002, step 0472/0496: loss=1.4564592838287354\n",
            "epoch 009/010, batch 002/002, step 0473/0496: loss=1.6326844692230225\n",
            "epoch 009/010, batch 002/002, step 0474/0496: loss=1.3869233131408691\n",
            "epoch 009/010, batch 002/002, step 0475/0496: loss=1.7956503629684448\n",
            "epoch 009/010, batch 002/002, step 0476/0496: loss=2.7660164833068848\n",
            "epoch 009/010, batch 002/002, step 0477/0496: loss=1.681943655014038\n",
            "epoch 009/010, batch 002/002, step 0478/0496: loss=1.5067646503448486\n",
            "epoch 009/010, batch 002/002, step 0479/0496: loss=1.6516294479370117\n",
            "epoch 009/010, batch 002/002, step 0480/0496: loss=1.6054474115371704\n",
            "epoch 009/010, batch 002/002, step 0481/0496: loss=1.4003185033798218\n",
            "epoch 009/010, batch 002/002, step 0482/0496: loss=1.5654985904693604\n",
            "epoch 009/010, batch 002/002, step 0483/0496: loss=2.034208297729492\n",
            "epoch 009/010, batch 002/002, step 0484/0496: loss=1.592531442642212\n",
            "epoch 009/010, batch 002/002, step 0485/0496: loss=2.146047353744507\n",
            "epoch 009/010, batch 002/002, step 0486/0496: loss=1.3908805847167969\n",
            "epoch 009/010, batch 002/002, step 0487/0496: loss=1.606964349746704\n",
            "epoch 009/010, batch 002/002, step 0488/0496: loss=1.3105664253234863\n",
            "epoch 009/010, batch 002/002, step 0489/0496: loss=2.4038596153259277\n",
            "epoch 009/010, batch 002/002, step 0490/0496: loss=1.4756879806518555\n",
            "epoch 009/010, batch 002/002, step 0491/0496: loss=1.488353967666626\n",
            "epoch 009/010, batch 002/002, step 0492/0496: loss=1.1957478523254395\n",
            "epoch 009/010, batch 002/002, step 0493/0496: loss=1.3713210821151733\n",
            "epoch 009/010, batch 002/002, step 0494/0496: loss=2.5363080501556396\n",
            "epoch 009/010, batch 002/002, step 0495/0496: loss=1.6313631534576416\n",
            "epoch 009/010, batch 002/002, step 0496/0496: loss=1.8276695013046265\n",
            "epoch 010/010, batch 001/002, step 0001/0496: loss=1.5387104749679565\n",
            "epoch 010/010, batch 001/002, step 0002/0496: loss=1.5236032009124756\n",
            "epoch 010/010, batch 001/002, step 0003/0496: loss=1.1944328546524048\n",
            "epoch 010/010, batch 001/002, step 0004/0496: loss=1.9038894176483154\n",
            "epoch 010/010, batch 001/002, step 0005/0496: loss=1.8553822040557861\n",
            "epoch 010/010, batch 001/002, step 0006/0496: loss=1.1547350883483887\n",
            "epoch 010/010, batch 001/002, step 0007/0496: loss=1.4120067358016968\n",
            "epoch 010/010, batch 001/002, step 0008/0496: loss=1.1519105434417725\n",
            "epoch 010/010, batch 001/002, step 0009/0496: loss=1.1465346813201904\n",
            "epoch 010/010, batch 001/002, step 0010/0496: loss=1.8390100002288818\n",
            "epoch 010/010, batch 001/002, step 0011/0496: loss=1.1601676940917969\n",
            "epoch 010/010, batch 001/002, step 0012/0496: loss=1.1355067491531372\n",
            "epoch 010/010, batch 001/002, step 0013/0496: loss=1.5488319396972656\n",
            "epoch 010/010, batch 001/002, step 0014/0496: loss=1.353910207748413\n",
            "epoch 010/010, batch 001/002, step 0015/0496: loss=1.293440341949463\n",
            "epoch 010/010, batch 001/002, step 0016/0496: loss=1.4755383729934692\n",
            "epoch 010/010, batch 001/002, step 0017/0496: loss=1.503227710723877\n",
            "epoch 010/010, batch 001/002, step 0018/0496: loss=1.3285433053970337\n",
            "epoch 010/010, batch 001/002, step 0019/0496: loss=2.1062841415405273\n",
            "epoch 010/010, batch 001/002, step 0020/0496: loss=1.951305866241455\n",
            "epoch 010/010, batch 001/002, step 0021/0496: loss=1.6854937076568604\n",
            "epoch 010/010, batch 001/002, step 0022/0496: loss=1.997478723526001\n",
            "epoch 010/010, batch 001/002, step 0023/0496: loss=1.6738557815551758\n",
            "epoch 010/010, batch 001/002, step 0024/0496: loss=1.6796692609786987\n",
            "epoch 010/010, batch 001/002, step 0025/0496: loss=1.9272394180297852\n",
            "epoch 010/010, batch 001/002, step 0026/0496: loss=2.765479564666748\n",
            "epoch 010/010, batch 001/002, step 0027/0496: loss=1.4735753536224365\n",
            "epoch 010/010, batch 001/002, step 0028/0496: loss=2.708997964859009\n",
            "epoch 010/010, batch 001/002, step 0029/0496: loss=1.634687066078186\n",
            "epoch 010/010, batch 001/002, step 0030/0496: loss=2.093343734741211\n",
            "epoch 010/010, batch 001/002, step 0031/0496: loss=1.745951771736145\n",
            "epoch 010/010, batch 001/002, step 0032/0496: loss=2.3594799041748047\n",
            "epoch 010/010, batch 001/002, step 0033/0496: loss=1.9948437213897705\n",
            "epoch 010/010, batch 001/002, step 0034/0496: loss=1.6479133367538452\n",
            "epoch 010/010, batch 001/002, step 0035/0496: loss=1.904123306274414\n",
            "epoch 010/010, batch 001/002, step 0036/0496: loss=1.7702357769012451\n",
            "epoch 010/010, batch 001/002, step 0037/0496: loss=2.1004371643066406\n",
            "epoch 010/010, batch 001/002, step 0038/0496: loss=1.823000192642212\n",
            "epoch 010/010, batch 001/002, step 0039/0496: loss=1.7820429801940918\n",
            "epoch 010/010, batch 001/002, step 0040/0496: loss=1.5251010656356812\n",
            "epoch 010/010, batch 001/002, step 0041/0496: loss=1.739212989807129\n",
            "epoch 010/010, batch 001/002, step 0042/0496: loss=2.501695156097412\n",
            "epoch 010/010, batch 001/002, step 0043/0496: loss=3.3413305282592773\n",
            "epoch 010/010, batch 001/002, step 0044/0496: loss=1.7115929126739502\n",
            "epoch 010/010, batch 001/002, step 0045/0496: loss=3.276134729385376\n",
            "epoch 010/010, batch 001/002, step 0046/0496: loss=1.5713677406311035\n",
            "epoch 010/010, batch 001/002, step 0047/0496: loss=4.7739787101745605\n",
            "epoch 010/010, batch 001/002, step 0048/0496: loss=1.4078198671340942\n",
            "epoch 010/010, batch 001/002, step 0049/0496: loss=2.33758544921875\n",
            "epoch 010/010, batch 001/002, step 0050/0496: loss=1.7616968154907227\n",
            "epoch 010/010, batch 001/002, step 0051/0496: loss=2.0119857788085938\n",
            "epoch 010/010, batch 001/002, step 0052/0496: loss=1.5317344665527344\n",
            "epoch 010/010, batch 001/002, step 0053/0496: loss=2.130427122116089\n",
            "epoch 010/010, batch 001/002, step 0054/0496: loss=2.0292248725891113\n",
            "epoch 010/010, batch 001/002, step 0055/0496: loss=1.2497730255126953\n",
            "epoch 010/010, batch 001/002, step 0056/0496: loss=2.939469814300537\n",
            "epoch 010/010, batch 001/002, step 0057/0496: loss=1.8246376514434814\n",
            "epoch 010/010, batch 001/002, step 0058/0496: loss=2.0040085315704346\n",
            "epoch 010/010, batch 001/002, step 0059/0496: loss=1.6285202503204346\n",
            "epoch 010/010, batch 001/002, step 0060/0496: loss=2.2769224643707275\n",
            "epoch 010/010, batch 001/002, step 0061/0496: loss=1.2517030239105225\n",
            "epoch 010/010, batch 001/002, step 0062/0496: loss=1.3315868377685547\n",
            "epoch 010/010, batch 001/002, step 0063/0496: loss=1.3865963220596313\n",
            "epoch 010/010, batch 001/002, step 0064/0496: loss=1.4563125371932983\n",
            "epoch 010/010, batch 001/002, step 0065/0496: loss=1.2459359169006348\n",
            "epoch 010/010, batch 001/002, step 0066/0496: loss=1.4024477005004883\n",
            "epoch 010/010, batch 001/002, step 0067/0496: loss=2.5062074661254883\n",
            "epoch 010/010, batch 001/002, step 0068/0496: loss=1.5449354648590088\n",
            "epoch 010/010, batch 001/002, step 0069/0496: loss=1.8401764631271362\n",
            "epoch 010/010, batch 001/002, step 0070/0496: loss=1.679098129272461\n",
            "epoch 010/010, batch 001/002, step 0071/0496: loss=1.1577638387680054\n",
            "epoch 010/010, batch 001/002, step 0072/0496: loss=1.4748444557189941\n",
            "epoch 010/010, batch 001/002, step 0073/0496: loss=2.6408910751342773\n",
            "epoch 010/010, batch 001/002, step 0074/0496: loss=1.4550129175186157\n",
            "epoch 010/010, batch 001/002, step 0075/0496: loss=2.600717782974243\n",
            "epoch 010/010, batch 001/002, step 0076/0496: loss=1.3870420455932617\n",
            "epoch 010/010, batch 001/002, step 0077/0496: loss=1.2543702125549316\n",
            "epoch 010/010, batch 001/002, step 0078/0496: loss=1.3840519189834595\n",
            "epoch 010/010, batch 001/002, step 0079/0496: loss=1.1660938262939453\n",
            "epoch 010/010, batch 001/002, step 0080/0496: loss=1.8425058126449585\n",
            "epoch 010/010, batch 001/002, step 0081/0496: loss=1.2858686447143555\n",
            "epoch 010/010, batch 001/002, step 0082/0496: loss=1.611025094985962\n",
            "epoch 010/010, batch 001/002, step 0083/0496: loss=1.3172075748443604\n",
            "epoch 010/010, batch 001/002, step 0084/0496: loss=2.418924331665039\n",
            "epoch 010/010, batch 001/002, step 0085/0496: loss=2.1049892902374268\n",
            "epoch 010/010, batch 001/002, step 0086/0496: loss=1.362586259841919\n",
            "epoch 010/010, batch 001/002, step 0087/0496: loss=1.2273375988006592\n",
            "epoch 010/010, batch 001/002, step 0088/0496: loss=1.640663504600525\n",
            "epoch 010/010, batch 001/002, step 0089/0496: loss=1.8636881113052368\n",
            "epoch 010/010, batch 001/002, step 0090/0496: loss=1.5596954822540283\n",
            "epoch 010/010, batch 001/002, step 0091/0496: loss=2.0520167350769043\n",
            "epoch 010/010, batch 001/002, step 0092/0496: loss=1.3182684183120728\n",
            "epoch 010/010, batch 001/002, step 0093/0496: loss=1.364884853363037\n",
            "epoch 010/010, batch 001/002, step 0094/0496: loss=1.9711391925811768\n",
            "epoch 010/010, batch 001/002, step 0095/0496: loss=1.3079829216003418\n",
            "epoch 010/010, batch 001/002, step 0096/0496: loss=1.2050862312316895\n",
            "epoch 010/010, batch 001/002, step 0097/0496: loss=1.1752136945724487\n",
            "epoch 010/010, batch 001/002, step 0098/0496: loss=1.3116648197174072\n",
            "epoch 010/010, batch 001/002, step 0099/0496: loss=1.6025447845458984\n",
            "epoch 010/010, batch 001/002, step 0100/0496: loss=1.7638893127441406\n",
            "epoch 010/010, batch 001/002, step 0101/0496: loss=1.5070576667785645\n",
            "epoch 010/010, batch 001/002, step 0102/0496: loss=1.5526725053787231\n",
            "epoch 010/010, batch 001/002, step 0103/0496: loss=1.5544708967208862\n",
            "epoch 010/010, batch 001/002, step 0104/0496: loss=1.3599865436553955\n",
            "epoch 010/010, batch 001/002, step 0105/0496: loss=1.493408441543579\n",
            "epoch 010/010, batch 001/002, step 0106/0496: loss=1.3813787698745728\n",
            "epoch 010/010, batch 001/002, step 0107/0496: loss=1.4782553911209106\n",
            "epoch 010/010, batch 001/002, step 0108/0496: loss=1.4747178554534912\n",
            "epoch 010/010, batch 001/002, step 0109/0496: loss=1.835176706314087\n",
            "epoch 010/010, batch 001/002, step 0110/0496: loss=1.6302847862243652\n",
            "epoch 010/010, batch 001/002, step 0111/0496: loss=1.543149471282959\n",
            "epoch 010/010, batch 001/002, step 0112/0496: loss=1.4437756538391113\n",
            "epoch 010/010, batch 001/002, step 0113/0496: loss=1.4946348667144775\n",
            "epoch 010/010, batch 001/002, step 0114/0496: loss=1.4794995784759521\n",
            "epoch 010/010, batch 001/002, step 0115/0496: loss=1.1976261138916016\n",
            "epoch 010/010, batch 001/002, step 0116/0496: loss=3.059011697769165\n",
            "epoch 010/010, batch 001/002, step 0117/0496: loss=1.2482224702835083\n",
            "epoch 010/010, batch 001/002, step 0118/0496: loss=1.6618831157684326\n",
            "epoch 010/010, batch 001/002, step 0119/0496: loss=1.6078064441680908\n",
            "epoch 010/010, batch 001/002, step 0120/0496: loss=1.6058815717697144\n",
            "epoch 010/010, batch 001/002, step 0121/0496: loss=1.9811816215515137\n",
            "epoch 010/010, batch 001/002, step 0122/0496: loss=1.3079447746276855\n",
            "epoch 010/010, batch 001/002, step 0123/0496: loss=1.0209026336669922\n",
            "epoch 010/010, batch 001/002, step 0124/0496: loss=2.3208446502685547\n",
            "epoch 010/010, batch 001/002, step 0125/0496: loss=1.5086791515350342\n",
            "epoch 010/010, batch 001/002, step 0126/0496: loss=1.4059991836547852\n",
            "epoch 010/010, batch 001/002, step 0127/0496: loss=1.6458606719970703\n",
            "epoch 010/010, batch 001/002, step 0128/0496: loss=1.1429166793823242\n",
            "epoch 010/010, batch 001/002, step 0129/0496: loss=2.4032435417175293\n",
            "epoch 010/010, batch 001/002, step 0130/0496: loss=2.103713035583496\n",
            "epoch 010/010, batch 001/002, step 0131/0496: loss=1.81591796875\n",
            "epoch 010/010, batch 001/002, step 0132/0496: loss=2.226094961166382\n",
            "epoch 010/010, batch 001/002, step 0133/0496: loss=1.7051811218261719\n",
            "epoch 010/010, batch 001/002, step 0134/0496: loss=2.1842284202575684\n",
            "epoch 010/010, batch 001/002, step 0135/0496: loss=1.9638457298278809\n",
            "epoch 010/010, batch 001/002, step 0136/0496: loss=2.1448988914489746\n",
            "epoch 010/010, batch 001/002, step 0137/0496: loss=1.5128602981567383\n",
            "epoch 010/010, batch 001/002, step 0138/0496: loss=1.8171069622039795\n",
            "epoch 010/010, batch 001/002, step 0139/0496: loss=1.8550547361373901\n",
            "epoch 010/010, batch 001/002, step 0140/0496: loss=1.989578127861023\n",
            "epoch 010/010, batch 001/002, step 0141/0496: loss=1.6722546815872192\n",
            "epoch 010/010, batch 001/002, step 0142/0496: loss=2.187257766723633\n",
            "epoch 010/010, batch 001/002, step 0143/0496: loss=1.5300028324127197\n",
            "epoch 010/010, batch 001/002, step 0144/0496: loss=1.795496940612793\n",
            "epoch 010/010, batch 001/002, step 0145/0496: loss=1.8164024353027344\n",
            "epoch 010/010, batch 001/002, step 0146/0496: loss=2.1618988513946533\n",
            "epoch 010/010, batch 001/002, step 0147/0496: loss=1.1780097484588623\n",
            "epoch 010/010, batch 001/002, step 0148/0496: loss=1.643149733543396\n",
            "epoch 010/010, batch 001/002, step 0149/0496: loss=1.2332680225372314\n",
            "epoch 010/010, batch 001/002, step 0150/0496: loss=1.7603340148925781\n",
            "epoch 010/010, batch 001/002, step 0151/0496: loss=1.166304349899292\n",
            "epoch 010/010, batch 001/002, step 0152/0496: loss=1.5483890771865845\n",
            "epoch 010/010, batch 001/002, step 0153/0496: loss=1.3802907466888428\n",
            "epoch 010/010, batch 001/002, step 0154/0496: loss=1.4415700435638428\n",
            "epoch 010/010, batch 001/002, step 0155/0496: loss=1.6882061958312988\n",
            "epoch 010/010, batch 001/002, step 0156/0496: loss=1.6673825979232788\n",
            "epoch 010/010, batch 001/002, step 0157/0496: loss=1.6484371423721313\n",
            "epoch 010/010, batch 001/002, step 0158/0496: loss=1.2003228664398193\n",
            "epoch 010/010, batch 001/002, step 0159/0496: loss=1.1167445182800293\n",
            "epoch 010/010, batch 001/002, step 0160/0496: loss=1.4840474128723145\n",
            "epoch 010/010, batch 001/002, step 0161/0496: loss=1.6693096160888672\n",
            "epoch 010/010, batch 001/002, step 0162/0496: loss=1.349588394165039\n",
            "epoch 010/010, batch 001/002, step 0163/0496: loss=1.8117451667785645\n",
            "epoch 010/010, batch 001/002, step 0164/0496: loss=1.2380982637405396\n",
            "epoch 010/010, batch 001/002, step 0165/0496: loss=1.843787670135498\n",
            "epoch 010/010, batch 001/002, step 0166/0496: loss=2.1454873085021973\n",
            "epoch 010/010, batch 001/002, step 0167/0496: loss=1.6176893711090088\n",
            "epoch 010/010, batch 001/002, step 0168/0496: loss=1.658904790878296\n",
            "epoch 010/010, batch 001/002, step 0169/0496: loss=1.3895902633666992\n",
            "epoch 010/010, batch 001/002, step 0170/0496: loss=1.5697736740112305\n",
            "epoch 010/010, batch 001/002, step 0171/0496: loss=1.7146408557891846\n",
            "epoch 010/010, batch 001/002, step 0172/0496: loss=1.8776813745498657\n",
            "epoch 010/010, batch 001/002, step 0173/0496: loss=1.6631779670715332\n",
            "epoch 010/010, batch 001/002, step 0174/0496: loss=1.1500567197799683\n",
            "epoch 010/010, batch 001/002, step 0175/0496: loss=1.6777433156967163\n",
            "epoch 010/010, batch 001/002, step 0176/0496: loss=1.4052976369857788\n",
            "epoch 010/010, batch 001/002, step 0177/0496: loss=1.211714267730713\n",
            "epoch 010/010, batch 001/002, step 0178/0496: loss=2.865291118621826\n",
            "epoch 010/010, batch 001/002, step 0179/0496: loss=1.8430670499801636\n",
            "epoch 010/010, batch 001/002, step 0180/0496: loss=2.6626765727996826\n",
            "epoch 010/010, batch 001/002, step 0181/0496: loss=1.1856234073638916\n",
            "epoch 010/010, batch 001/002, step 0182/0496: loss=1.9117004871368408\n",
            "epoch 010/010, batch 001/002, step 0183/0496: loss=1.7325083017349243\n",
            "epoch 010/010, batch 001/002, step 0184/0496: loss=2.8177809715270996\n",
            "epoch 010/010, batch 001/002, step 0185/0496: loss=1.354380488395691\n",
            "epoch 010/010, batch 001/002, step 0186/0496: loss=4.120634078979492\n",
            "epoch 010/010, batch 001/002, step 0187/0496: loss=1.500776767730713\n",
            "epoch 010/010, batch 001/002, step 0188/0496: loss=3.1444485187530518\n",
            "epoch 010/010, batch 001/002, step 0189/0496: loss=2.751612663269043\n",
            "epoch 010/010, batch 001/002, step 0190/0496: loss=2.725698947906494\n",
            "epoch 010/010, batch 001/002, step 0191/0496: loss=1.725762963294983\n",
            "epoch 010/010, batch 001/002, step 0192/0496: loss=1.9742599725723267\n",
            "epoch 010/010, batch 001/002, step 0193/0496: loss=2.325186014175415\n",
            "epoch 010/010, batch 001/002, step 0194/0496: loss=1.8313391208648682\n",
            "epoch 010/010, batch 001/002, step 0195/0496: loss=1.8355990648269653\n",
            "epoch 010/010, batch 001/002, step 0196/0496: loss=2.831531524658203\n",
            "epoch 010/010, batch 001/002, step 0197/0496: loss=2.3180317878723145\n",
            "epoch 010/010, batch 001/002, step 0198/0496: loss=1.9682596921920776\n",
            "epoch 010/010, batch 001/002, step 0199/0496: loss=2.1484017372131348\n",
            "epoch 010/010, batch 001/002, step 0200/0496: loss=2.4819040298461914\n",
            "epoch 010/010, batch 001/002, step 0201/0496: loss=2.631992816925049\n",
            "epoch 010/010, batch 001/002, step 0202/0496: loss=2.093259572982788\n",
            "epoch 010/010, batch 001/002, step 0203/0496: loss=1.826742172241211\n",
            "epoch 010/010, batch 001/002, step 0204/0496: loss=2.1663031578063965\n",
            "epoch 010/010, batch 001/002, step 0205/0496: loss=2.0563297271728516\n",
            "epoch 010/010, batch 001/002, step 0206/0496: loss=2.3395910263061523\n",
            "epoch 010/010, batch 001/002, step 0207/0496: loss=1.50164794921875\n",
            "epoch 010/010, batch 001/002, step 0208/0496: loss=3.105407238006592\n",
            "epoch 010/010, batch 001/002, step 0209/0496: loss=1.6214687824249268\n",
            "epoch 010/010, batch 001/002, step 0210/0496: loss=2.177682399749756\n",
            "epoch 010/010, batch 001/002, step 0211/0496: loss=1.458802580833435\n",
            "epoch 010/010, batch 001/002, step 0212/0496: loss=3.3020942211151123\n",
            "epoch 010/010, batch 001/002, step 0213/0496: loss=1.2986900806427002\n",
            "epoch 010/010, batch 001/002, step 0214/0496: loss=2.384903907775879\n",
            "epoch 010/010, batch 001/002, step 0215/0496: loss=2.536032199859619\n",
            "epoch 010/010, batch 001/002, step 0216/0496: loss=2.457223892211914\n",
            "epoch 010/010, batch 001/002, step 0217/0496: loss=2.9428107738494873\n",
            "epoch 010/010, batch 001/002, step 0218/0496: loss=2.2045276165008545\n",
            "epoch 010/010, batch 001/002, step 0219/0496: loss=1.4584786891937256\n",
            "epoch 010/010, batch 001/002, step 0220/0496: loss=1.549991488456726\n",
            "epoch 010/010, batch 001/002, step 0221/0496: loss=1.2542303800582886\n",
            "epoch 010/010, batch 001/002, step 0222/0496: loss=1.3575636148452759\n",
            "epoch 010/010, batch 001/002, step 0223/0496: loss=1.433027744293213\n",
            "epoch 010/010, batch 001/002, step 0224/0496: loss=2.8588685989379883\n",
            "epoch 010/010, batch 001/002, step 0225/0496: loss=1.3291617631912231\n",
            "epoch 010/010, batch 001/002, step 0226/0496: loss=2.0642547607421875\n",
            "epoch 010/010, batch 001/002, step 0227/0496: loss=1.417362928390503\n",
            "epoch 010/010, batch 001/002, step 0228/0496: loss=1.5552526712417603\n",
            "epoch 010/010, batch 001/002, step 0229/0496: loss=1.8069490194320679\n",
            "epoch 010/010, batch 001/002, step 0230/0496: loss=1.9514622688293457\n",
            "epoch 010/010, batch 001/002, step 0231/0496: loss=1.9809174537658691\n",
            "epoch 010/010, batch 001/002, step 0232/0496: loss=1.2251337766647339\n",
            "epoch 010/010, batch 001/002, step 0233/0496: loss=3.0055160522460938\n",
            "epoch 010/010, batch 001/002, step 0234/0496: loss=2.604499101638794\n",
            "epoch 010/010, batch 001/002, step 0235/0496: loss=2.260089874267578\n",
            "epoch 010/010, batch 001/002, step 0236/0496: loss=1.1378852128982544\n",
            "epoch 010/010, batch 001/002, step 0237/0496: loss=2.7908382415771484\n",
            "epoch 010/010, batch 001/002, step 0238/0496: loss=1.4865977764129639\n",
            "epoch 010/010, batch 001/002, step 0239/0496: loss=2.9375948905944824\n",
            "epoch 010/010, batch 001/002, step 0240/0496: loss=1.9590493440628052\n",
            "epoch 010/010, batch 001/002, step 0241/0496: loss=2.5926260948181152\n",
            "epoch 010/010, batch 001/002, step 0242/0496: loss=1.5363140106201172\n",
            "epoch 010/010, batch 001/002, step 0243/0496: loss=3.3398537635803223\n",
            "epoch 010/010, batch 001/002, step 0244/0496: loss=1.49246084690094\n",
            "epoch 010/010, batch 001/002, step 0245/0496: loss=1.780327320098877\n",
            "epoch 010/010, batch 001/002, step 0246/0496: loss=1.6809649467468262\n",
            "epoch 010/010, batch 001/002, step 0247/0496: loss=2.0744872093200684\n",
            "epoch 010/010, batch 001/002, step 0248/0496: loss=1.8689446449279785\n",
            "epoch 010/010, batch 001/002, step 0249/0496: loss=1.6769282817840576\n",
            "epoch 010/010, batch 001/002, step 0250/0496: loss=1.7311701774597168\n",
            "epoch 010/010, batch 001/002, step 0251/0496: loss=1.7215900421142578\n",
            "epoch 010/010, batch 001/002, step 0252/0496: loss=1.5927760601043701\n",
            "epoch 010/010, batch 001/002, step 0253/0496: loss=1.7760850191116333\n",
            "epoch 010/010, batch 001/002, step 0254/0496: loss=1.3014007806777954\n",
            "epoch 010/010, batch 001/002, step 0255/0496: loss=1.1017729043960571\n",
            "epoch 010/010, batch 001/002, step 0256/0496: loss=1.4743258953094482\n",
            "epoch 010/010, batch 001/002, step 0257/0496: loss=1.4275082349777222\n",
            "epoch 010/010, batch 001/002, step 0258/0496: loss=1.7204147577285767\n",
            "epoch 010/010, batch 001/002, step 0259/0496: loss=1.8384860754013062\n",
            "epoch 010/010, batch 001/002, step 0260/0496: loss=1.3689262866973877\n",
            "epoch 010/010, batch 001/002, step 0261/0496: loss=1.419961929321289\n",
            "epoch 010/010, batch 001/002, step 0262/0496: loss=1.8722995519638062\n",
            "epoch 010/010, batch 001/002, step 0263/0496: loss=1.740729808807373\n",
            "epoch 010/010, batch 001/002, step 0264/0496: loss=1.5013947486877441\n",
            "epoch 010/010, batch 001/002, step 0265/0496: loss=2.292295455932617\n",
            "epoch 010/010, batch 001/002, step 0266/0496: loss=1.3356510400772095\n",
            "epoch 010/010, batch 001/002, step 0267/0496: loss=1.5488181114196777\n",
            "epoch 010/010, batch 001/002, step 0268/0496: loss=1.6908867359161377\n",
            "epoch 010/010, batch 001/002, step 0269/0496: loss=1.8048231601715088\n",
            "epoch 010/010, batch 001/002, step 0270/0496: loss=1.2791435718536377\n",
            "epoch 010/010, batch 001/002, step 0271/0496: loss=1.5614374876022339\n",
            "epoch 010/010, batch 001/002, step 0272/0496: loss=1.533143401145935\n",
            "epoch 010/010, batch 001/002, step 0273/0496: loss=1.2208540439605713\n",
            "epoch 010/010, batch 001/002, step 0274/0496: loss=1.584082841873169\n",
            "epoch 010/010, batch 001/002, step 0275/0496: loss=1.3408317565917969\n",
            "epoch 010/010, batch 001/002, step 0276/0496: loss=2.333348512649536\n",
            "epoch 010/010, batch 001/002, step 0277/0496: loss=1.5095674991607666\n",
            "epoch 010/010, batch 001/002, step 0278/0496: loss=1.1433708667755127\n",
            "epoch 010/010, batch 001/002, step 0279/0496: loss=1.456498384475708\n",
            "epoch 010/010, batch 001/002, step 0280/0496: loss=1.9144949913024902\n",
            "epoch 010/010, batch 001/002, step 0281/0496: loss=1.1434253454208374\n",
            "epoch 010/010, batch 001/002, step 0282/0496: loss=1.081842064857483\n",
            "epoch 010/010, batch 001/002, step 0283/0496: loss=1.3417494297027588\n",
            "epoch 010/010, batch 001/002, step 0284/0496: loss=1.2575643062591553\n",
            "epoch 010/010, batch 001/002, step 0285/0496: loss=1.5774582624435425\n",
            "epoch 010/010, batch 001/002, step 0286/0496: loss=1.4396684169769287\n",
            "epoch 010/010, batch 001/002, step 0287/0496: loss=1.4331681728363037\n",
            "epoch 010/010, batch 001/002, step 0288/0496: loss=1.4383111000061035\n",
            "epoch 010/010, batch 001/002, step 0289/0496: loss=2.1423544883728027\n",
            "epoch 010/010, batch 001/002, step 0290/0496: loss=1.3890982866287231\n",
            "epoch 010/010, batch 001/002, step 0291/0496: loss=1.1985454559326172\n",
            "epoch 010/010, batch 001/002, step 0292/0496: loss=1.4942219257354736\n",
            "epoch 010/010, batch 001/002, step 0293/0496: loss=1.585433006286621\n",
            "epoch 010/010, batch 001/002, step 0294/0496: loss=1.4812055826187134\n",
            "epoch 010/010, batch 001/002, step 0295/0496: loss=1.4935219287872314\n",
            "epoch 010/010, batch 001/002, step 0296/0496: loss=1.255049705505371\n",
            "epoch 010/010, batch 001/002, step 0297/0496: loss=1.386069416999817\n",
            "epoch 010/010, batch 001/002, step 0298/0496: loss=1.1813035011291504\n",
            "epoch 010/010, batch 001/002, step 0299/0496: loss=1.522318720817566\n",
            "epoch 010/010, batch 001/002, step 0300/0496: loss=1.3845674991607666\n",
            "epoch 010/010, batch 001/002, step 0301/0496: loss=1.322989821434021\n",
            "epoch 010/010, batch 001/002, step 0302/0496: loss=2.373281240463257\n",
            "epoch 010/010, batch 001/002, step 0303/0496: loss=1.6780061721801758\n",
            "epoch 010/010, batch 001/002, step 0304/0496: loss=1.7175979614257812\n",
            "epoch 010/010, batch 001/002, step 0305/0496: loss=1.65199875831604\n",
            "epoch 010/010, batch 001/002, step 0306/0496: loss=1.6083590984344482\n",
            "epoch 010/010, batch 001/002, step 0307/0496: loss=1.3427753448486328\n",
            "epoch 010/010, batch 001/002, step 0308/0496: loss=1.4144678115844727\n",
            "epoch 010/010, batch 001/002, step 0309/0496: loss=1.327589750289917\n",
            "epoch 010/010, batch 001/002, step 0310/0496: loss=1.3505173921585083\n",
            "epoch 010/010, batch 001/002, step 0311/0496: loss=1.313887596130371\n",
            "epoch 010/010, batch 001/002, step 0312/0496: loss=1.2117626667022705\n",
            "epoch 010/010, batch 001/002, step 0313/0496: loss=1.5102028846740723\n",
            "epoch 010/010, batch 001/002, step 0314/0496: loss=1.2442004680633545\n",
            "epoch 010/010, batch 001/002, step 0315/0496: loss=1.700381875038147\n",
            "epoch 010/010, batch 001/002, step 0316/0496: loss=1.5212434530258179\n",
            "epoch 010/010, batch 001/002, step 0317/0496: loss=1.626244068145752\n",
            "epoch 010/010, batch 001/002, step 0318/0496: loss=1.318070411682129\n",
            "epoch 010/010, batch 001/002, step 0319/0496: loss=1.3210269212722778\n",
            "epoch 010/010, batch 001/002, step 0320/0496: loss=1.6448643207550049\n",
            "epoch 010/010, batch 001/002, step 0321/0496: loss=1.6890029907226562\n",
            "epoch 010/010, batch 001/002, step 0322/0496: loss=1.1122212409973145\n",
            "epoch 010/010, batch 001/002, step 0323/0496: loss=1.4181596040725708\n",
            "epoch 010/010, batch 001/002, step 0324/0496: loss=1.6395673751831055\n",
            "epoch 010/010, batch 001/002, step 0325/0496: loss=1.443058729171753\n",
            "epoch 010/010, batch 001/002, step 0326/0496: loss=1.4568419456481934\n",
            "epoch 010/010, batch 001/002, step 0327/0496: loss=2.639780044555664\n",
            "epoch 010/010, batch 001/002, step 0328/0496: loss=1.6594918966293335\n",
            "epoch 010/010, batch 001/002, step 0329/0496: loss=1.4889941215515137\n",
            "epoch 010/010, batch 001/002, step 0330/0496: loss=1.8161711692810059\n",
            "epoch 010/010, batch 001/002, step 0331/0496: loss=1.7029953002929688\n",
            "epoch 010/010, batch 001/002, step 0332/0496: loss=1.2371082305908203\n",
            "epoch 010/010, batch 001/002, step 0333/0496: loss=1.2589926719665527\n",
            "epoch 010/010, batch 001/002, step 0334/0496: loss=1.4327054023742676\n",
            "epoch 010/010, batch 001/002, step 0335/0496: loss=1.4652299880981445\n",
            "epoch 010/010, batch 001/002, step 0336/0496: loss=1.7391798496246338\n",
            "epoch 010/010, batch 001/002, step 0337/0496: loss=1.3382874727249146\n",
            "epoch 010/010, batch 001/002, step 0338/0496: loss=1.529857873916626\n",
            "epoch 010/010, batch 001/002, step 0339/0496: loss=1.429619312286377\n",
            "epoch 010/010, batch 001/002, step 0340/0496: loss=1.1938599348068237\n",
            "epoch 010/010, batch 001/002, step 0341/0496: loss=1.5751326084136963\n",
            "epoch 010/010, batch 001/002, step 0342/0496: loss=1.344846487045288\n",
            "epoch 010/010, batch 001/002, step 0343/0496: loss=1.244573712348938\n",
            "epoch 010/010, batch 001/002, step 0344/0496: loss=2.4132351875305176\n",
            "epoch 010/010, batch 001/002, step 0345/0496: loss=1.2772181034088135\n",
            "epoch 010/010, batch 001/002, step 0346/0496: loss=1.4928958415985107\n",
            "epoch 010/010, batch 001/002, step 0347/0496: loss=1.4459037780761719\n",
            "epoch 010/010, batch 001/002, step 0348/0496: loss=1.1511255502700806\n",
            "epoch 010/010, batch 001/002, step 0349/0496: loss=1.3509986400604248\n",
            "epoch 010/010, batch 001/002, step 0350/0496: loss=1.620792269706726\n",
            "epoch 010/010, batch 001/002, step 0351/0496: loss=1.0824899673461914\n",
            "epoch 010/010, batch 001/002, step 0352/0496: loss=1.3136409521102905\n",
            "epoch 010/010, batch 001/002, step 0353/0496: loss=1.2456929683685303\n",
            "epoch 010/010, batch 001/002, step 0354/0496: loss=1.8127226829528809\n",
            "epoch 010/010, batch 001/002, step 0355/0496: loss=2.272019386291504\n",
            "epoch 010/010, batch 001/002, step 0356/0496: loss=1.5314997434616089\n",
            "epoch 010/010, batch 001/002, step 0357/0496: loss=1.4119963645935059\n",
            "epoch 010/010, batch 001/002, step 0358/0496: loss=1.5561139583587646\n",
            "epoch 010/010, batch 001/002, step 0359/0496: loss=1.5880277156829834\n",
            "epoch 010/010, batch 001/002, step 0360/0496: loss=1.3413894176483154\n",
            "epoch 010/010, batch 001/002, step 0361/0496: loss=1.4652255773544312\n",
            "epoch 010/010, batch 001/002, step 0362/0496: loss=2.1606218814849854\n",
            "epoch 010/010, batch 001/002, step 0363/0496: loss=1.9076151847839355\n",
            "epoch 010/010, batch 001/002, step 0364/0496: loss=1.6122972965240479\n",
            "epoch 010/010, batch 001/002, step 0365/0496: loss=1.1722726821899414\n",
            "epoch 010/010, batch 001/002, step 0366/0496: loss=1.6524370908737183\n",
            "epoch 010/010, batch 001/002, step 0367/0496: loss=1.8804579973220825\n",
            "epoch 010/010, batch 001/002, step 0368/0496: loss=1.3853635787963867\n",
            "epoch 010/010, batch 001/002, step 0369/0496: loss=1.1485437154769897\n",
            "epoch 010/010, batch 001/002, step 0370/0496: loss=1.291992425918579\n",
            "epoch 010/010, batch 001/002, step 0371/0496: loss=1.5735048055648804\n",
            "epoch 010/010, batch 001/002, step 0372/0496: loss=1.7555371522903442\n",
            "epoch 010/010, batch 001/002, step 0373/0496: loss=1.4624435901641846\n",
            "epoch 010/010, batch 001/002, step 0374/0496: loss=0.9873633980751038\n",
            "epoch 010/010, batch 001/002, step 0375/0496: loss=2.58581280708313\n",
            "epoch 010/010, batch 001/002, step 0376/0496: loss=1.2288588285446167\n",
            "epoch 010/010, batch 001/002, step 0377/0496: loss=1.473559021949768\n",
            "epoch 010/010, batch 001/002, step 0378/0496: loss=1.3751991987228394\n",
            "epoch 010/010, batch 001/002, step 0379/0496: loss=1.679762601852417\n",
            "epoch 010/010, batch 001/002, step 0380/0496: loss=1.5970289707183838\n",
            "epoch 010/010, batch 001/002, step 0381/0496: loss=1.2176082134246826\n",
            "epoch 010/010, batch 001/002, step 0382/0496: loss=1.1029019355773926\n",
            "epoch 010/010, batch 001/002, step 0383/0496: loss=1.3891189098358154\n",
            "epoch 010/010, batch 001/002, step 0384/0496: loss=1.4866547584533691\n",
            "epoch 010/010, batch 001/002, step 0385/0496: loss=1.4863563776016235\n",
            "epoch 010/010, batch 001/002, step 0386/0496: loss=1.7762714624404907\n",
            "epoch 010/010, batch 001/002, step 0387/0496: loss=1.4855871200561523\n",
            "epoch 010/010, batch 001/002, step 0388/0496: loss=1.3066554069519043\n",
            "epoch 010/010, batch 001/002, step 0389/0496: loss=1.274876356124878\n",
            "epoch 010/010, batch 001/002, step 0390/0496: loss=1.6839991807937622\n",
            "epoch 010/010, batch 001/002, step 0391/0496: loss=1.3569111824035645\n",
            "epoch 010/010, batch 001/002, step 0392/0496: loss=1.3938095569610596\n",
            "epoch 010/010, batch 001/002, step 0393/0496: loss=1.7069525718688965\n",
            "epoch 010/010, batch 001/002, step 0394/0496: loss=1.0700584650039673\n",
            "epoch 010/010, batch 001/002, step 0395/0496: loss=1.342319130897522\n",
            "epoch 010/010, batch 001/002, step 0396/0496: loss=1.432269811630249\n",
            "epoch 010/010, batch 001/002, step 0397/0496: loss=2.6093220710754395\n",
            "epoch 010/010, batch 001/002, step 0398/0496: loss=1.4239577054977417\n",
            "epoch 010/010, batch 001/002, step 0399/0496: loss=1.624860405921936\n",
            "epoch 010/010, batch 001/002, step 0400/0496: loss=1.9392765760421753\n",
            "epoch 010/010, batch 001/002, step 0401/0496: loss=1.3408899307250977\n",
            "epoch 010/010, batch 001/002, step 0402/0496: loss=1.624690055847168\n",
            "epoch 010/010, batch 001/002, step 0403/0496: loss=1.3295546770095825\n",
            "epoch 010/010, batch 001/002, step 0404/0496: loss=1.3894952535629272\n",
            "epoch 010/010, batch 001/002, step 0405/0496: loss=1.1609179973602295\n",
            "epoch 010/010, batch 001/002, step 0406/0496: loss=2.061786651611328\n",
            "epoch 010/010, batch 001/002, step 0407/0496: loss=1.27992582321167\n",
            "epoch 010/010, batch 001/002, step 0408/0496: loss=1.5768964290618896\n",
            "epoch 010/010, batch 001/002, step 0409/0496: loss=1.4355790615081787\n",
            "epoch 010/010, batch 001/002, step 0410/0496: loss=1.3283222913742065\n",
            "epoch 010/010, batch 001/002, step 0411/0496: loss=1.4044469594955444\n",
            "epoch 010/010, batch 001/002, step 0412/0496: loss=2.157132625579834\n",
            "epoch 010/010, batch 001/002, step 0413/0496: loss=1.371467113494873\n",
            "epoch 010/010, batch 001/002, step 0414/0496: loss=1.4608163833618164\n",
            "epoch 010/010, batch 001/002, step 0415/0496: loss=2.8708643913269043\n",
            "epoch 010/010, batch 001/002, step 0416/0496: loss=1.4064021110534668\n",
            "epoch 010/010, batch 001/002, step 0417/0496: loss=1.6261359453201294\n",
            "epoch 010/010, batch 001/002, step 0418/0496: loss=1.4043078422546387\n",
            "epoch 010/010, batch 001/002, step 0419/0496: loss=1.3765583038330078\n",
            "epoch 010/010, batch 001/002, step 0420/0496: loss=1.3255218267440796\n",
            "epoch 010/010, batch 001/002, step 0421/0496: loss=1.4775264263153076\n",
            "epoch 010/010, batch 001/002, step 0422/0496: loss=1.6733952760696411\n",
            "epoch 010/010, batch 001/002, step 0423/0496: loss=2.040553092956543\n",
            "epoch 010/010, batch 001/002, step 0424/0496: loss=1.8010618686676025\n",
            "epoch 010/010, batch 001/002, step 0425/0496: loss=2.0338857173919678\n",
            "epoch 010/010, batch 001/002, step 0426/0496: loss=1.456591010093689\n",
            "epoch 010/010, batch 001/002, step 0427/0496: loss=1.4851725101470947\n",
            "epoch 010/010, batch 001/002, step 0428/0496: loss=1.442493200302124\n",
            "epoch 010/010, batch 001/002, step 0429/0496: loss=1.7776072025299072\n",
            "epoch 010/010, batch 001/002, step 0430/0496: loss=1.3148307800292969\n",
            "epoch 010/010, batch 001/002, step 0431/0496: loss=2.0304157733917236\n",
            "epoch 010/010, batch 001/002, step 0432/0496: loss=1.5961107015609741\n",
            "epoch 010/010, batch 001/002, step 0433/0496: loss=2.155416965484619\n",
            "epoch 010/010, batch 001/002, step 0434/0496: loss=1.7502673864364624\n",
            "epoch 010/010, batch 001/002, step 0435/0496: loss=1.4961590766906738\n",
            "epoch 010/010, batch 001/002, step 0436/0496: loss=1.3669463396072388\n",
            "epoch 010/010, batch 001/002, step 0437/0496: loss=1.7352077960968018\n",
            "epoch 010/010, batch 001/002, step 0438/0496: loss=1.9275224208831787\n",
            "epoch 010/010, batch 001/002, step 0439/0496: loss=1.4313554763793945\n",
            "epoch 010/010, batch 001/002, step 0440/0496: loss=1.8367795944213867\n",
            "epoch 010/010, batch 001/002, step 0441/0496: loss=1.7501673698425293\n",
            "epoch 010/010, batch 001/002, step 0442/0496: loss=1.9784190654754639\n",
            "epoch 010/010, batch 001/002, step 0443/0496: loss=1.6185222864151\n",
            "epoch 010/010, batch 001/002, step 0444/0496: loss=1.5471606254577637\n",
            "epoch 010/010, batch 001/002, step 0445/0496: loss=1.5778205394744873\n",
            "epoch 010/010, batch 001/002, step 0446/0496: loss=1.7701191902160645\n",
            "epoch 010/010, batch 001/002, step 0447/0496: loss=1.6321685314178467\n",
            "epoch 010/010, batch 001/002, step 0448/0496: loss=1.2574604749679565\n",
            "epoch 010/010, batch 001/002, step 0449/0496: loss=1.387465476989746\n",
            "epoch 010/010, batch 001/002, step 0450/0496: loss=1.5925979614257812\n",
            "epoch 010/010, batch 001/002, step 0451/0496: loss=2.2145400047302246\n",
            "epoch 010/010, batch 001/002, step 0452/0496: loss=1.7941007614135742\n",
            "epoch 010/010, batch 001/002, step 0453/0496: loss=2.1622579097747803\n",
            "epoch 010/010, batch 001/002, step 0454/0496: loss=1.454927921295166\n",
            "epoch 010/010, batch 001/002, step 0455/0496: loss=1.4395534992218018\n",
            "epoch 010/010, batch 001/002, step 0456/0496: loss=1.5472428798675537\n",
            "epoch 010/010, batch 001/002, step 0457/0496: loss=1.8646903038024902\n",
            "epoch 010/010, batch 001/002, step 0458/0496: loss=1.4367613792419434\n",
            "epoch 010/010, batch 001/002, step 0459/0496: loss=2.0956616401672363\n",
            "epoch 010/010, batch 001/002, step 0460/0496: loss=1.5611848831176758\n",
            "epoch 010/010, batch 001/002, step 0461/0496: loss=2.590127944946289\n",
            "epoch 010/010, batch 001/002, step 0462/0496: loss=1.6607263088226318\n",
            "epoch 010/010, batch 001/002, step 0463/0496: loss=1.6873151063919067\n",
            "epoch 010/010, batch 001/002, step 0464/0496: loss=1.5636532306671143\n",
            "epoch 010/010, batch 001/002, step 0465/0496: loss=1.4454970359802246\n",
            "epoch 010/010, batch 001/002, step 0466/0496: loss=1.5271682739257812\n",
            "epoch 010/010, batch 001/002, step 0467/0496: loss=1.675446629524231\n",
            "epoch 010/010, batch 001/002, step 0468/0496: loss=1.6623859405517578\n",
            "epoch 010/010, batch 001/002, step 0469/0496: loss=1.5289287567138672\n",
            "epoch 010/010, batch 001/002, step 0470/0496: loss=1.7382469177246094\n",
            "epoch 010/010, batch 001/002, step 0471/0496: loss=2.5919644832611084\n",
            "epoch 010/010, batch 001/002, step 0472/0496: loss=1.3825900554656982\n",
            "epoch 010/010, batch 001/002, step 0473/0496: loss=1.3505076169967651\n",
            "epoch 010/010, batch 001/002, step 0474/0496: loss=1.4288347959518433\n",
            "epoch 010/010, batch 001/002, step 0475/0496: loss=1.601262092590332\n",
            "epoch 010/010, batch 001/002, step 0476/0496: loss=1.1727986335754395\n",
            "epoch 010/010, batch 001/002, step 0477/0496: loss=1.6898655891418457\n",
            "epoch 010/010, batch 001/002, step 0478/0496: loss=1.3741434812545776\n",
            "epoch 010/010, batch 001/002, step 0479/0496: loss=1.5209773778915405\n",
            "epoch 010/010, batch 001/002, step 0480/0496: loss=1.1527276039123535\n",
            "epoch 010/010, batch 001/002, step 0481/0496: loss=1.77644681930542\n",
            "epoch 010/010, batch 001/002, step 0482/0496: loss=1.6242914199829102\n",
            "epoch 010/010, batch 001/002, step 0483/0496: loss=1.3976073265075684\n",
            "epoch 010/010, batch 001/002, step 0484/0496: loss=2.805159568786621\n",
            "epoch 010/010, batch 001/002, step 0485/0496: loss=1.2031047344207764\n",
            "epoch 010/010, batch 001/002, step 0486/0496: loss=1.988266944885254\n",
            "epoch 010/010, batch 001/002, step 0487/0496: loss=1.345686435699463\n",
            "epoch 010/010, batch 001/002, step 0488/0496: loss=1.302488088607788\n",
            "epoch 010/010, batch 001/002, step 0489/0496: loss=1.8749973773956299\n",
            "epoch 010/010, batch 001/002, step 0490/0496: loss=1.4721322059631348\n",
            "epoch 010/010, batch 001/002, step 0491/0496: loss=1.509918451309204\n",
            "epoch 010/010, batch 001/002, step 0492/0496: loss=2.163483142852783\n",
            "epoch 010/010, batch 001/002, step 0493/0496: loss=1.181342601776123\n",
            "epoch 010/010, batch 001/002, step 0494/0496: loss=1.6916239261627197\n",
            "epoch 010/010, batch 001/002, step 0495/0496: loss=2.5268592834472656\n",
            "epoch 010/010, batch 001/002, step 0496/0496: loss=1.1804131269454956\n",
            "epoch 010/010, batch 002/002, step 0001/0496: loss=1.9075886011123657\n",
            "epoch 010/010, batch 002/002, step 0002/0496: loss=1.8618263006210327\n",
            "epoch 010/010, batch 002/002, step 0003/0496: loss=1.4745774269104004\n",
            "epoch 010/010, batch 002/002, step 0004/0496: loss=1.870133876800537\n",
            "epoch 010/010, batch 002/002, step 0005/0496: loss=1.4533970355987549\n",
            "epoch 010/010, batch 002/002, step 0006/0496: loss=1.5046613216400146\n",
            "epoch 010/010, batch 002/002, step 0007/0496: loss=1.5406172275543213\n",
            "epoch 010/010, batch 002/002, step 0008/0496: loss=1.5494933128356934\n",
            "epoch 010/010, batch 002/002, step 0009/0496: loss=1.5729130506515503\n",
            "epoch 010/010, batch 002/002, step 0010/0496: loss=1.279300570487976\n",
            "epoch 010/010, batch 002/002, step 0011/0496: loss=1.598490595817566\n",
            "epoch 010/010, batch 002/002, step 0012/0496: loss=1.0016523599624634\n",
            "epoch 010/010, batch 002/002, step 0013/0496: loss=1.6060919761657715\n",
            "epoch 010/010, batch 002/002, step 0014/0496: loss=1.65450119972229\n",
            "epoch 010/010, batch 002/002, step 0015/0496: loss=2.632411241531372\n",
            "epoch 010/010, batch 002/002, step 0016/0496: loss=1.5522711277008057\n",
            "epoch 010/010, batch 002/002, step 0017/0496: loss=1.090632438659668\n",
            "epoch 010/010, batch 002/002, step 0018/0496: loss=1.1980712413787842\n",
            "epoch 010/010, batch 002/002, step 0019/0496: loss=1.3257079124450684\n",
            "epoch 010/010, batch 002/002, step 0020/0496: loss=1.2169125080108643\n",
            "epoch 010/010, batch 002/002, step 0021/0496: loss=1.7166439294815063\n",
            "epoch 010/010, batch 002/002, step 0022/0496: loss=1.8312745094299316\n",
            "epoch 010/010, batch 002/002, step 0023/0496: loss=1.7591233253479004\n",
            "epoch 010/010, batch 002/002, step 0024/0496: loss=1.5620245933532715\n",
            "epoch 010/010, batch 002/002, step 0025/0496: loss=1.516645908355713\n",
            "epoch 010/010, batch 002/002, step 0026/0496: loss=1.3973149061203003\n",
            "epoch 010/010, batch 002/002, step 0027/0496: loss=1.575913667678833\n",
            "epoch 010/010, batch 002/002, step 0028/0496: loss=1.268599033355713\n",
            "epoch 010/010, batch 002/002, step 0029/0496: loss=1.479568600654602\n",
            "epoch 010/010, batch 002/002, step 0030/0496: loss=1.3366472721099854\n",
            "epoch 010/010, batch 002/002, step 0031/0496: loss=1.2535251379013062\n",
            "epoch 010/010, batch 002/002, step 0032/0496: loss=1.3835506439208984\n",
            "epoch 010/010, batch 002/002, step 0033/0496: loss=1.0889184474945068\n",
            "epoch 010/010, batch 002/002, step 0034/0496: loss=1.1401387453079224\n",
            "epoch 010/010, batch 002/002, step 0035/0496: loss=1.3451076745986938\n",
            "epoch 010/010, batch 002/002, step 0036/0496: loss=1.0643783807754517\n",
            "epoch 010/010, batch 002/002, step 0037/0496: loss=1.5663294792175293\n",
            "epoch 010/010, batch 002/002, step 0038/0496: loss=1.2998253107070923\n",
            "epoch 010/010, batch 002/002, step 0039/0496: loss=1.2170193195343018\n",
            "epoch 010/010, batch 002/002, step 0040/0496: loss=1.4611070156097412\n",
            "epoch 010/010, batch 002/002, step 0041/0496: loss=1.4039905071258545\n",
            "epoch 010/010, batch 002/002, step 0042/0496: loss=1.5373706817626953\n",
            "epoch 010/010, batch 002/002, step 0043/0496: loss=1.4824941158294678\n",
            "epoch 010/010, batch 002/002, step 0044/0496: loss=2.0463666915893555\n",
            "epoch 010/010, batch 002/002, step 0045/0496: loss=1.513016939163208\n",
            "epoch 010/010, batch 002/002, step 0046/0496: loss=1.3426669836044312\n",
            "epoch 010/010, batch 002/002, step 0047/0496: loss=1.3980052471160889\n",
            "epoch 010/010, batch 002/002, step 0048/0496: loss=2.500398874282837\n",
            "epoch 010/010, batch 002/002, step 0049/0496: loss=1.279175877571106\n",
            "epoch 010/010, batch 002/002, step 0050/0496: loss=1.4492087364196777\n",
            "epoch 010/010, batch 002/002, step 0051/0496: loss=1.31950843334198\n",
            "epoch 010/010, batch 002/002, step 0052/0496: loss=1.0060324668884277\n",
            "epoch 010/010, batch 002/002, step 0053/0496: loss=1.216306447982788\n",
            "epoch 010/010, batch 002/002, step 0054/0496: loss=1.352421522140503\n",
            "epoch 010/010, batch 002/002, step 0055/0496: loss=1.0968245267868042\n",
            "epoch 010/010, batch 002/002, step 0056/0496: loss=1.741004467010498\n",
            "epoch 010/010, batch 002/002, step 0057/0496: loss=1.7469090223312378\n",
            "epoch 010/010, batch 002/002, step 0058/0496: loss=1.2275400161743164\n",
            "epoch 010/010, batch 002/002, step 0059/0496: loss=1.5330398082733154\n",
            "epoch 010/010, batch 002/002, step 0060/0496: loss=1.367749810218811\n",
            "epoch 010/010, batch 002/002, step 0061/0496: loss=1.5283596515655518\n",
            "epoch 010/010, batch 002/002, step 0062/0496: loss=1.37738835811615\n",
            "epoch 010/010, batch 002/002, step 0063/0496: loss=2.697500705718994\n",
            "epoch 010/010, batch 002/002, step 0064/0496: loss=1.1870543956756592\n",
            "epoch 010/010, batch 002/002, step 0065/0496: loss=1.6281626224517822\n",
            "epoch 010/010, batch 002/002, step 0066/0496: loss=1.743974208831787\n",
            "epoch 010/010, batch 002/002, step 0067/0496: loss=1.8766252994537354\n",
            "epoch 010/010, batch 002/002, step 0068/0496: loss=1.2494595050811768\n",
            "epoch 010/010, batch 002/002, step 0069/0496: loss=1.382689118385315\n",
            "epoch 010/010, batch 002/002, step 0070/0496: loss=1.49459707736969\n",
            "epoch 010/010, batch 002/002, step 0071/0496: loss=1.5396690368652344\n",
            "epoch 010/010, batch 002/002, step 0072/0496: loss=1.466904640197754\n",
            "epoch 010/010, batch 002/002, step 0073/0496: loss=2.250314235687256\n",
            "epoch 010/010, batch 002/002, step 0074/0496: loss=1.5993446111679077\n",
            "epoch 010/010, batch 002/002, step 0075/0496: loss=1.855289101600647\n",
            "epoch 010/010, batch 002/002, step 0076/0496: loss=2.066789150238037\n",
            "epoch 010/010, batch 002/002, step 0077/0496: loss=1.7726844549179077\n",
            "epoch 010/010, batch 002/002, step 0078/0496: loss=1.6205676794052124\n",
            "epoch 010/010, batch 002/002, step 0079/0496: loss=1.714207649230957\n",
            "epoch 010/010, batch 002/002, step 0080/0496: loss=1.4973992109298706\n",
            "epoch 010/010, batch 002/002, step 0081/0496: loss=2.747931718826294\n",
            "epoch 010/010, batch 002/002, step 0082/0496: loss=1.0121264457702637\n",
            "epoch 010/010, batch 002/002, step 0083/0496: loss=1.3991813659667969\n",
            "epoch 010/010, batch 002/002, step 0084/0496: loss=1.6826406717300415\n",
            "epoch 010/010, batch 002/002, step 0085/0496: loss=1.6595103740692139\n",
            "epoch 010/010, batch 002/002, step 0086/0496: loss=1.436246395111084\n",
            "epoch 010/010, batch 002/002, step 0087/0496: loss=1.5413525104522705\n",
            "epoch 010/010, batch 002/002, step 0088/0496: loss=1.0372077226638794\n",
            "epoch 010/010, batch 002/002, step 0089/0496: loss=1.163330316543579\n",
            "epoch 010/010, batch 002/002, step 0090/0496: loss=1.2809467315673828\n",
            "epoch 010/010, batch 002/002, step 0091/0496: loss=1.6093006134033203\n",
            "epoch 010/010, batch 002/002, step 0092/0496: loss=1.1733208894729614\n",
            "epoch 010/010, batch 002/002, step 0093/0496: loss=1.6312810182571411\n",
            "epoch 010/010, batch 002/002, step 0094/0496: loss=1.4512614011764526\n",
            "epoch 010/010, batch 002/002, step 0095/0496: loss=1.3791980743408203\n",
            "epoch 010/010, batch 002/002, step 0096/0496: loss=2.210587501525879\n",
            "epoch 010/010, batch 002/002, step 0097/0496: loss=2.0093905925750732\n",
            "epoch 010/010, batch 002/002, step 0098/0496: loss=1.2362494468688965\n",
            "epoch 010/010, batch 002/002, step 0099/0496: loss=1.7435717582702637\n",
            "epoch 010/010, batch 002/002, step 0100/0496: loss=1.3984901905059814\n",
            "epoch 010/010, batch 002/002, step 0101/0496: loss=1.7106480598449707\n",
            "epoch 010/010, batch 002/002, step 0102/0496: loss=1.0988752841949463\n",
            "epoch 010/010, batch 002/002, step 0103/0496: loss=1.7568694353103638\n",
            "epoch 010/010, batch 002/002, step 0104/0496: loss=1.4905682802200317\n",
            "epoch 010/010, batch 002/002, step 0105/0496: loss=1.859492540359497\n",
            "epoch 010/010, batch 002/002, step 0106/0496: loss=1.5535874366760254\n",
            "epoch 010/010, batch 002/002, step 0107/0496: loss=1.9089974164962769\n",
            "epoch 010/010, batch 002/002, step 0108/0496: loss=1.252642035484314\n",
            "epoch 010/010, batch 002/002, step 0109/0496: loss=1.5770351886749268\n",
            "epoch 010/010, batch 002/002, step 0110/0496: loss=1.9583306312561035\n",
            "epoch 010/010, batch 002/002, step 0111/0496: loss=1.7144168615341187\n",
            "epoch 010/010, batch 002/002, step 0112/0496: loss=1.4469759464263916\n",
            "epoch 010/010, batch 002/002, step 0113/0496: loss=1.5759581327438354\n",
            "epoch 010/010, batch 002/002, step 0114/0496: loss=1.5074834823608398\n",
            "epoch 010/010, batch 002/002, step 0115/0496: loss=1.2688639163970947\n",
            "epoch 010/010, batch 002/002, step 0116/0496: loss=1.8543572425842285\n",
            "epoch 010/010, batch 002/002, step 0117/0496: loss=2.0717930793762207\n",
            "epoch 010/010, batch 002/002, step 0118/0496: loss=1.473137617111206\n",
            "epoch 010/010, batch 002/002, step 0119/0496: loss=1.6777491569519043\n",
            "epoch 010/010, batch 002/002, step 0120/0496: loss=1.3172969818115234\n",
            "epoch 010/010, batch 002/002, step 0121/0496: loss=1.580346941947937\n",
            "epoch 010/010, batch 002/002, step 0122/0496: loss=1.4007363319396973\n",
            "epoch 010/010, batch 002/002, step 0123/0496: loss=1.4788029193878174\n",
            "epoch 010/010, batch 002/002, step 0124/0496: loss=1.5342093706130981\n",
            "epoch 010/010, batch 002/002, step 0125/0496: loss=1.4318372011184692\n",
            "epoch 010/010, batch 002/002, step 0126/0496: loss=1.3999321460723877\n",
            "epoch 010/010, batch 002/002, step 0127/0496: loss=1.621989130973816\n",
            "epoch 010/010, batch 002/002, step 0128/0496: loss=1.81881582736969\n",
            "epoch 010/010, batch 002/002, step 0129/0496: loss=1.340395450592041\n",
            "epoch 010/010, batch 002/002, step 0130/0496: loss=1.4269548654556274\n",
            "epoch 010/010, batch 002/002, step 0131/0496: loss=1.3026342391967773\n",
            "epoch 010/010, batch 002/002, step 0132/0496: loss=1.1748656034469604\n",
            "epoch 010/010, batch 002/002, step 0133/0496: loss=1.6026779413223267\n",
            "epoch 010/010, batch 002/002, step 0134/0496: loss=1.4787511825561523\n",
            "epoch 010/010, batch 002/002, step 0135/0496: loss=1.5457804203033447\n",
            "epoch 010/010, batch 002/002, step 0136/0496: loss=1.1841161251068115\n",
            "epoch 010/010, batch 002/002, step 0137/0496: loss=1.31044340133667\n",
            "epoch 010/010, batch 002/002, step 0138/0496: loss=1.377776861190796\n",
            "epoch 010/010, batch 002/002, step 0139/0496: loss=1.3838014602661133\n",
            "epoch 010/010, batch 002/002, step 0140/0496: loss=1.3694484233856201\n",
            "epoch 010/010, batch 002/002, step 0141/0496: loss=2.7641029357910156\n",
            "epoch 010/010, batch 002/002, step 0142/0496: loss=1.2380695343017578\n",
            "epoch 010/010, batch 002/002, step 0143/0496: loss=1.7088927030563354\n",
            "epoch 010/010, batch 002/002, step 0144/0496: loss=1.5791891813278198\n",
            "epoch 010/010, batch 002/002, step 0145/0496: loss=1.2943038940429688\n",
            "epoch 010/010, batch 002/002, step 0146/0496: loss=1.3929531574249268\n",
            "epoch 010/010, batch 002/002, step 0147/0496: loss=1.177826166152954\n",
            "epoch 010/010, batch 002/002, step 0148/0496: loss=1.7816095352172852\n",
            "epoch 010/010, batch 002/002, step 0149/0496: loss=1.6547831296920776\n",
            "epoch 010/010, batch 002/002, step 0150/0496: loss=1.206382155418396\n",
            "epoch 010/010, batch 002/002, step 0151/0496: loss=1.2159373760223389\n",
            "epoch 010/010, batch 002/002, step 0152/0496: loss=1.4807889461517334\n",
            "epoch 010/010, batch 002/002, step 0153/0496: loss=1.4307458400726318\n",
            "epoch 010/010, batch 002/002, step 0154/0496: loss=1.409201741218567\n",
            "epoch 010/010, batch 002/002, step 0155/0496: loss=2.0289993286132812\n",
            "epoch 010/010, batch 002/002, step 0156/0496: loss=1.2133084535598755\n",
            "epoch 010/010, batch 002/002, step 0157/0496: loss=1.3183367252349854\n",
            "epoch 010/010, batch 002/002, step 0158/0496: loss=1.3629915714263916\n",
            "epoch 010/010, batch 002/002, step 0159/0496: loss=1.7814147472381592\n",
            "epoch 010/010, batch 002/002, step 0160/0496: loss=2.035365581512451\n",
            "epoch 010/010, batch 002/002, step 0161/0496: loss=0.980483889579773\n",
            "epoch 010/010, batch 002/002, step 0162/0496: loss=1.4750523567199707\n",
            "epoch 010/010, batch 002/002, step 0163/0496: loss=1.6620694398880005\n",
            "epoch 010/010, batch 002/002, step 0164/0496: loss=1.467339277267456\n",
            "epoch 010/010, batch 002/002, step 0165/0496: loss=1.9402992725372314\n",
            "epoch 010/010, batch 002/002, step 0166/0496: loss=1.3052167892456055\n",
            "epoch 010/010, batch 002/002, step 0167/0496: loss=1.6369311809539795\n",
            "epoch 010/010, batch 002/002, step 0168/0496: loss=1.1493676900863647\n",
            "epoch 010/010, batch 002/002, step 0169/0496: loss=1.596160888671875\n",
            "epoch 010/010, batch 002/002, step 0170/0496: loss=1.5051766633987427\n",
            "epoch 010/010, batch 002/002, step 0171/0496: loss=2.043017625808716\n",
            "epoch 010/010, batch 002/002, step 0172/0496: loss=1.4794762134552002\n",
            "epoch 010/010, batch 002/002, step 0173/0496: loss=1.6021357774734497\n",
            "epoch 010/010, batch 002/002, step 0174/0496: loss=1.5487967729568481\n",
            "epoch 010/010, batch 002/002, step 0175/0496: loss=1.3505792617797852\n",
            "epoch 010/010, batch 002/002, step 0176/0496: loss=1.683703064918518\n",
            "epoch 010/010, batch 002/002, step 0177/0496: loss=2.7142786979675293\n",
            "epoch 010/010, batch 002/002, step 0178/0496: loss=1.8868638277053833\n",
            "epoch 010/010, batch 002/002, step 0179/0496: loss=1.356232762336731\n",
            "epoch 010/010, batch 002/002, step 0180/0496: loss=1.8014389276504517\n",
            "epoch 010/010, batch 002/002, step 0181/0496: loss=1.566703200340271\n",
            "epoch 010/010, batch 002/002, step 0182/0496: loss=1.8132370710372925\n",
            "epoch 010/010, batch 002/002, step 0183/0496: loss=1.5586951971054077\n",
            "epoch 010/010, batch 002/002, step 0184/0496: loss=1.4333856105804443\n",
            "epoch 010/010, batch 002/002, step 0185/0496: loss=1.1476229429244995\n",
            "epoch 010/010, batch 002/002, step 0186/0496: loss=1.480170488357544\n",
            "epoch 010/010, batch 002/002, step 0187/0496: loss=1.0950405597686768\n",
            "epoch 010/010, batch 002/002, step 0188/0496: loss=1.574524164199829\n",
            "epoch 010/010, batch 002/002, step 0189/0496: loss=1.316495418548584\n",
            "epoch 010/010, batch 002/002, step 0190/0496: loss=1.6291471719741821\n",
            "epoch 010/010, batch 002/002, step 0191/0496: loss=1.4393281936645508\n",
            "epoch 010/010, batch 002/002, step 0192/0496: loss=1.415945291519165\n",
            "epoch 010/010, batch 002/002, step 0193/0496: loss=1.6776342391967773\n",
            "epoch 010/010, batch 002/002, step 0194/0496: loss=1.6655360460281372\n",
            "epoch 010/010, batch 002/002, step 0195/0496: loss=1.5301109552383423\n",
            "epoch 010/010, batch 002/002, step 0196/0496: loss=1.6359655857086182\n",
            "epoch 010/010, batch 002/002, step 0197/0496: loss=1.7844516038894653\n",
            "epoch 010/010, batch 002/002, step 0198/0496: loss=1.90099036693573\n",
            "epoch 010/010, batch 002/002, step 0199/0496: loss=1.3669298887252808\n",
            "epoch 010/010, batch 002/002, step 0200/0496: loss=1.7091648578643799\n",
            "epoch 010/010, batch 002/002, step 0201/0496: loss=1.169970989227295\n",
            "epoch 010/010, batch 002/002, step 0202/0496: loss=2.1169631481170654\n",
            "epoch 010/010, batch 002/002, step 0203/0496: loss=1.520638346672058\n",
            "epoch 010/010, batch 002/002, step 0204/0496: loss=1.7834548950195312\n",
            "epoch 010/010, batch 002/002, step 0205/0496: loss=1.1790845394134521\n",
            "epoch 010/010, batch 002/002, step 0206/0496: loss=2.372847080230713\n",
            "epoch 010/010, batch 002/002, step 0207/0496: loss=2.5990982055664062\n",
            "epoch 010/010, batch 002/002, step 0208/0496: loss=1.6338791847229004\n",
            "epoch 010/010, batch 002/002, step 0209/0496: loss=2.497143268585205\n",
            "epoch 010/010, batch 002/002, step 0210/0496: loss=1.4988059997558594\n",
            "epoch 010/010, batch 002/002, step 0211/0496: loss=2.234776258468628\n",
            "epoch 010/010, batch 002/002, step 0212/0496: loss=1.5286897420883179\n",
            "epoch 010/010, batch 002/002, step 0213/0496: loss=1.7239234447479248\n",
            "epoch 010/010, batch 002/002, step 0214/0496: loss=1.8233020305633545\n",
            "epoch 010/010, batch 002/002, step 0215/0496: loss=1.9116270542144775\n",
            "epoch 010/010, batch 002/002, step 0216/0496: loss=1.466407060623169\n",
            "epoch 010/010, batch 002/002, step 0217/0496: loss=1.4176520109176636\n",
            "epoch 010/010, batch 002/002, step 0218/0496: loss=1.2178981304168701\n",
            "epoch 010/010, batch 002/002, step 0219/0496: loss=2.1547017097473145\n",
            "epoch 010/010, batch 002/002, step 0220/0496: loss=1.700025200843811\n",
            "epoch 010/010, batch 002/002, step 0221/0496: loss=1.0897579193115234\n",
            "epoch 010/010, batch 002/002, step 0222/0496: loss=1.2754969596862793\n",
            "epoch 010/010, batch 002/002, step 0223/0496: loss=1.7153551578521729\n",
            "epoch 010/010, batch 002/002, step 0224/0496: loss=1.3597074747085571\n",
            "epoch 010/010, batch 002/002, step 0225/0496: loss=1.3345038890838623\n",
            "epoch 010/010, batch 002/002, step 0226/0496: loss=1.7857414484024048\n",
            "epoch 010/010, batch 002/002, step 0227/0496: loss=1.9863793849945068\n",
            "epoch 010/010, batch 002/002, step 0228/0496: loss=2.1867516040802\n",
            "epoch 010/010, batch 002/002, step 0229/0496: loss=1.7572579383850098\n",
            "epoch 010/010, batch 002/002, step 0230/0496: loss=1.2630844116210938\n",
            "epoch 010/010, batch 002/002, step 0231/0496: loss=1.5121710300445557\n",
            "epoch 010/010, batch 002/002, step 0232/0496: loss=1.5938142538070679\n",
            "epoch 010/010, batch 002/002, step 0233/0496: loss=1.5410690307617188\n",
            "epoch 010/010, batch 002/002, step 0234/0496: loss=1.4787012338638306\n",
            "epoch 010/010, batch 002/002, step 0235/0496: loss=1.5230683088302612\n",
            "epoch 010/010, batch 002/002, step 0236/0496: loss=1.4490492343902588\n",
            "epoch 010/010, batch 002/002, step 0237/0496: loss=1.2378461360931396\n",
            "epoch 010/010, batch 002/002, step 0238/0496: loss=1.866189956665039\n",
            "epoch 010/010, batch 002/002, step 0239/0496: loss=1.9307174682617188\n",
            "epoch 010/010, batch 002/002, step 0240/0496: loss=1.4838438034057617\n",
            "epoch 010/010, batch 002/002, step 0241/0496: loss=1.6579391956329346\n",
            "epoch 010/010, batch 002/002, step 0242/0496: loss=2.2430291175842285\n",
            "epoch 010/010, batch 002/002, step 0243/0496: loss=2.8256258964538574\n",
            "epoch 010/010, batch 002/002, step 0244/0496: loss=1.5717979669570923\n",
            "epoch 010/010, batch 002/002, step 0245/0496: loss=1.499119758605957\n",
            "epoch 010/010, batch 002/002, step 0246/0496: loss=1.6167011260986328\n",
            "epoch 010/010, batch 002/002, step 0247/0496: loss=1.548424482345581\n",
            "epoch 010/010, batch 002/002, step 0248/0496: loss=2.3556056022644043\n",
            "epoch 010/010, batch 002/002, step 0249/0496: loss=1.49569571018219\n",
            "epoch 010/010, batch 002/002, step 0250/0496: loss=1.199811577796936\n",
            "epoch 010/010, batch 002/002, step 0251/0496: loss=1.6139171123504639\n",
            "epoch 010/010, batch 002/002, step 0252/0496: loss=1.2495821714401245\n",
            "epoch 010/010, batch 002/002, step 0253/0496: loss=1.6000502109527588\n",
            "epoch 010/010, batch 002/002, step 0254/0496: loss=1.4961881637573242\n",
            "epoch 010/010, batch 002/002, step 0255/0496: loss=1.3850685358047485\n",
            "epoch 010/010, batch 002/002, step 0256/0496: loss=1.425601840019226\n",
            "epoch 010/010, batch 002/002, step 0257/0496: loss=1.1865785121917725\n",
            "epoch 010/010, batch 002/002, step 0258/0496: loss=1.1445728540420532\n",
            "epoch 010/010, batch 002/002, step 0259/0496: loss=1.5125222206115723\n",
            "epoch 010/010, batch 002/002, step 0260/0496: loss=1.4719669818878174\n",
            "epoch 010/010, batch 002/002, step 0261/0496: loss=1.2778167724609375\n",
            "epoch 010/010, batch 002/002, step 0262/0496: loss=1.4638547897338867\n",
            "epoch 010/010, batch 002/002, step 0263/0496: loss=1.1882554292678833\n",
            "epoch 010/010, batch 002/002, step 0264/0496: loss=1.5190842151641846\n",
            "epoch 010/010, batch 002/002, step 0265/0496: loss=1.3889155387878418\n",
            "epoch 010/010, batch 002/002, step 0266/0496: loss=1.3854610919952393\n",
            "epoch 010/010, batch 002/002, step 0267/0496: loss=1.3545806407928467\n",
            "epoch 010/010, batch 002/002, step 0268/0496: loss=1.6591441631317139\n",
            "epoch 010/010, batch 002/002, step 0269/0496: loss=1.2790322303771973\n",
            "epoch 010/010, batch 002/002, step 0270/0496: loss=1.2670683860778809\n",
            "epoch 010/010, batch 002/002, step 0271/0496: loss=1.5909831523895264\n",
            "epoch 010/010, batch 002/002, step 0272/0496: loss=2.7470428943634033\n",
            "epoch 010/010, batch 002/002, step 0273/0496: loss=1.3233870267868042\n",
            "epoch 010/010, batch 002/002, step 0274/0496: loss=1.173405408859253\n",
            "epoch 010/010, batch 002/002, step 0275/0496: loss=1.3956680297851562\n",
            "epoch 010/010, batch 002/002, step 0276/0496: loss=1.2641571760177612\n",
            "epoch 010/010, batch 002/002, step 0277/0496: loss=1.3958227634429932\n",
            "epoch 010/010, batch 002/002, step 0278/0496: loss=0.9968144297599792\n",
            "epoch 010/010, batch 002/002, step 0279/0496: loss=1.3102672100067139\n",
            "epoch 010/010, batch 002/002, step 0280/0496: loss=1.473557949066162\n",
            "epoch 010/010, batch 002/002, step 0281/0496: loss=1.1922457218170166\n",
            "epoch 010/010, batch 002/002, step 0282/0496: loss=2.6147613525390625\n",
            "epoch 010/010, batch 002/002, step 0283/0496: loss=1.7472076416015625\n",
            "epoch 010/010, batch 002/002, step 0284/0496: loss=1.295669674873352\n",
            "epoch 010/010, batch 002/002, step 0285/0496: loss=0.9804869890213013\n",
            "epoch 010/010, batch 002/002, step 0286/0496: loss=1.289057970046997\n",
            "epoch 010/010, batch 002/002, step 0287/0496: loss=1.2472176551818848\n",
            "epoch 010/010, batch 002/002, step 0288/0496: loss=1.2920973300933838\n",
            "epoch 010/010, batch 002/002, step 0289/0496: loss=1.6233787536621094\n",
            "epoch 010/010, batch 002/002, step 0290/0496: loss=2.206998109817505\n",
            "epoch 010/010, batch 002/002, step 0291/0496: loss=1.9682629108428955\n",
            "epoch 010/010, batch 002/002, step 0292/0496: loss=1.3476035594940186\n",
            "epoch 010/010, batch 002/002, step 0293/0496: loss=0.9664930105209351\n",
            "epoch 010/010, batch 002/002, step 0294/0496: loss=1.3985472917556763\n",
            "epoch 010/010, batch 002/002, step 0295/0496: loss=1.3302593231201172\n",
            "epoch 010/010, batch 002/002, step 0296/0496: loss=2.4934144020080566\n",
            "epoch 010/010, batch 002/002, step 0297/0496: loss=1.4952518939971924\n",
            "epoch 010/010, batch 002/002, step 0298/0496: loss=1.1583375930786133\n",
            "epoch 010/010, batch 002/002, step 0299/0496: loss=1.26942777633667\n",
            "epoch 010/010, batch 002/002, step 0300/0496: loss=1.1088685989379883\n",
            "epoch 010/010, batch 002/002, step 0301/0496: loss=1.5957612991333008\n",
            "epoch 010/010, batch 002/002, step 0302/0496: loss=1.392249345779419\n",
            "epoch 010/010, batch 002/002, step 0303/0496: loss=1.2054940462112427\n",
            "epoch 010/010, batch 002/002, step 0304/0496: loss=1.3351072072982788\n",
            "epoch 010/010, batch 002/002, step 0305/0496: loss=1.1503188610076904\n",
            "epoch 010/010, batch 002/002, step 0306/0496: loss=1.7056825160980225\n",
            "epoch 010/010, batch 002/002, step 0307/0496: loss=1.4383511543273926\n",
            "epoch 010/010, batch 002/002, step 0308/0496: loss=1.2569682598114014\n",
            "epoch 010/010, batch 002/002, step 0309/0496: loss=1.3525116443634033\n",
            "epoch 010/010, batch 002/002, step 0310/0496: loss=1.368955135345459\n",
            "epoch 010/010, batch 002/002, step 0311/0496: loss=2.8251285552978516\n",
            "epoch 010/010, batch 002/002, step 0312/0496: loss=1.6000797748565674\n",
            "epoch 010/010, batch 002/002, step 0313/0496: loss=1.5993727445602417\n",
            "epoch 010/010, batch 002/002, step 0314/0496: loss=1.178685188293457\n",
            "epoch 010/010, batch 002/002, step 0315/0496: loss=1.4323227405548096\n",
            "epoch 010/010, batch 002/002, step 0316/0496: loss=1.6989362239837646\n",
            "epoch 010/010, batch 002/002, step 0317/0496: loss=1.3752851486206055\n",
            "epoch 010/010, batch 002/002, step 0318/0496: loss=1.6100438833236694\n",
            "epoch 010/010, batch 002/002, step 0319/0496: loss=1.5913233757019043\n",
            "epoch 010/010, batch 002/002, step 0320/0496: loss=1.5937860012054443\n",
            "epoch 010/010, batch 002/002, step 0321/0496: loss=1.446298599243164\n",
            "epoch 010/010, batch 002/002, step 0322/0496: loss=1.3450820446014404\n",
            "epoch 010/010, batch 002/002, step 0323/0496: loss=1.1280009746551514\n",
            "epoch 010/010, batch 002/002, step 0324/0496: loss=1.6300129890441895\n",
            "epoch 010/010, batch 002/002, step 0325/0496: loss=1.4809072017669678\n",
            "epoch 010/010, batch 002/002, step 0326/0496: loss=1.386087417602539\n",
            "epoch 010/010, batch 002/002, step 0327/0496: loss=1.4288108348846436\n",
            "epoch 010/010, batch 002/002, step 0328/0496: loss=1.4910337924957275\n",
            "epoch 010/010, batch 002/002, step 0329/0496: loss=1.4955157041549683\n",
            "epoch 010/010, batch 002/002, step 0330/0496: loss=1.5032620429992676\n",
            "epoch 010/010, batch 002/002, step 0331/0496: loss=2.62672758102417\n",
            "epoch 010/010, batch 002/002, step 0332/0496: loss=1.6204990148544312\n",
            "epoch 010/010, batch 002/002, step 0333/0496: loss=1.4567605257034302\n",
            "epoch 010/010, batch 002/002, step 0334/0496: loss=2.4160125255584717\n",
            "epoch 010/010, batch 002/002, step 0335/0496: loss=1.1364514827728271\n",
            "epoch 010/010, batch 002/002, step 0336/0496: loss=1.0043652057647705\n",
            "epoch 010/010, batch 002/002, step 0337/0496: loss=1.5086508989334106\n",
            "epoch 010/010, batch 002/002, step 0338/0496: loss=1.2670366764068604\n",
            "epoch 010/010, batch 002/002, step 0339/0496: loss=1.0069549083709717\n",
            "epoch 010/010, batch 002/002, step 0340/0496: loss=2.6683578491210938\n",
            "epoch 010/010, batch 002/002, step 0341/0496: loss=0.8311614990234375\n",
            "epoch 010/010, batch 002/002, step 0342/0496: loss=1.6251225471496582\n",
            "epoch 010/010, batch 002/002, step 0343/0496: loss=1.209605097770691\n",
            "epoch 010/010, batch 002/002, step 0344/0496: loss=1.7977882623672485\n",
            "epoch 010/010, batch 002/002, step 0345/0496: loss=1.1079018115997314\n",
            "epoch 010/010, batch 002/002, step 0346/0496: loss=1.4290461540222168\n",
            "epoch 010/010, batch 002/002, step 0347/0496: loss=2.495351791381836\n",
            "epoch 010/010, batch 002/002, step 0348/0496: loss=1.5612136125564575\n",
            "epoch 010/010, batch 002/002, step 0349/0496: loss=1.6616108417510986\n",
            "epoch 010/010, batch 002/002, step 0350/0496: loss=1.7061878442764282\n",
            "epoch 010/010, batch 002/002, step 0351/0496: loss=1.4769068956375122\n",
            "epoch 010/010, batch 002/002, step 0352/0496: loss=1.2763473987579346\n",
            "epoch 010/010, batch 002/002, step 0353/0496: loss=1.6567339897155762\n",
            "epoch 010/010, batch 002/002, step 0354/0496: loss=1.4827208518981934\n",
            "epoch 010/010, batch 002/002, step 0355/0496: loss=1.1776320934295654\n",
            "epoch 010/010, batch 002/002, step 0356/0496: loss=1.8939628601074219\n",
            "epoch 010/010, batch 002/002, step 0357/0496: loss=1.1434959173202515\n",
            "epoch 010/010, batch 002/002, step 0358/0496: loss=1.2921240329742432\n",
            "epoch 010/010, batch 002/002, step 0359/0496: loss=1.1412444114685059\n",
            "epoch 010/010, batch 002/002, step 0360/0496: loss=1.3873169422149658\n",
            "epoch 010/010, batch 002/002, step 0361/0496: loss=1.288717269897461\n",
            "epoch 010/010, batch 002/002, step 0362/0496: loss=1.3392256498336792\n",
            "epoch 010/010, batch 002/002, step 0363/0496: loss=1.741020679473877\n",
            "epoch 010/010, batch 002/002, step 0364/0496: loss=1.4693334102630615\n",
            "epoch 010/010, batch 002/002, step 0365/0496: loss=1.4793857336044312\n",
            "epoch 010/010, batch 002/002, step 0366/0496: loss=1.169884443283081\n",
            "epoch 010/010, batch 002/002, step 0367/0496: loss=1.4628679752349854\n",
            "epoch 010/010, batch 002/002, step 0368/0496: loss=1.4391852617263794\n",
            "epoch 010/010, batch 002/002, step 0369/0496: loss=1.2044743299484253\n",
            "epoch 010/010, batch 002/002, step 0370/0496: loss=1.3993523120880127\n",
            "epoch 010/010, batch 002/002, step 0371/0496: loss=1.5855755805969238\n",
            "epoch 010/010, batch 002/002, step 0372/0496: loss=1.1561635732650757\n",
            "epoch 010/010, batch 002/002, step 0373/0496: loss=1.4960458278656006\n",
            "epoch 010/010, batch 002/002, step 0374/0496: loss=1.3128455877304077\n",
            "epoch 010/010, batch 002/002, step 0375/0496: loss=2.2455966472625732\n",
            "epoch 010/010, batch 002/002, step 0376/0496: loss=1.6713659763336182\n",
            "epoch 010/010, batch 002/002, step 0377/0496: loss=1.8802709579467773\n",
            "epoch 010/010, batch 002/002, step 0378/0496: loss=1.2845349311828613\n",
            "epoch 010/010, batch 002/002, step 0379/0496: loss=2.4233548641204834\n",
            "epoch 010/010, batch 002/002, step 0380/0496: loss=1.3090994358062744\n",
            "epoch 010/010, batch 002/002, step 0381/0496: loss=1.6508033275604248\n",
            "epoch 010/010, batch 002/002, step 0382/0496: loss=1.619357705116272\n",
            "epoch 010/010, batch 002/002, step 0383/0496: loss=1.2711502313613892\n",
            "epoch 010/010, batch 002/002, step 0384/0496: loss=1.5573437213897705\n",
            "epoch 010/010, batch 002/002, step 0385/0496: loss=1.7894299030303955\n",
            "epoch 010/010, batch 002/002, step 0386/0496: loss=1.1964223384857178\n",
            "epoch 010/010, batch 002/002, step 0387/0496: loss=4.103240013122559\n",
            "epoch 010/010, batch 002/002, step 0388/0496: loss=1.3644614219665527\n",
            "epoch 010/010, batch 002/002, step 0389/0496: loss=1.1772606372833252\n",
            "epoch 010/010, batch 002/002, step 0390/0496: loss=1.7617794275283813\n",
            "epoch 010/010, batch 002/002, step 0391/0496: loss=1.0271930694580078\n",
            "epoch 010/010, batch 002/002, step 0392/0496: loss=1.6581711769104004\n",
            "epoch 010/010, batch 002/002, step 0393/0496: loss=0.9609203338623047\n",
            "epoch 010/010, batch 002/002, step 0394/0496: loss=1.2354254722595215\n",
            "epoch 010/010, batch 002/002, step 0395/0496: loss=1.2868714332580566\n",
            "epoch 010/010, batch 002/002, step 0396/0496: loss=1.3676326274871826\n",
            "epoch 010/010, batch 002/002, step 0397/0496: loss=0.9446921944618225\n",
            "epoch 010/010, batch 002/002, step 0398/0496: loss=1.15962553024292\n",
            "epoch 010/010, batch 002/002, step 0399/0496: loss=1.5571242570877075\n",
            "epoch 010/010, batch 002/002, step 0400/0496: loss=1.1342296600341797\n",
            "epoch 010/010, batch 002/002, step 0401/0496: loss=1.167205810546875\n",
            "epoch 010/010, batch 002/002, step 0402/0496: loss=1.488785743713379\n",
            "epoch 010/010, batch 002/002, step 0403/0496: loss=1.4010990858078003\n",
            "epoch 010/010, batch 002/002, step 0404/0496: loss=1.3192857503890991\n",
            "epoch 010/010, batch 002/002, step 0405/0496: loss=0.9813671708106995\n",
            "epoch 010/010, batch 002/002, step 0406/0496: loss=1.2995922565460205\n",
            "epoch 010/010, batch 002/002, step 0407/0496: loss=2.2714426517486572\n",
            "epoch 010/010, batch 002/002, step 0408/0496: loss=1.3837544918060303\n",
            "epoch 010/010, batch 002/002, step 0409/0496: loss=1.1340255737304688\n",
            "epoch 010/010, batch 002/002, step 0410/0496: loss=1.3196637630462646\n",
            "epoch 010/010, batch 002/002, step 0411/0496: loss=1.2841644287109375\n",
            "epoch 010/010, batch 002/002, step 0412/0496: loss=1.0898616313934326\n",
            "epoch 010/010, batch 002/002, step 0413/0496: loss=1.4562036991119385\n",
            "epoch 010/010, batch 002/002, step 0414/0496: loss=1.2245808839797974\n",
            "epoch 010/010, batch 002/002, step 0415/0496: loss=1.1819157600402832\n",
            "epoch 010/010, batch 002/002, step 0416/0496: loss=1.294081449508667\n",
            "epoch 010/010, batch 002/002, step 0417/0496: loss=1.3194730281829834\n",
            "epoch 010/010, batch 002/002, step 0418/0496: loss=1.372795581817627\n",
            "epoch 010/010, batch 002/002, step 0419/0496: loss=1.2842307090759277\n",
            "epoch 010/010, batch 002/002, step 0420/0496: loss=1.4746158123016357\n",
            "epoch 010/010, batch 002/002, step 0421/0496: loss=1.2959351539611816\n",
            "epoch 010/010, batch 002/002, step 0422/0496: loss=1.2671470642089844\n",
            "epoch 010/010, batch 002/002, step 0423/0496: loss=1.4527451992034912\n",
            "epoch 010/010, batch 002/002, step 0424/0496: loss=1.2246451377868652\n",
            "epoch 010/010, batch 002/002, step 0425/0496: loss=1.2461953163146973\n",
            "epoch 010/010, batch 002/002, step 0426/0496: loss=1.579035997390747\n",
            "epoch 010/010, batch 002/002, step 0427/0496: loss=1.5543737411499023\n",
            "epoch 010/010, batch 002/002, step 0428/0496: loss=0.9402856826782227\n",
            "epoch 010/010, batch 002/002, step 0429/0496: loss=1.2871620655059814\n",
            "epoch 010/010, batch 002/002, step 0430/0496: loss=1.2062287330627441\n",
            "epoch 010/010, batch 002/002, step 0431/0496: loss=1.509936809539795\n",
            "epoch 010/010, batch 002/002, step 0432/0496: loss=1.474363088607788\n",
            "epoch 010/010, batch 002/002, step 0433/0496: loss=2.4459469318389893\n",
            "epoch 010/010, batch 002/002, step 0434/0496: loss=1.5293166637420654\n",
            "epoch 010/010, batch 002/002, step 0435/0496: loss=0.8631331324577332\n",
            "epoch 010/010, batch 002/002, step 0436/0496: loss=1.4135220050811768\n",
            "epoch 010/010, batch 002/002, step 0437/0496: loss=1.612169861793518\n",
            "epoch 010/010, batch 002/002, step 0438/0496: loss=1.6353068351745605\n",
            "epoch 010/010, batch 002/002, step 0439/0496: loss=1.2638423442840576\n",
            "epoch 010/010, batch 002/002, step 0440/0496: loss=1.2351112365722656\n",
            "epoch 010/010, batch 002/002, step 0441/0496: loss=1.5474607944488525\n",
            "epoch 010/010, batch 002/002, step 0442/0496: loss=1.1425495147705078\n",
            "epoch 010/010, batch 002/002, step 0443/0496: loss=1.2968189716339111\n",
            "epoch 010/010, batch 002/002, step 0444/0496: loss=1.1909148693084717\n",
            "epoch 010/010, batch 002/002, step 0445/0496: loss=1.0868148803710938\n",
            "epoch 010/010, batch 002/002, step 0446/0496: loss=1.316916823387146\n",
            "epoch 010/010, batch 002/002, step 0447/0496: loss=1.2991080284118652\n",
            "epoch 010/010, batch 002/002, step 0448/0496: loss=1.2911639213562012\n",
            "epoch 010/010, batch 002/002, step 0449/0496: loss=1.124043345451355\n",
            "epoch 010/010, batch 002/002, step 0450/0496: loss=1.2757476568222046\n",
            "epoch 010/010, batch 002/002, step 0451/0496: loss=1.2682303190231323\n",
            "epoch 010/010, batch 002/002, step 0452/0496: loss=0.9823158383369446\n",
            "epoch 010/010, batch 002/002, step 0453/0496: loss=1.11976158618927\n",
            "epoch 010/010, batch 002/002, step 0454/0496: loss=1.5757628679275513\n",
            "epoch 010/010, batch 002/002, step 0455/0496: loss=2.2245945930480957\n",
            "epoch 010/010, batch 002/002, step 0456/0496: loss=1.1469839811325073\n",
            "epoch 010/010, batch 002/002, step 0457/0496: loss=1.3072988986968994\n",
            "epoch 010/010, batch 002/002, step 0458/0496: loss=1.2443228960037231\n",
            "epoch 010/010, batch 002/002, step 0459/0496: loss=1.1570193767547607\n",
            "epoch 010/010, batch 002/002, step 0460/0496: loss=2.9436614513397217\n",
            "epoch 010/010, batch 002/002, step 0461/0496: loss=1.8002663850784302\n",
            "epoch 010/010, batch 002/002, step 0462/0496: loss=1.21044921875\n",
            "epoch 010/010, batch 002/002, step 0463/0496: loss=1.5669523477554321\n",
            "epoch 010/010, batch 002/002, step 0464/0496: loss=1.1596102714538574\n",
            "epoch 010/010, batch 002/002, step 0465/0496: loss=1.1994142532348633\n",
            "epoch 010/010, batch 002/002, step 0466/0496: loss=1.491379976272583\n",
            "epoch 010/010, batch 002/002, step 0467/0496: loss=1.0866999626159668\n",
            "epoch 010/010, batch 002/002, step 0468/0496: loss=1.1967530250549316\n",
            "epoch 010/010, batch 002/002, step 0469/0496: loss=1.323357343673706\n",
            "epoch 010/010, batch 002/002, step 0470/0496: loss=1.2409605979919434\n",
            "epoch 010/010, batch 002/002, step 0471/0496: loss=1.435591697692871\n",
            "epoch 010/010, batch 002/002, step 0472/0496: loss=1.169161081314087\n",
            "epoch 010/010, batch 002/002, step 0473/0496: loss=2.469158172607422\n",
            "epoch 010/010, batch 002/002, step 0474/0496: loss=1.3121592998504639\n",
            "epoch 010/010, batch 002/002, step 0475/0496: loss=1.434693455696106\n",
            "epoch 010/010, batch 002/002, step 0476/0496: loss=1.1298186779022217\n",
            "epoch 010/010, batch 002/002, step 0477/0496: loss=1.5932166576385498\n",
            "epoch 010/010, batch 002/002, step 0478/0496: loss=1.2157971858978271\n",
            "epoch 010/010, batch 002/002, step 0479/0496: loss=1.184751272201538\n",
            "epoch 010/010, batch 002/002, step 0480/0496: loss=1.565835952758789\n",
            "epoch 010/010, batch 002/002, step 0481/0496: loss=1.2953816652297974\n",
            "epoch 010/010, batch 002/002, step 0482/0496: loss=1.4028916358947754\n",
            "epoch 010/010, batch 002/002, step 0483/0496: loss=1.0717511177062988\n",
            "epoch 010/010, batch 002/002, step 0484/0496: loss=1.6656436920166016\n",
            "epoch 010/010, batch 002/002, step 0485/0496: loss=1.356105089187622\n",
            "epoch 010/010, batch 002/002, step 0486/0496: loss=1.3049633502960205\n",
            "epoch 010/010, batch 002/002, step 0487/0496: loss=1.4518325328826904\n",
            "epoch 010/010, batch 002/002, step 0488/0496: loss=1.5286184549331665\n",
            "epoch 010/010, batch 002/002, step 0489/0496: loss=2.510633945465088\n",
            "epoch 010/010, batch 002/002, step 0490/0496: loss=1.0770660638809204\n",
            "epoch 010/010, batch 002/002, step 0491/0496: loss=2.1576650142669678\n",
            "epoch 010/010, batch 002/002, step 0492/0496: loss=1.4623496532440186\n",
            "epoch 010/010, batch 002/002, step 0493/0496: loss=0.8459208011627197\n",
            "epoch 010/010, batch 002/002, step 0494/0496: loss=1.3841673135757446\n",
            "epoch 010/010, batch 002/002, step 0495/0496: loss=1.3178244829177856\n",
            "epoch 010/010, batch 002/002, step 0496/0496: loss=1.3080101013183594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swG7GeDpWT_Z"
      },
      "source": [
        "loss should go down from ca. 1000 to around 1\n",
        "\n",
        "next test eval?"
      ]
    }
  ]
}